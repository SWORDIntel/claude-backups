#!/bin/bash
# Qwen 2.5 Local Inference Server Startup Script
# Generated by Claude Local Inference Implementation

set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
QWEN_SERVER="$SCRIPT_DIR/local-models/qwen-openvino/qwen_inference_server.py"
TORCH_VENV="$SCRIPT_DIR/.torch-venv"

echo "üöÄ Starting Qwen 2.5 Local Inference Server"
echo "   Server: $QWEN_SERVER"
echo "   Environment: $TORCH_VENV"
echo "   URL: http://localhost:8000"
echo "   Zero-token local inference for 98-agent system"
echo ""

# Check if torch virtual environment exists
if [ ! -d "$TORCH_VENV" ]; then
    echo "‚ùå Error: Torch virtual environment not found at $TORCH_VENV"
    echo "Please run the installer with --local-opus to set up the environment"
    exit 1
fi

# Check if server file exists
if [ ! -f "$QWEN_SERVER" ]; then
    echo "‚ùå Error: Qwen server not found at $QWEN_SERVER"
    echo "Please ensure the local inference system is properly installed"
    exit 1
fi

# Activate virtual environment and start server
cd "$SCRIPT_DIR"
source "$TORCH_VENV/bin/activate"

echo "üéØ Activating virtual environment..."
echo "üì¶ Python: $(python3 --version)"
echo "üß† Starting Qwen 2.5-32B inference server..."
echo ""

# Check for dependencies
python3 -c "
import sys
try:
    import openvino
    print('‚úì OpenVINO available')
except ImportError:
    print('‚úó OpenVINO not available')
    sys.exit(1)

try:
    from optimum.intel import OVModelForCausalLM
    print('‚úì Optimum Intel available')
except ImportError:
    print('‚úó Optimum Intel not available')
    sys.exit(1)

try:
    import fastapi
    print('‚úì FastAPI available')
except ImportError:
    print('‚úó FastAPI not available')
    sys.exit(1)

print('‚úÖ All dependencies ready')
"

if [ $? -ne 0 ]; then
    echo "‚ùå Dependency check failed"
    exit 1
fi

echo ""
echo "üöÄ Launching Qwen 2.5 inference server..."

# Start the server
python3 "$QWEN_SERVER" "$@"

echo "üèÅ Qwen inference server stopped"