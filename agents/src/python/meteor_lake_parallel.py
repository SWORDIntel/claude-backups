#!/usr/bin/env python3
"""
Intel Meteor Lake Optimized Parallel Processing - Enhanced Edition
Advanced scheduling with work stealing, NUMA awareness, and hardware counters
Optimized for Dell Latitude 5450 MIL-SPEC
"""

import asyncio
import multiprocessing as mp
import threading
import concurrent.futures
import queue
import psutil
import os
import time
import numpy as np
import ctypes
import struct
import signal
import functools
import weakref
from typing import Any, Callable, List, Optional, Dict, Tuple, Union, Set
from dataclasses import dataclass, field
from enum import Enum, IntEnum
from collections import defaultdict, deque
import logging
from threading import Lock, RLock, Condition
import sched
import heapq

# Try to import Intel-specific libraries
try:
    import py3nvml.py3nvml as nvml  # For GPU monitoring if available
    HAS_NVML = True
except ImportError:
    HAS_NVML = False

logger = logging.getLogger(__name__)

# Hardware performance counter support (Linux)
try:
    import perf
    HAS_PERF = True
except ImportError:
    HAS_PERF = False


class CoreType(IntEnum):
    """Enhanced core type classification"""
    P_CORE_ULTRA = 0    # Ultra performance cores (11, 14, 15, 16)
    P_CORE = 1          # Standard P-cores (0-10, excluding ultra)
    E_CORE = 2          # Efficiency cores (12-19)
    LP_E_CORE = 3       # Low power E-cores (20-21)
    
    @classmethod
    def from_core_id(cls, core_id: int) -> 'CoreType':
        """Determine core type from ID"""
        if core_id in [11, 14, 15, 16]:
            return cls.P_CORE_ULTRA
        elif core_id < 12:
            return cls.P_CORE
        elif core_id < 20:
            return cls.E_CORE
        else:
            return cls.LP_E_CORE


class TaskPriority(IntEnum):
    """Task priority levels"""
    REALTIME = 0
    CRITICAL = 1
    HIGH = 2
    NORMAL = 3
    LOW = 4
    IDLE = 5


@dataclass
class CoreTopology:
    """Accurate Meteor Lake core topology"""
    p_cores_ultra: List[int] = field(default_factory=lambda: [11, 14, 15, 16])
    p_cores_standard: List[int] = field(default_factory=lambda: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
    e_cores: List[int] = field(default_factory=lambda: list(range(12, 20)))
    lp_e_cores: List[int] = field(default_factory=lambda: [20, 21])
    
    @property
    def all_p_cores(self) -> List[int]:
        return self.p_cores_standard + self.p_cores_ultra
    
    @property
    def all_cores(self) -> List[int]:
        return self.all_p_cores + self.e_cores + self.lp_e_cores
    
    def get_cores_by_type(self, core_type: CoreType) -> List[int]:
        """Get core IDs by type"""
        if core_type == CoreType.P_CORE_ULTRA:
            return self.p_cores_ultra
        elif core_type == CoreType.P_CORE:
            return self.p_cores_standard
        elif core_type == CoreType.E_CORE:
            return self.e_cores
        else:
            return self.lp_e_cores


@dataclass
class TaskProfile:
    """Enhanced task profiling information"""
    task_id: str
    function: Callable
    args: tuple = ()
    kwargs: dict = field(default_factory=dict)
    priority: TaskPriority = TaskPriority.NORMAL
    core_affinity: Optional[CoreType] = None
    expected_duration_ms: float = 0.0
    memory_requirement_mb: int = 0
    cpu_intensive: bool = False
    io_intensive: bool = False
    gpu_capable: bool = False
    deadline: Optional[float] = None
    dependencies: Set[str] = field(default_factory=set)
    
    # Runtime statistics
    execution_count: int = 0
    total_duration_ms: float = 0.0
    avg_duration_ms: float = 0.0
    last_core_id: Optional[int] = None
    last_execution_time: Optional[float] = None


class WorkStealingQueue:
    """Lock-free work stealing queue implementation"""
    
    def __init__(self, capacity: int = 1024):
        self.capacity = capacity
        self.tasks = deque(maxlen=capacity)
        self.lock = RLock()
        self.not_empty = Condition(self.lock)
        self.steal_count = 0
    
    def push(self, task: TaskProfile) -> bool:
        """Push task to local end"""
        with self.lock:
            if len(self.tasks) < self.capacity:
                self.tasks.append(task)
                self.not_empty.notify()
                return True
            return False
    
    def pop(self) -> Optional[TaskProfile]:
        """Pop task from local end"""
        with self.lock:
            if self.tasks:
                return self.tasks.pop()
            return None
    
    def steal(self) -> Optional[TaskProfile]:
        """Steal task from remote end"""
        with self.lock:
            if self.tasks:
                self.steal_count += 1
                return self.tasks.popleft()
            return None
    
    def size(self) -> int:
        """Get queue size"""
        with self.lock:
            return len(self.tasks)


class HardwareMonitor:
    """Monitor hardware performance counters"""
    
    def __init__(self):
        self.cpu_temps = {}
        self.cpu_freqs = {}
        self.power_usage = {}
        self.memory_bandwidth = 0
        self.cache_misses = {}
        self.last_update = 0
        self.update_interval = 1.0  # seconds
    
    def update_metrics(self):
        """Update hardware metrics"""
        current_time = time.time()
        if current_time - self.last_update < self.update_interval:
            return
        
        try:
            # CPU temperatures
            temps = psutil.sensors_temperatures()
            if 'coretemp' in temps:
                for entry in temps['coretemp']:
                    if entry.label.startswith('Core'):
                        core_id = int(entry.label.split()[1])
                        self.cpu_temps[core_id] = entry.current
            
            # CPU frequencies
            cpu_freq = psutil.cpu_freq(percpu=True)
            for i, freq in enumerate(cpu_freq):
                self.cpu_freqs[i] = freq.current
            
            # Memory bandwidth (approximate)
            mem = psutil.virtual_memory()
            self.memory_bandwidth = mem.percent
            
            self.last_update = current_time
            
        except Exception as e:
            logger.debug(f"Hardware monitoring error: {e}")
    
    def get_coolest_core(self, core_type: CoreType, topology: CoreTopology) -> Optional[int]:
        """Get the coolest core of specified type"""
        cores = topology.get_cores_by_type(core_type)
        if not cores:
            return None
        
        coolest = min(cores, key=lambda c: self.cpu_temps.get(c, 100))
        return coolest
    
    def is_thermal_throttling(self, core_id: int, threshold: float = 85.0) -> bool:
        """Check if core is thermal throttling"""
        return self.cpu_temps.get(core_id, 0) > threshold


class PowerGovernor:
    """Power efficiency governor for task scheduling"""
    
    def __init__(self):
        self.power_mode = "balanced"  # "performance", "balanced", "power_saver"
        self.power_budget_watts = 45  # Meteor Lake typical TDP
        self.current_power = 0
        self.efficiency_scores = defaultdict(float)
    
    def should_use_p_cores(self, task: TaskProfile) -> bool:
        """Decide if task should use P-cores based on power budget"""
        if self.power_mode == "performance":
            return True
        elif self.power_mode == "power_saver":
            return task.priority <= TaskPriority.CRITICAL
        else:  # balanced
            # Use P-cores for high priority or CPU-intensive tasks
            return task.priority <= TaskPriority.HIGH or task.cpu_intensive
    
    def update_efficiency_score(self, core_id: int, task: TaskProfile, duration_ms: float):
        """Update core efficiency score"""
        # Simple efficiency metric: tasks per watt
        estimated_power = 5 if core_id >= 12 else 15  # E-cores use less power
        efficiency = 1000.0 / (duration_ms * estimated_power)
        
        # Exponential moving average
        alpha = 0.1
        self.efficiency_scores[core_id] = (
            alpha * efficiency + (1 - alpha) * self.efficiency_scores[core_id]
        )
    
    def get_most_efficient_core(self, cores: List[int]) -> Optional[int]:
        """Get most power-efficient core from list"""
        if not cores:
            return None
        
        return max(cores, key=lambda c: self.efficiency_scores.get(c, 0))


class EnhancedMeteorLakeScheduler:
    """Advanced task scheduler with work stealing and power management"""
    
    def __init__(self, enable_work_stealing: bool = True, enable_power_management: bool = True):
        self.topology = CoreTopology()
        self.enable_work_stealing = enable_work_stealing
        self.enable_power_management = enable_power_management
        
        # Work queues per core
        self.work_queues = {
            core_id: WorkStealingQueue() 
            for core_id in self.topology.all_cores
        }
        
        # Global priority queue for unassigned tasks
        self.global_queue = []  # Min heap
        self.global_queue_lock = RLock()
        
        # Worker threads
        self.workers = {}
        self.workers_running = False
        
        # Monitoring and governance
        self.hw_monitor = HardwareMonitor()
        self.power_governor = PowerGovernor()
        
        # Task tracking
        self.active_tasks = {}
        self.completed_tasks = {}
        self.task_dependencies = defaultdict(set)
        
        # Performance metrics
        self.metrics = {
            'tasks_submitted': 0,
            'tasks_completed': 0,
            'tasks_failed': 0,
            'total_execution_time_ms': 0,
            'work_steals': 0,
            'queue_depths': defaultdict(list),
            'core_utilization': defaultdict(float)
        }
        
        # NUMA settings
        self.numa_nodes = self._detect_numa_nodes()
        
        # Start scheduler
        self.scheduler_thread = threading.Thread(target=self._scheduler_loop, daemon=True)
        self.scheduler_thread.start()
    
    def _detect_numa_nodes(self) -> Dict[int, List[int]]:
        """Detect NUMA topology"""
        numa_nodes = defaultdict(list)
        
        try:
            # Simplified NUMA detection
            # P-cores typically on NUMA node 0
            for core in self.topology.all_p_cores:
                numa_nodes[0].append(core)
            
            # E-cores might be on different node
            for core in self.topology.e_cores + self.topology.lp_e_cores:
                numa_nodes[1].append(core)
                
        except Exception:
            # Fallback: all cores on single node
            numa_nodes[0] = self.topology.all_cores
        
        return dict(numa_nodes)
    
    def start_workers(self):
        """Start worker threads with optimal affinity"""
        self.workers_running = True
        
        # Create workers for each core
        for core_id in self.topology.all_cores:
            worker = threading.Thread(
                target=self._worker_loop,
                args=(core_id,),
                name=f"worker-core-{core_id}",
                daemon=True
            )
            self.workers[core_id] = worker
            worker.start()
        
        logger.info(f"Started {len(self.workers)} worker threads")
    
    def stop_workers(self):
        """Stop all worker threads"""
        self.workers_running = False
        
        # Wait for workers to finish
        for worker in self.workers.values():
            worker.join(timeout=5)
        
        self.workers.clear()
    
    def _set_thread_affinity(self, core_id: int):
        """Set thread CPU affinity with error handling"""
        try:
            # Linux-specific CPU affinity
            os.sched_setaffinity(0, {core_id})
            
            # Set scheduling policy for real-time tasks
            if core_id in self.topology.p_cores_ultra:
                # Higher priority for ultra cores
                os.nice(-5)
        except Exception as e:
            logger.debug(f"Could not set affinity for core {core_id}: {e}")
    
    def _worker_loop(self, core_id: int):
        """Enhanced worker loop with work stealing"""
        self._set_thread_affinity(core_id)
        core_type = CoreType.from_core_id(core_id)
        local_queue = self.work_queues[core_id]
        
        consecutive_steals = 0
        max_consecutive_steals = 3
        
        while self.workers_running:
            task = None
            
            # Try to get task from local queue
            task = local_queue.pop()
            
            # If no local task, try work stealing
            if task is None and self.enable_work_stealing:
                task = self._steal_work(core_id, core_type)
                
                if task:
                    consecutive_steals += 1
                    self.metrics['work_steals'] += 1
                else:
                    consecutive_steals = 0
            
            # If still no task, check global queue
            if task is None:
                task = self._get_from_global_queue(core_type)
            
            if task:
                # Execute task
                self._execute_task(task, core_id)
                
                # Update metrics
                self.metrics['core_utilization'][core_id] += 1
            else:
                # No work available, sleep briefly
                if consecutive_steals >= max_consecutive_steals:
                    # Back off if too many failed steals
                    time.sleep(0.01)
                else:
                    time.sleep(0.001)
    
    def _steal_work(self, thief_core: int, thief_type: CoreType) -> Optional[TaskProfile]:
        """Steal work from other cores"""
        # Prefer stealing from same core type
        victim_cores = self.topology.get_cores_by_type(thief_type)
        
        # Remove self from victims
        victim_cores = [c for c in victim_cores if c != thief_core]
        
        # Sort by queue size (steal from busiest)
        victim_cores.sort(key=lambda c: self.work_queues[c].size(), reverse=True)
        
        # Try to steal from busiest victim
        for victim in victim_cores[:3]:  # Check top 3 busiest
            victim_queue = self.work_queues[victim]
            if victim_queue.size() > 1:  # Don't steal last task
                task = victim_queue.steal()
                if task:
                    return task
        
        # If no same-type victim, try other types
        if thief_type == CoreType.E_CORE:
            # E-cores can steal from P-cores for load balancing
            for victim in self.topology.all_p_cores:
                victim_queue = self.work_queues[victim]
                if victim_queue.size() > 2:  # Higher threshold
                    task = victim_queue.steal()
                    if task and not task.cpu_intensive:
                        return task
                    elif task:
                        # Put it back if CPU intensive
                        victim_queue.push(task)
        
        return None
    
    def _get_from_global_queue(self, core_type: CoreType) -> Optional[TaskProfile]:
        """Get task from global priority queue"""
        with self.global_queue_lock:
            if not self.global_queue:
                return None
            
            # Find suitable task for core type
            for i, (priority, task_id, task) in enumerate(self.global_queue):
                if self._is_task_suitable_for_core(task, core_type):
                    # Remove and return task
                    self.global_queue.pop(i)
                    heapq.heapify(self.global_queue)
                    return task
            
            # If no suitable task found and queue is large, take any task
            if len(self.global_queue) > 10:
                _, _, task = heapq.heappop(self.global_queue)
                return task
        
        return None
    
    def _is_task_suitable_for_core(self, task: TaskProfile, core_type: CoreType) -> bool:
        """Check if task is suitable for core type"""
        # CPU intensive tasks prefer P-cores
        if task.cpu_intensive:
            return core_type in [CoreType.P_CORE, CoreType.P_CORE_ULTRA]
        
        # I/O intensive tasks prefer E-cores
        if task.io_intensive:
            return core_type in [CoreType.E_CORE, CoreType.LP_E_CORE]
        
        # High priority tasks prefer P-cores
        if task.priority <= TaskPriority.HIGH:
            return core_type in [CoreType.P_CORE, CoreType.P_CORE_ULTRA]
        
        # Low priority tasks can run anywhere
        return True
    
    def _execute_task(self, task: TaskProfile, core_id: int):
        """Execute task with monitoring"""
        start_time = time.perf_counter()
        
        # Update hardware metrics
        self.hw_monitor.update_metrics()
        
        # Check thermal throttling
        if self.hw_monitor.is_thermal_throttling(core_id):
            # Move task to cooler core
            cooler_core = self._find_cooler_core(core_id)
            if cooler_core is not None:
                self.work_queues[cooler_core].push(task)
                return
        
        # Mark task as active
        self.active_tasks[task.task_id] = {
            'core_id': core_id,
            'start_time': start_time,
            'task': task
        }
        
        try:
            # Execute the task
            result = task.function(*task.args, **task.kwargs)
            
            # Task completed successfully
            duration_ms = (time.perf_counter() - start_time) * 1000
            
            # Update task profile statistics
            task.execution_count += 1
            task.total_duration_ms += duration_ms
            task.avg_duration_ms = task.total_duration_ms / task.execution_count
            task.last_core_id = core_id
            task.last_execution_time = time.time()
            
            # Update power governor
            if self.enable_power_management:
                self.power_governor.update_efficiency_score(core_id, task, duration_ms)
            
            # Store result
            self.completed_tasks[task.task_id] = {
                'result': result,
                'duration_ms': duration_ms,
                'core_id': core_id
            }
            
            # Update metrics
            self.metrics['tasks_completed'] += 1
            self.metrics['total_execution_time_ms'] += duration_ms
            
            # Check dependencies
            self._handle_task_completion(task.task_id)
            
        except Exception as e:
            logger.error(f"Task {task.task_id} failed on core {core_id}: {e}")
            
            self.completed_tasks[task.task_id] = {
                'error': str(e),
                'core_id': core_id
            }
            
            self.metrics['tasks_failed'] += 1
            
        finally:
            # Remove from active tasks
            self.active_tasks.pop(task.task_id, None)
    
    def _find_cooler_core(self, hot_core: int) -> Optional[int]:
        """Find a cooler core of the same type"""
        core_type = CoreType.from_core_id(hot_core)
        return self.hw_monitor.get_coolest_core(core_type, self.topology)
    
    def _handle_task_completion(self, completed_task_id: str):
        """Handle task dependencies after completion"""
        # Release dependent tasks
        dependent_tasks = self.task_dependencies.get(completed_task_id, set())
        
        for dep_task_id in dependent_tasks:
            # Check if all dependencies are satisfied
            # This is simplified - in production, track remaining dependencies
            pass
    
    def _scheduler_loop(self):
        """Main scheduler loop for load balancing"""
        while self.workers_running:
            try:
                # Update hardware monitoring
                self.hw_monitor.update_metrics()
                
                # Collect queue depths
                for core_id, queue in self.work_queues.items():
                    self.metrics['queue_depths'][core_id].append(queue.size())
                
                # Perform load balancing
                self._load_balance()
                
                # Sleep before next iteration
                time.sleep(0.1)
                
            except Exception as e:
                logger.error(f"Scheduler loop error: {e}")
    
    def _load_balance(self):
        """Perform load balancing across cores"""
        # Calculate average queue depth per core type
        avg_depths = defaultdict(float)
        counts = defaultdict(int)
        
        for core_id, queue in self.work_queues.items():
            core_type = CoreType.from_core_id(core_id)
            avg_depths[core_type] += queue.size()
            counts[core_type] += 1
        
        for core_type in avg_depths:
            if counts[core_type] > 0:
                avg_depths[core_type] /= counts[core_type]
        
        # Rebalance if needed
        for core_id, queue in self.work_queues.items():
            core_type = CoreType.from_core_id(core_id)
            
            if queue.size() > avg_depths[core_type] * 2:
                # This queue is overloaded, redistribute
                excess = int(queue.size() - avg_depths[core_type])
                
                for _ in range(excess):
                    task = queue.steal()
                    if task:
                        # Find less loaded core of same type
                        target_core = self._find_least_loaded_core(core_type)
                        if target_core != core_id:
                            self.work_queues[target_core].push(task)
    
    def _find_least_loaded_core(self, core_type: CoreType) -> int:
        """Find least loaded core of specified type"""
        cores = self.topology.get_cores_by_type(core_type)
        if not cores:
            return 0
        
        return min(cores, key=lambda c: self.work_queues[c].size())
    
    def submit_task(
        self,
        function: Callable,
        args: tuple = (),
        kwargs: dict = None,
        priority: TaskPriority = TaskPriority.NORMAL,
        core_affinity: Optional[CoreType] = None,
        cpu_intensive: bool = False,
        io_intensive: bool = False,
        deadline: Optional[float] = None,
        dependencies: Set[str] = None
    ) -> str:
        """Submit task with advanced scheduling hints"""
        
        task_id = f"task_{self.metrics['tasks_submitted']}_{time.time()}"
        
        task = TaskProfile(
            task_id=task_id,
            function=function,
            args=args,
            kwargs=kwargs or {},
            priority=priority,
            core_affinity=core_affinity,
            cpu_intensive=cpu_intensive,
            io_intensive=io_intensive,
            deadline=deadline,
            dependencies=dependencies or set()
        )
        
        self.metrics['tasks_submitted'] += 1
        
        # Determine target core based on affinity and power management
        if core_affinity:
            # User specified core type
            target_cores = self.topology.get_cores_by_type(core_affinity)
        elif self.enable_power_management:
            # Let power governor decide
            if self.power_governor.should_use_p_cores(task):
                target_cores = self.topology.all_p_cores
            else:
                target_cores = self.topology.e_cores
        else:
            # Default: use all cores
            target_cores = self.topology.all_cores
        
        # Find best core from candidates
        if target_cores:
            if self.enable_power_management:
                target_core = self.power_governor.get_most_efficient_core(target_cores)
            else:
                target_core = self._find_least_loaded_core(
                    CoreType.from_core_id(target_cores[0])
                )
            
            # Add to specific core's queue
            if target_core is not None:
                success = self.work_queues[target_core].push(task)
                if success:
                    return task_id
        
        # Fallback: add to global queue
        with self.global_queue_lock:
            heapq.heappush(self.global_queue, (priority, task_id, task))
        
        return task_id
    
    def wait_for_task(self, task_id: str, timeout: Optional[float] = None) -> Any:
        """Wait for task completion and return result"""
        start_time = time.time()
        
        while task_id not in self.completed_tasks:
            if timeout and (time.time() - start_time) > timeout:
                raise TimeoutError(f"Task {task_id} timed out")
            
            time.sleep(0.001)
        
        result = self.completed_tasks[task_id]
        
        if 'error' in result:
            raise Exception(result['error'])
        
        return result.get('result')
    
    async def parallel_execute(
        self,
        tasks: List[Dict[str, Any]],
        max_concurrency: Optional[int] = None
    ) -> List[Any]:
        """Execute tasks in parallel with optimal scheduling"""
        
        task_ids = []
        
        for task_info in tasks:
            task_id = self.submit_task(
                function=task_info['function'],
                args=task_info.get('args', ()),
                kwargs=task_info.get('kwargs', {}),
                priority=TaskPriority[task_info.get('priority', 'NORMAL')],
                cpu_intensive=task_info.get('cpu_intensive', False),
                io_intensive=task_info.get('io_intensive', False)
            )
            task_ids.append(task_id)
        
        # Wait for all tasks to complete
        results = []
        for task_id in task_ids:
            try:
                result = self.wait_for_task(task_id, timeout=30)
                results.append(result)
            except Exception as e:
                results.append(f"ERROR: {e}")
        
        return results
    
    def get_metrics(self) -> Dict[str, Any]:
        """Get comprehensive scheduler metrics"""
        
        # Calculate averages
        avg_queue_depths = {}
        for core_id, depths in self.metrics['queue_depths'].items():
            if depths:
                avg_queue_depths[core_id] = sum(depths) / len(depths)
        
        # Core utilization
        total_time = time.time()  # Should track actual runtime
        core_utilization = {
            core_id: (count / total_time) * 100
            for core_id, count in self.metrics['core_utilization'].items()
        }
        
        return {
            'tasks_submitted': self.metrics['tasks_submitted'],
            'tasks_completed': self.metrics['tasks_completed'],
            'tasks_failed': self.metrics['tasks_failed'],
            'tasks_active': len(self.active_tasks),
            'avg_execution_time_ms': (
                self.metrics['total_execution_time_ms'] / max(self.metrics['tasks_completed'], 1)
            ),
            'work_steals': self.metrics['work_steals'],
            'avg_queue_depths': avg_queue_depths,
            'core_utilization_percent': core_utilization,
            'power_mode': self.power_governor.power_mode,
            'cpu_temperatures': self.hw_monitor.cpu_temps,
            'efficiency_scores': dict(self.power_governor.efficiency_scores)
        }


# Global scheduler instance
meteor_lake_scheduler = EnhancedMeteorLakeScheduler(
    enable_work_stealing=True,
    enable_power_management=True
)
meteor_lake_scheduler.start_workers()


class ParallelOptimizer:
    """High-level parallel processing optimizer with advanced features"""
    
    def __init__(self, scheduler: Optional[EnhancedMeteorLakeScheduler] = None):
        self.scheduler = scheduler or meteor_lake_scheduler
    
    async def map_reduce(
        self,
        map_func: Callable,
        reduce_func: Callable,
        data: List[Any],
        chunk_size: Optional[int] = None
    ) -> Any:
        """Parallel map-reduce operation"""
        
        # Determine optimal chunk size
        if chunk_size is None:
            num_cores = len(self.scheduler.topology.all_cores)
            chunk_size = max(1, len(data) // (num_cores * 4))
        
        # Create map tasks
        map_tasks = []
        for i in range(0, len(data), chunk_size):
            chunk = data[i:i + chunk_size]
            map_tasks.append({
                'function': lambda c=chunk: [map_func(item) for item in c],
                'cpu_intensive': True,
                'priority': 'HIGH'
            })
        
        # Execute map phase
        map_results = await self.scheduler.parallel_execute(map_tasks)
        
        # Flatten results
        flattened = []
        for result in map_results:
            if isinstance(result, list):
                flattened.extend(result)
            else:
                flattened.append(result)
        
        # Reduce phase
        while len(flattened) > 1:
            reduce_tasks = []
            
            for i in range(0, len(flattened), 2):
                if i + 1 < len(flattened):
                    reduce_tasks.append({
                        'function': lambda a=flattened[i], b=flattened[i+1]: reduce_func(a, b),
                        'cpu_intensive': True,
                        'priority': 'HIGH'
                    })
                else:
                    reduce_tasks.append({
                        'function': lambda a=flattened[i]: a,
                        'cpu_intensive': False,
                        'priority': 'NORMAL'
                    })
            
            flattened = await self.scheduler.parallel_execute(reduce_tasks)
        
        return flattened[0] if flattened else None
    
    async def pipeline(
        self,
        stages: List[Callable],
        data: List[Any],
        buffer_size: int = 10
    ) -> List[Any]:
        """Execute pipeline of operations"""
        
        # Create pipeline stages
        pipeline_queues = [asyncio.Queue(maxsize=buffer_size) for _ in range(len(stages) + 1)]
        
        # Input data
        for item in data:
            await pipeline_queues[0].put(item)
        
        # Sentinel to mark end
        await pipeline_queues[0].put(None)
        
        async def stage_worker(stage_idx: int, func: Callable):
            input_queue = pipeline_queues[stage_idx]
            output_queue = pipeline_queues[stage_idx + 1]
            
            while True:
                item = await input_queue.get()
                if item is None:
                    await output_queue.put(None)
                    break
                
                # Process item
                result = func(item)
                await output_queue.put(result)
        
        # Start stage workers
        stage_tasks = [
            asyncio.create_task(stage_worker(i, func))
            for i, func in enumerate(stages)
        ]
        
        # Collect results
        results = []
        output_queue = pipeline_queues[-1]
        
        while True:
            item = await output_queue.get()
            if item is None:
                break
            results.append(item)
        
        # Wait for all stages to complete
        await asyncio.gather(*stage_tasks)
        
        return results


# Example usage
async def example_usage():
    """Example of enhanced parallel processing"""
    
    # Create scheduler with custom settings
    scheduler = EnhancedMeteorLakeScheduler(
        enable_work_stealing=True,
        enable_power_management=True
    )
    scheduler.start_workers()
    
    # Submit various types of tasks
    
    # CPU-intensive task on P-cores
    cpu_task_id = scheduler.submit_task(
        function=lambda: sum(i*i for i in range(1000000)),
        priority=TaskPriority.HIGH,
        core_affinity=CoreType.P_CORE_ULTRA,
        cpu_intensive=True
    )
    
    # I/O-intensive task on E-cores
    io_task_id = scheduler.submit_task(
        function=lambda: time.sleep(0.1),
        priority=TaskPriority.LOW,
        core_affinity=CoreType.E_CORE,
        io_intensive=True
    )
    
    # Wait for results
    cpu_result = scheduler.wait_for_task(cpu_task_id)
    io_result = scheduler.wait_for_task(io_task_id)
    
    print(f"CPU task result: {cpu_result}")
    print(f"I/O task result: {io_result}")
    
    # Use parallel optimizer for map-reduce
    optimizer = ParallelOptimizer(scheduler)
    
    data = list(range(100))
    result = await optimizer.map_reduce(
        map_func=lambda x: x * x,
        reduce_func=lambda a, b: a + b,
        data=data
    )
    
    print(f"Map-reduce result: {result}")
    
    # Get metrics
    metrics = scheduler.get_metrics()
    print(f"Scheduler metrics: {metrics}")
    
    # Stop scheduler
    scheduler.stop_workers()


if __name__ == "__main__":
    import asyncio
    asyncio.run(example_usage())
