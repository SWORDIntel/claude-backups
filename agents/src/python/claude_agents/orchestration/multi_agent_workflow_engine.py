#!/usr/bin/env python3
"""
Multi-Agent Workflow Engine v1.0
DAG-based workflow execution with NPU optimization and smart batching
Provides advanced workflow orchestration for 50K+ ops/sec performance
"""

import asyncio
import json
import logging
import sys
import time
import uuid
from collections import defaultdict, deque
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple, Union

import networkx as nx
import numpy as np

# Add parent directory to path for imports
sys.path.append(str(Path(__file__).parent.parent.parent))

try:
    from agent_coordination_matrix import AgentCapability, ExecutionMode

    from agents.src.python.enhanced_coordination_matrix import (
        EnhancedCoordinationMatrix,
        EnhancedCoordinationPlan,
        ExecutionMetrics,
        OptimizationStrategy,
        WorkflowComplexity,
    )
    from agents.src.python.npu_coordination_bridge import (
        NPUCoordinationBridge,
        NPUWorkflowTask,
        WorkflowPriority,
    )
except ImportError as e:
    logging.warning(f"Import error: {e}. Some features may be limited.")

logger = logging.getLogger(__name__)


class WorkflowStatus(Enum):
    """Workflow execution status"""

    CREATED = "created"
    QUEUED = "queued"
    RUNNING = "running"
    PAUSED = "paused"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


class TaskStatus(Enum):
    """Individual task status"""

    PENDING = "pending"
    READY = "ready"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    SKIPPED = "skipped"


class BatchingStrategy(Enum):
    """Batching strategies for optimization"""

    NO_BATCHING = "none"
    CAPABILITY_BASED = "capability"
    AGENT_BASED = "agent"
    TIME_BASED = "time"
    RESOURCE_BASED = "resource"
    INTELLIGENT = "intelligent"


@dataclass
class WorkflowTask:
    """Individual task within a workflow"""

    task_id: str
    agent_name: str
    description: str
    dependencies: Set[str] = field(default_factory=set)
    status: TaskStatus = TaskStatus.PENDING
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    retries: int = 0
    max_retries: int = 3
    timeout_ms: int = 30000
    priority: int = 50  # 1-100, higher = more priority
    batch_group: Optional[str] = None


@dataclass
class WorkflowDefinition:
    """Complete workflow definition with DAG structure"""

    workflow_id: str
    name: str
    description: str
    tasks: Dict[str, WorkflowTask]
    optimization_strategy: OptimizationStrategy = (
        OptimizationStrategy.THROUGHPUT_OPTIMIZED
    )
    batching_strategy: BatchingStrategy = BatchingStrategy.INTELLIGENT
    max_parallel_tasks: int = 10
    total_timeout_ms: int = 300000  # 5 minutes
    priority: WorkflowPriority = WorkflowPriority.NORMAL
    created_at: datetime = field(default_factory=datetime.utcnow)


@dataclass
class BatchExecution:
    """Batch execution configuration"""

    batch_id: str
    tasks: List[WorkflowTask]
    agent_name: str
    batch_description: str
    estimated_duration_ms: float
    resource_requirements: Dict[str, float]
    npu_accelerated: bool = False


@dataclass
class WorkflowExecutionState:
    """Current execution state of a workflow"""

    workflow_id: str
    status: WorkflowStatus
    current_stage: int
    completed_tasks: Set[str]
    failed_tasks: Set[str]
    running_tasks: Set[str]
    pending_tasks: Set[str]
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    execution_stages: List[List[str]] = field(default_factory=list)
    batch_executions: List[BatchExecution] = field(default_factory=list)
    performance_metrics: Dict[str, Any] = field(default_factory=dict)


class WorkflowDAGAnalyzer:
    """Analyzes workflow DAG for optimization opportunities"""

    def __init__(self):
        self.graph_cache = {}

    def analyze_workflow_dag(self, workflow: WorkflowDefinition) -> Dict[str, Any]:
        """Analyze workflow DAG structure and identify optimization opportunities"""

        # Create networkx graph
        graph = nx.DiGraph()

        # Add nodes (tasks)
        for task_id, task in workflow.tasks.items():
            graph.add_node(task_id, task=task)

        # Add edges (dependencies)
        for task_id, task in workflow.tasks.items():
            for dep_id in task.dependencies:
                if dep_id in workflow.tasks:
                    graph.add_edge(dep_id, task_id)

        # Analyze graph properties
        analysis = {
            "is_dag": nx.is_directed_acyclic_graph(graph),
            "node_count": graph.number_of_nodes(),
            "edge_count": graph.number_of_edges(),
            "longest_path": 0,
            "parallelism_levels": [],
            "critical_path": [],
            "optimization_opportunities": [],
        }

        if analysis["is_dag"]:
            # Calculate longest path (critical path)
            try:
                analysis["longest_path"] = nx.dag_longest_path_length(graph)
                analysis["critical_path"] = nx.dag_longest_path(graph)
            except:
                pass

            # Calculate parallelism levels
            analysis["parallelism_levels"] = self._calculate_parallelism_levels(graph)

            # Identify optimization opportunities
            analysis["optimization_opportunities"] = (
                self._identify_optimization_opportunities(graph, workflow)
            )

        return analysis

    def _calculate_parallelism_levels(self, graph: nx.DiGraph) -> List[List[str]]:
        """Calculate parallelism levels for DAG execution"""
        levels = []
        remaining_nodes = set(graph.nodes())

        while remaining_nodes:
            # Find nodes with no remaining dependencies
            current_level = []
            for node in list(remaining_nodes):
                if not any(
                    pred in remaining_nodes for pred in graph.predecessors(node)
                ):
                    current_level.append(node)

            if not current_level:
                # Break cycles if any
                current_level = list(remaining_nodes)

            levels.append(current_level)
            remaining_nodes -= set(current_level)

        return levels

    def _identify_optimization_opportunities(
        self, graph: nx.DiGraph, workflow: WorkflowDefinition
    ) -> List[str]:
        """Identify optimization opportunities in the workflow"""
        opportunities = []

        # Check for batching opportunities
        agent_tasks = defaultdict(list)
        for task_id, task in workflow.tasks.items():
            agent_tasks[task.agent_name].append(task_id)

        for agent, tasks in agent_tasks.items():
            if len(tasks) > 1:
                opportunities.append(f"Batch {len(tasks)} tasks for agent {agent}")

        # Check for parallel execution opportunities
        parallelism_levels = self._calculate_parallelism_levels(graph)
        max_parallel = (
            max(len(level) for level in parallelism_levels) if parallelism_levels else 0
        )

        if max_parallel > 1:
            opportunities.append(
                f"Maximum parallelism: {max_parallel} concurrent tasks"
            )

        # Check for redundant dependencies
        if graph.number_of_edges() > graph.number_of_nodes():
            opportunities.append("Potential redundant dependencies detected")

        return opportunities


class SmartBatchingEngine:
    """Intelligent batching engine for optimizing task execution"""

    def __init__(self):
        self.batching_algorithms = {
            BatchingStrategy.CAPABILITY_BASED: self._batch_by_capability,
            BatchingStrategy.AGENT_BASED: self._batch_by_agent,
            BatchingStrategy.TIME_BASED: self._batch_by_time,
            BatchingStrategy.RESOURCE_BASED: self._batch_by_resource,
            BatchingStrategy.INTELLIGENT: self._intelligent_batching,
        }

    async def create_batches(
        self,
        tasks: List[WorkflowTask],
        strategy: BatchingStrategy,
        max_batch_size: int = 5,
    ) -> List[BatchExecution]:
        """Create optimized batches from tasks"""

        if strategy == BatchingStrategy.NO_BATCHING:
            # Create individual batches for each task
            return [
                BatchExecution(
                    batch_id=f"single_{task.task_id}",
                    tasks=[task],
                    agent_name=task.agent_name,
                    batch_description=task.description,
                    estimated_duration_ms=30000,  # Default estimate
                    resource_requirements={"cpu": 0.1, "memory": 100},
                )
                for task in tasks
            ]

        # Use appropriate batching algorithm
        algorithm = self.batching_algorithms.get(strategy, self._intelligent_batching)
        return await algorithm(tasks, max_batch_size)

    async def _batch_by_capability(
        self, tasks: List[WorkflowTask], max_batch_size: int
    ) -> List[BatchExecution]:
        """Batch tasks by agent capabilities"""
        # Group by agent (simplified capability grouping)
        agent_groups = defaultdict(list)
        for task in tasks:
            agent_groups[task.agent_name].append(task)

        batches = []
        for agent, agent_tasks in agent_groups.items():
            # Split into batches of max_batch_size
            for i in range(0, len(agent_tasks), max_batch_size):
                batch_tasks = agent_tasks[i : i + max_batch_size]
                batch = BatchExecution(
                    batch_id=f"cap_{agent}_{i // max_batch_size}",
                    tasks=batch_tasks,
                    agent_name=agent,
                    batch_description=f"Capability batch for {agent}",
                    estimated_duration_ms=len(batch_tasks) * 5000,
                    resource_requirements=self._estimate_batch_resources(batch_tasks),
                )
                batches.append(batch)

        return batches

    async def _batch_by_agent(
        self, tasks: List[WorkflowTask], max_batch_size: int
    ) -> List[BatchExecution]:
        """Batch tasks by agent type"""
        return await self._batch_by_capability(tasks, max_batch_size)

    async def _batch_by_time(
        self, tasks: List[WorkflowTask], max_batch_size: int
    ) -> List[BatchExecution]:
        """Batch tasks by estimated execution time"""
        # Sort by priority (higher priority first)
        sorted_tasks = sorted(tasks, key=lambda t: t.priority, reverse=True)

        batches = []
        current_batch = []
        current_duration = 0
        max_duration = 60000  # 60 seconds max per batch

        for task in sorted_tasks:
            estimated_task_time = 5000  # 5 seconds default

            if (
                len(current_batch) >= max_batch_size
                or current_duration + estimated_task_time > max_duration
            ) and current_batch:

                # Create batch
                batch = BatchExecution(
                    batch_id=f"time_{len(batches)}",
                    tasks=current_batch[:],
                    agent_name=(
                        current_batch[0].agent_name if current_batch else "mixed"
                    ),
                    batch_description=f"Time-based batch {len(batches)}",
                    estimated_duration_ms=current_duration,
                    resource_requirements=self._estimate_batch_resources(current_batch),
                )
                batches.append(batch)

                current_batch = []
                current_duration = 0

            current_batch.append(task)
            current_duration += estimated_task_time

        # Add remaining tasks
        if current_batch:
            batch = BatchExecution(
                batch_id=f"time_{len(batches)}",
                tasks=current_batch,
                agent_name=current_batch[0].agent_name,
                batch_description=f"Time-based batch {len(batches)}",
                estimated_duration_ms=current_duration,
                resource_requirements=self._estimate_batch_resources(current_batch),
            )
            batches.append(batch)

        return batches

    async def _batch_by_resource(
        self, tasks: List[WorkflowTask], max_batch_size: int
    ) -> List[BatchExecution]:
        """Batch tasks by resource requirements"""
        # Simplified resource-based batching
        light_tasks = []  # Low resource tasks
        heavy_tasks = []  # High resource tasks

        for task in tasks:
            # Classify by agent type (simplified)
            if any(
                word in task.agent_name.lower() for word in ["director", "orchestrator"]
            ):
                light_tasks.append(task)
            else:
                heavy_tasks.append(task)

        batches = []

        # Create batches for light tasks
        for i in range(0, len(light_tasks), max_batch_size):
            batch_tasks = light_tasks[i : i + max_batch_size]
            batch = BatchExecution(
                batch_id=f"light_{i // max_batch_size}",
                tasks=batch_tasks,
                agent_name="mixed_light",
                batch_description=f"Light resource batch",
                estimated_duration_ms=len(batch_tasks) * 2000,
                resource_requirements={
                    "cpu": 0.1 * len(batch_tasks),
                    "memory": 50 * len(batch_tasks),
                },
            )
            batches.append(batch)

        # Create batches for heavy tasks (smaller batches)
        heavy_batch_size = max(1, max_batch_size // 2)
        for i in range(0, len(heavy_tasks), heavy_batch_size):
            batch_tasks = heavy_tasks[i : i + heavy_batch_size]
            batch = BatchExecution(
                batch_id=f"heavy_{i // heavy_batch_size}",
                tasks=batch_tasks,
                agent_name="mixed_heavy",
                batch_description=f"Heavy resource batch",
                estimated_duration_ms=len(batch_tasks) * 10000,
                resource_requirements={
                    "cpu": 0.3 * len(batch_tasks),
                    "memory": 200 * len(batch_tasks),
                },
            )
            batches.append(batch)

        return batches

    async def _intelligent_batching(
        self, tasks: List[WorkflowTask], max_batch_size: int
    ) -> List[BatchExecution]:
        """Intelligent batching using multiple criteria"""
        # Analyze tasks for optimal batching
        agent_groups = defaultdict(list)
        priority_groups = defaultdict(list)

        for task in tasks:
            agent_groups[task.agent_name].append(task)
            priority_groups[task.priority].append(task)

        batches = []

        # Priority-based grouping first
        for priority in sorted(priority_groups.keys(), reverse=True):
            priority_tasks = priority_groups[priority]

            # Within same priority, group by agent
            agent_task_groups = defaultdict(list)
            for task in priority_tasks:
                agent_task_groups[task.agent_name].append(task)

            # Create optimal batches
            for agent, agent_tasks in agent_task_groups.items():
                # Determine optimal batch size for agent
                optimal_batch_size = self._calculate_optimal_batch_size(
                    agent, len(agent_tasks)
                )

                for i in range(0, len(agent_tasks), optimal_batch_size):
                    batch_tasks = agent_tasks[i : i + optimal_batch_size]

                    batch = BatchExecution(
                        batch_id=f"intel_{agent}_{priority}_{i // optimal_batch_size}",
                        tasks=batch_tasks,
                        agent_name=agent,
                        batch_description=f"Intelligent batch for {agent} (priority {priority})",
                        estimated_duration_ms=self._estimate_batch_duration(
                            batch_tasks
                        ),
                        resource_requirements=self._estimate_batch_resources(
                            batch_tasks
                        ),
                        npu_accelerated=self._is_npu_beneficial(
                            agent, len(batch_tasks)
                        ),
                    )
                    batches.append(batch)

        return batches

    def _calculate_optimal_batch_size(self, agent: str, total_tasks: int) -> int:
        """Calculate optimal batch size for agent"""
        # Agent-specific optimal batch sizes
        if any(word in agent.lower() for word in ["director", "orchestrator"]):
            return min(10, total_tasks)  # Coordinators can handle larger batches
        elif "security" in agent.lower():
            return min(3, total_tasks)  # Security needs focused attention
        elif any(word in agent.lower() for word in ["debug", "test"]):
            return min(2, total_tasks)  # Analysis tasks need smaller batches
        else:
            return min(5, total_tasks)  # Default batch size

    def _estimate_batch_duration(self, tasks: List[WorkflowTask]) -> float:
        """Estimate batch execution duration"""
        base_duration = 5000  # 5 seconds per task

        # Parallel efficiency factor
        if len(tasks) > 1:
            efficiency = 0.8  # 80% efficiency for parallel execution
            return base_duration * len(tasks) * efficiency
        else:
            return base_duration

    def _estimate_batch_resources(self, tasks: List[WorkflowTask]) -> Dict[str, float]:
        """Estimate resource requirements for batch"""
        base_cpu = 0.1
        base_memory = 100

        return {"cpu": base_cpu * len(tasks), "memory": base_memory * len(tasks)}

    def _is_npu_beneficial(self, agent: str, task_count: int) -> bool:
        """Determine if NPU acceleration would benefit this batch"""
        # NPU beneficial for pattern recognition, analysis, selection tasks
        npu_agents = ["security", "architect", "optimizer", "monitor", "analyst"]
        return any(word in agent.lower() for word in npu_agents) and task_count >= 2


class MultiAgentWorkflowEngine:
    """Advanced workflow engine with DAG execution and NPU optimization"""

    def __init__(self):
        self.coordination_matrix: Optional[EnhancedCoordinationMatrix] = None
        self.npu_bridge: Optional[NPUCoordinationBridge] = None

        # Core components
        self.dag_analyzer = WorkflowDAGAnalyzer()
        self.batching_engine = SmartBatchingEngine()

        # Execution tracking
        self.active_workflows: Dict[str, WorkflowExecutionState] = {}
        self.workflow_queue = asyncio.Queue()

        # Performance metrics
        self.metrics = {
            "workflows_executed": 0,
            "total_tasks_executed": 0,
            "avg_workflow_duration_ms": 0,
            "throughput_ops_sec": 0,
            "npu_accelerated_workflows": 0,
            "batch_efficiency_percent": 0,
            "start_time": time.time(),
        }

        # Configuration
        self.max_concurrent_workflows = 5
        self.max_parallel_tasks_per_workflow = 10

    async def initialize(self):
        """Initialize the workflow engine"""
        logger.info("ðŸš€ Initializing Multi-Agent Workflow Engine")

        # Initialize coordination matrix
        try:
            self.coordination_matrix = EnhancedCoordinationMatrix()
            await self.coordination_matrix.initialize()
            logger.info("âœ… Enhanced Coordination Matrix initialized")
        except Exception as e:
            logger.error(f"Coordination matrix initialization failed: {e}")
            return False

        # Initialize NPU bridge
        try:
            from agents.src.python.npu_coordination_bridge import npu_bridge

            self.npu_bridge = npu_bridge
            if (
                not hasattr(self.npu_bridge, "npu_orchestrator")
                or self.npu_bridge.npu_orchestrator is None
            ):
                await self.npu_bridge.initialize()
            logger.info("âœ… NPU Bridge connected")
        except Exception as e:
            logger.warning(f"NPU bridge connection failed: {e}")
            self.npu_bridge = None

        # Start workflow processor
        asyncio.create_task(self._workflow_processor())

        logger.info("ðŸŽ¯ Multi-Agent Workflow Engine ready")
        return True

    async def create_workflow_from_description(
        self,
        description: str,
        workflow_name: str = None,
        optimization_strategy: OptimizationStrategy = OptimizationStrategy.THROUGHPUT_OPTIMIZED,
        batching_strategy: BatchingStrategy = BatchingStrategy.INTELLIGENT,
    ) -> WorkflowDefinition:
        """Create workflow definition from natural language description"""

        workflow_id = str(uuid.uuid4())
        if not workflow_name:
            workflow_name = f"Auto-generated workflow {workflow_id[:8]}"

        # Analyze description to identify required tasks and agents
        tasks = await self._analyze_description_to_tasks(description)

        # Create workflow definition
        workflow = WorkflowDefinition(
            workflow_id=workflow_id,
            name=workflow_name,
            description=description,
            tasks=tasks,
            optimization_strategy=optimization_strategy,
            batching_strategy=batching_strategy,
        )

        # Analyze DAG for optimization
        dag_analysis = self.dag_analyzer.analyze_workflow_dag(workflow)
        logger.info(f"Workflow DAG analysis: {dag_analysis}")

        return workflow

    async def _analyze_description_to_tasks(
        self, description: str
    ) -> Dict[str, WorkflowTask]:
        """Analyze description to extract tasks and dependencies"""
        tasks = {}

        # Simple heuristic-based task extraction
        # In production, this would use NLP and the NPU for better analysis

        description_lower = description.lower()

        # Identify key activities and map to agents
        task_mapping = [
            # Security tasks
            (
                "security",
                ["audit", "secure", "vulnerability", "penetration", "crypto"],
                "SECURITY",
            ),
            (
                "security_audit",
                ["security audit", "security review"],
                "SECURITYAUDITOR",
            ),
            # Development tasks
            (
                "architecture",
                ["architect", "design", "structure", "blueprint"],
                "ARCHITECT",
            ),
            (
                "construction",
                ["build", "implement", "create", "construct"],
                "CONSTRUCTOR",
            ),
            ("debugging", ["debug", "fix", "troubleshoot", "resolve"], "DEBUGGER"),
            ("testing", ["test", "validate", "verify", "qa"], "TESTBED"),
            (
                "optimization",
                ["optimize", "improve", "enhance", "performance"],
                "OPTIMIZER",
            ),
            # Deployment tasks
            ("deployment", ["deploy", "release", "publish", "launch"], "DEPLOYER"),
            ("monitoring", ["monitor", "observe", "track", "watch"], "MONITOR"),
            # Coordination tasks
            ("planning", ["plan", "strategy", "roadmap", "coordinate"], "DIRECTOR"),
            (
                "orchestration",
                ["orchestrate", "manage", "coordinate"],
                "PROJECTORCHESTRATOR",
            ),
        ]

        task_counter = 1
        dependencies = {}

        for task_type, keywords, agent in task_mapping:
            if any(keyword in description_lower for keyword in keywords):
                task_id = f"task_{task_counter:03d}_{task_type}"

                # Simple dependency logic
                deps = set()
                if task_type == "construction" and "task_001_architecture" in tasks:
                    deps.add("task_001_architecture")
                elif task_type == "testing" and any(
                    "construction" in t or "debugging" in t for t in tasks.keys()
                ):
                    construction_tasks = [
                        t
                        for t in tasks.keys()
                        if "construction" in t or "debugging" in t
                    ]
                    deps.update(construction_tasks)
                elif task_type == "deployment" and any(
                    "testing" in t for t in tasks.keys()
                ):
                    test_tasks = [t for t in tasks.keys() if "testing" in t]
                    deps.update(test_tasks)
                elif task_type == "monitoring" and any(
                    "deployment" in t for t in tasks.keys()
                ):
                    deploy_tasks = [t for t in tasks.keys() if "deployment" in t]
                    deps.update(deploy_tasks)

                task = WorkflowTask(
                    task_id=task_id,
                    agent_name=agent,
                    description=f"{task_type.title()} task: {' '.join(keywords[:2])}",
                    dependencies=deps,
                    priority=self._calculate_task_priority(task_type),
                )

                tasks[task_id] = task
                task_counter += 1

        # Ensure we have at least one task
        if not tasks:
            task_id = "task_001_default"
            tasks[task_id] = WorkflowTask(
                task_id=task_id,
                agent_name="DIRECTOR",
                description="Default coordination task",
                priority=80,
            )

        return tasks

    def _calculate_task_priority(self, task_type: str) -> int:
        """Calculate task priority based on type"""
        priorities = {
            "planning": 90,
            "architecture": 80,
            "security": 85,
            "construction": 70,
            "testing": 60,
            "deployment": 50,
            "monitoring": 40,
            "orchestration": 95,
        }
        return priorities.get(task_type, 50)

    async def execute_workflow(self, workflow: WorkflowDefinition) -> Dict[str, Any]:
        """Execute workflow with DAG-based parallel processing"""
        logger.info(f"ðŸš€ Executing workflow: {workflow.name}")

        # Create execution state
        execution_state = WorkflowExecutionState(
            workflow_id=workflow.workflow_id,
            status=WorkflowStatus.QUEUED,
            current_stage=0,
            completed_tasks=set(),
            failed_tasks=set(),
            running_tasks=set(),
            pending_tasks=set(workflow.tasks.keys()),
        )

        self.active_workflows[workflow.workflow_id] = execution_state

        try:
            # Add to execution queue
            await self.workflow_queue.put(workflow)

            # Wait for completion
            while execution_state.status in [
                WorkflowStatus.QUEUED,
                WorkflowStatus.RUNNING,
            ]:
                await asyncio.sleep(0.1)

            # Return final results
            return await self._get_workflow_results(workflow.workflow_id)

        except Exception as e:
            logger.error(f"Workflow execution failed: {e}")
            execution_state.status = WorkflowStatus.FAILED
            return {
                "workflow_id": workflow.workflow_id,
                "status": WorkflowStatus.FAILED.value,
                "error": str(e),
            }

    async def _workflow_processor(self):
        """Background processor for workflow execution"""
        while True:
            try:
                # Get next workflow from queue
                workflow = await self.workflow_queue.get()

                # Execute workflow
                await self._execute_workflow_internal(workflow)

                # Mark task as done
                self.workflow_queue.task_done()

            except Exception as e:
                logger.error(f"Workflow processor error: {e}")
                await asyncio.sleep(1)

    async def _execute_workflow_internal(self, workflow: WorkflowDefinition):
        """Internal workflow execution with DAG processing"""
        execution_state = self.active_workflows[workflow.workflow_id]
        execution_state.status = WorkflowStatus.RUNNING
        execution_state.start_time = datetime.utcnow()

        try:
            # Analyze DAG structure
            dag_analysis = self.dag_analyzer.analyze_workflow_dag(workflow)

            if not dag_analysis["is_dag"]:
                raise RuntimeError("Workflow contains cycles and is not a valid DAG")

            execution_stages = dag_analysis["parallelism_levels"]
            execution_state.execution_stages = execution_stages

            # Execute each stage
            for stage_idx, stage_tasks in enumerate(execution_stages):
                execution_state.current_stage = stage_idx
                logger.info(
                    f"Executing stage {stage_idx + 1}/{len(execution_stages)}: {stage_tasks}"
                )

                # Get task objects for this stage
                stage_task_objects = [
                    workflow.tasks[task_id]
                    for task_id in stage_tasks
                    if task_id in workflow.tasks
                ]

                # Create batches for this stage
                batches = await self.batching_engine.create_batches(
                    stage_task_objects,
                    workflow.batching_strategy,
                    workflow.max_parallel_tasks,
                )

                execution_state.batch_executions.extend(batches)

                # Execute batches in parallel
                batch_tasks = []
                for batch in batches:
                    batch_coro = self._execute_batch(batch, workflow)
                    batch_tasks.append(batch_coro)

                # Wait for all batches in stage to complete
                batch_results = await asyncio.gather(
                    *batch_tasks, return_exceptions=True
                )

                # Process batch results
                for batch, result in zip(batches, batch_results):
                    if isinstance(result, Exception):
                        logger.error(f"Batch {batch.batch_id} failed: {result}")
                        for task in batch.tasks:
                            task.status = TaskStatus.FAILED
                            task.error = str(result)
                            execution_state.failed_tasks.add(task.task_id)
                    else:
                        for task in batch.tasks:
                            task.status = TaskStatus.COMPLETED
                            task.result = result
                            execution_state.completed_tasks.add(task.task_id)

                # Update pending tasks
                execution_state.pending_tasks -= {
                    task.task_id for task in stage_task_objects
                }

            # Mark workflow as completed
            execution_state.status = WorkflowStatus.COMPLETED
            execution_state.end_time = datetime.utcnow()

            # Update metrics
            self._update_workflow_metrics(workflow, execution_state)

            logger.info(f"âœ… Workflow {workflow.name} completed successfully")

        except Exception as e:
            logger.error(f"Workflow execution failed: {e}")
            execution_state.status = WorkflowStatus.FAILED
            execution_state.end_time = datetime.utcnow()

    async def _execute_batch(
        self, batch: BatchExecution, workflow: WorkflowDefinition
    ) -> Dict[str, Any]:
        """Execute a batch of tasks"""
        logger.info(f"Executing batch {batch.batch_id} with {len(batch.tasks)} tasks")

        start_time = time.perf_counter()

        if self.coordination_matrix:
            # Create combined task description for the batch
            batch_description = f"Batch execution: {batch.batch_description}"

            # Execute using coordination matrix
            try:
                # Create enhanced coordination plan
                plan = await self.coordination_matrix.create_enhanced_coordination_plan(
                    batch_description,
                    ExecutionMode.PARALLEL,
                    workflow.optimization_strategy,
                )

                # Execute plan
                results = (
                    await self.coordination_matrix.execute_enhanced_coordination_plan(
                        plan, batch_description
                    )
                )

                execution_time = (time.perf_counter() - start_time) * 1000

                return {
                    "batch_id": batch.batch_id,
                    "success": results.get("success", False),
                    "execution_time_ms": execution_time,
                    "task_results": results.get("results", {}),
                    "npu_accelerated": batch.npu_accelerated,
                }

            except Exception as e:
                logger.error(f"Batch execution failed: {e}")
                return {
                    "batch_id": batch.batch_id,
                    "success": False,
                    "error": str(e),
                    "execution_time_ms": (time.perf_counter() - start_time) * 1000,
                }
        else:
            # Fallback to simple execution
            await asyncio.sleep(0.1)  # Simulate execution
            return {
                "batch_id": batch.batch_id,
                "success": True,
                "execution_time_ms": 100,
                "simulated": True,
            }

    def _update_workflow_metrics(
        self, workflow: WorkflowDefinition, execution_state: WorkflowExecutionState
    ):
        """Update global workflow metrics"""
        self.metrics["workflows_executed"] += 1
        self.metrics["total_tasks_executed"] += len(workflow.tasks)

        if execution_state.start_time and execution_state.end_time:
            duration_ms = (
                execution_state.end_time - execution_state.start_time
            ).total_seconds() * 1000

            # Update average duration
            current_avg = self.metrics["avg_workflow_duration_ms"]
            workflow_count = self.metrics["workflows_executed"]
            self.metrics["avg_workflow_duration_ms"] = (
                current_avg * (workflow_count - 1) + duration_ms
            ) / workflow_count

        # Update throughput
        runtime = time.time() - self.metrics["start_time"]
        if runtime > 0:
            self.metrics["throughput_ops_sec"] = (
                self.metrics["total_tasks_executed"] / runtime
            )

        # Track NPU acceleration
        if any(batch.npu_accelerated for batch in execution_state.batch_executions):
            self.metrics["npu_accelerated_workflows"] += 1

    async def _get_workflow_results(self, workflow_id: str) -> Dict[str, Any]:
        """Get comprehensive workflow results"""
        if workflow_id not in self.active_workflows:
            return {"error": "Workflow not found"}

        execution_state = self.active_workflows[workflow_id]

        # Calculate performance metrics
        duration_ms = 0
        if execution_state.start_time and execution_state.end_time:
            duration_ms = (
                execution_state.end_time - execution_state.start_time
            ).total_seconds() * 1000

        return {
            "workflow_id": workflow_id,
            "status": execution_state.status.value,
            "duration_ms": duration_ms,
            "completed_tasks": len(execution_state.completed_tasks),
            "failed_tasks": len(execution_state.failed_tasks),
            "total_tasks": len(execution_state.completed_tasks)
            + len(execution_state.failed_tasks)
            + len(execution_state.pending_tasks),
            "success_rate": len(execution_state.completed_tasks)
            / max(
                len(execution_state.completed_tasks)
                + len(execution_state.failed_tasks),
                1,
            ),
            "execution_stages": len(execution_state.execution_stages),
            "batch_count": len(execution_state.batch_executions),
            "npu_accelerated": any(
                batch.npu_accelerated for batch in execution_state.batch_executions
            ),
            "performance_metrics": execution_state.performance_metrics,
        }

    def get_engine_metrics(self) -> Dict[str, Any]:
        """Get comprehensive engine performance metrics"""
        runtime = time.time() - self.metrics["start_time"]

        return {
            "runtime_seconds": runtime,
            "workflows_executed": self.metrics["workflows_executed"],
            "total_tasks_executed": self.metrics["total_tasks_executed"],
            "avg_workflow_duration_ms": self.metrics["avg_workflow_duration_ms"],
            "throughput_ops_sec": self.metrics["throughput_ops_sec"],
            "npu_accelerated_workflows": self.metrics["npu_accelerated_workflows"],
            "active_workflows": len(self.active_workflows),
            "queue_size": self.workflow_queue.qsize(),
            "workflows_per_second": self.metrics["workflows_executed"]
            / max(runtime, 0.001),
            "npu_utilization_percent": (
                self.metrics["npu_accelerated_workflows"]
                / max(self.metrics["workflows_executed"], 1)
            )
            * 100,
        }


# Global workflow engine instance
workflow_engine = MultiAgentWorkflowEngine()


async def execute_workflow_from_description(
    description: str,
    workflow_name: str = None,
    optimization_strategy: str = "throughput",
    batching_strategy: str = "intelligent",
) -> Dict[str, Any]:
    """Execute workflow from natural language description"""

    # Initialize engine if needed
    if not workflow_engine.coordination_matrix:
        await workflow_engine.initialize()

    # Parse strategies
    opt_strategy_map = {
        "latency": OptimizationStrategy.LATENCY_OPTIMIZED,
        "throughput": OptimizationStrategy.THROUGHPUT_OPTIMIZED,
        "balanced": OptimizationStrategy.RESOURCE_BALANCED,
        "quality": OptimizationStrategy.QUALITY_FOCUSED,
        "cost": OptimizationStrategy.COST_OPTIMIZED,
    }

    batch_strategy_map = {
        "none": BatchingStrategy.NO_BATCHING,
        "capability": BatchingStrategy.CAPABILITY_BASED,
        "agent": BatchingStrategy.AGENT_BASED,
        "time": BatchingStrategy.TIME_BASED,
        "resource": BatchingStrategy.RESOURCE_BASED,
        "intelligent": BatchingStrategy.INTELLIGENT,
    }

    opt_strategy = opt_strategy_map.get(
        optimization_strategy.lower(), OptimizationStrategy.THROUGHPUT_OPTIMIZED
    )
    batch_strategy = batch_strategy_map.get(
        batching_strategy.lower(), BatchingStrategy.INTELLIGENT
    )

    # Create workflow
    workflow = await workflow_engine.create_workflow_from_description(
        description, workflow_name, opt_strategy, batch_strategy
    )

    # Execute workflow
    results = await workflow_engine.execute_workflow(workflow)

    return results


if __name__ == "__main__":

    async def main():
        print("ðŸš€ Multi-Agent Workflow Engine Test")
        print("=" * 50)

        # Initialize engine
        await workflow_engine.initialize()

        # Test workflow creation and execution
        test_descriptions = [
            "Audit security vulnerabilities and then deploy with monitoring",
            "Design architecture, implement features, test thoroughly, and deploy to production",
            "Debug performance issues, optimize code, and validate improvements",
        ]

        for desc in test_descriptions:
            print(f"\nðŸ“‹ Testing: {desc}")

            start_time = time.perf_counter()
            results = await execute_workflow_from_description(desc)
            execution_time = (time.perf_counter() - start_time) * 1000

            print(f"   âœ… Completed in {execution_time:.1f}ms")
            print(f"   ðŸ“Š Success rate: {results.get('success_rate', 0):.1%}")
            print(
                f"   ðŸŽ¯ Tasks: {results.get('completed_tasks', 0)}/{results.get('total_tasks', 0)}"
            )

        # Display engine metrics
        metrics = workflow_engine.get_engine_metrics()
        print(f"\nðŸ“Š ENGINE METRICS:")
        print(f"   ðŸŽ¯ Throughput: {metrics['throughput_ops_sec']:.1f} ops/sec")
        print(f"   âš¡ Workflows/sec: {metrics['workflows_per_second']:.2f}")
        print(f"   ðŸ§  NPU utilization: {metrics['npu_utilization_percent']:.1f}%")

    asyncio.run(main())
