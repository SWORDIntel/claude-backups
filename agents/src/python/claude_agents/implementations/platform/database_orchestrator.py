#!/usr/bin/env python3
"""
DATABASE-INTEGRATED PRODUCTION ORCHESTRATOR
Enhanced ProductionOrchestrator with PostgreSQL 17 integration
Transforms from mock mode to production-ready persistent orchestration
"""

import asyncio
import json
import logging
import time
import uuid
from datetime import datetime
from typing import Any, Dict, List, Optional

import asyncpg
from claude_agents.orchestration.agent_registry import get_registry
from claude_agents.orchestration.production_orchestrator import (
    CommandSet,
    CommandStep,
    ExecutionMode,
    ProductionOrchestrator,
)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DatabaseIntegratedOrchestrator(ProductionOrchestrator):
    """
    Production orchestrator with PostgreSQL 17 persistence
    Provides immediate benefits:
    - Persistent workflow state and recovery
    - Multi-user authentication and RBAC
    - Comprehensive audit logging
    - Real-time performance metrics
    - Session management across restarts
    """

    def __init__(self, db_config: Dict[str, str] = None):
        super().__init__()

        # Database configuration
        self.db_config = db_config or {
            "host": "localhost",
            "port": 5432,
            "user": "claude_auth",
            "password": "claude_auth_password123",
            "database": "claude_auth",
        }

        self.db_pool = None
        self.mock_mode = False  # Disable mock mode - we have real persistence!

        # Enhanced metrics with database backing
        self.db_metrics = {
            "persistent_workflows": 0,
            "user_sessions": 0,
            "security_events": 0,
            "database_latency_ms": 0.0,
        }

    async def initialize(self) -> bool:
        """Initialize with PostgreSQL 17 connection pool"""
        logger.info("Initializing Database-Integrated Orchestrator...")

        try:
            # Create optimized connection pool for high-performance orchestration
            self.db_pool = await asyncpg.create_pool(
                host=self.db_config["host"],
                port=self.db_config["port"],
                user=self.db_config["user"],
                password=self.db_config["password"],
                database=self.db_config["database"],
                min_size=20,  # Higher min for orchestration workloads
                max_size=100,  # Support massive concurrent orchestration
                command_timeout=5,  # Fast timeouts for responsiveness
                server_settings={
                    "jit": "on",  # PostgreSQL 17 JIT compilation
                    "max_parallel_workers_per_gather": "6",  # Enhanced parallel processing
                },
            )

            logger.info(
                "âœ… PostgreSQL 17 connection pool initialized (20-100 connections)"
            )

            # Create orchestration tables if they don't exist
            await self.create_orchestration_tables()

            # Recover any interrupted workflows
            recovered_workflows = await self.recover_workflows()
            if recovered_workflows:
                logger.info(
                    f"âœ… Recovered {len(recovered_workflows)} interrupted workflows"
                )

            # Initialize performance monitoring
            await self.initialize_performance_monitoring()

            # Update metrics
            await self.update_database_metrics()

            return await super().initialize()

        except Exception as e:
            logger.error(f"âŒ Database initialization failed: {e}")
            return False

    async def create_orchestration_tables(self):
        """Create tables for orchestration persistence"""
        async with self.db_pool.acquire() as conn:

            # Workflow state persistence
            await conn.execute(
                """
                CREATE TABLE IF NOT EXISTS orchestration_workflows (
                    workflow_id VARCHAR(64) PRIMARY KEY,
                    name VARCHAR(256) NOT NULL,
                    type VARCHAR(32) NOT NULL,
                    mode VARCHAR(32) NOT NULL,
                    priority INTEGER NOT NULL,
                    status VARCHAR(32) DEFAULT 'pending',
                    state JSONB DEFAULT JSON_OBJECT(),  -- PostgreSQL 17 JSON constructor
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                    started_at TIMESTAMP WITH TIME ZONE,
                    completed_at TIMESTAMP WITH TIME ZONE,
                    user_id UUID REFERENCES users(user_id),
                    
                    -- Performance tracking
                    estimated_duration FLOAT,
                    actual_duration FLOAT,
                    steps_completed INTEGER DEFAULT 0,
                    steps_total INTEGER DEFAULT 0
                );
            """
            )

            # Agent performance tracking
            await conn.execute(
                """
                CREATE TABLE IF NOT EXISTS agent_performance_metrics (
                    metric_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
                    agent_name VARCHAR(64) NOT NULL,
                    action VARCHAR(128) NOT NULL,
                    duration_ms FLOAT NOT NULL,
                    input_size INTEGER,
                    output_size INTEGER,
                    success BOOLEAN NOT NULL,
                    error_message TEXT,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                    workflow_id VARCHAR(64) REFERENCES orchestration_workflows(workflow_id),
                    user_id UUID REFERENCES users(user_id)
                );
            """
            )

            # Workflow step tracking
            await conn.execute(
                """
                CREATE TABLE IF NOT EXISTS workflow_steps (
                    step_id VARCHAR(64) PRIMARY KEY,
                    workflow_id VARCHAR(64) NOT NULL REFERENCES orchestration_workflows(workflow_id),
                    agent_name VARCHAR(64) NOT NULL,
                    action VARCHAR(128) NOT NULL,
                    payload JSONB DEFAULT JSON_OBJECT(),  -- PostgreSQL 17 JSON constructor
                    result JSONB DEFAULT JSON_OBJECT(),   -- PostgreSQL 17 JSON constructor
                    status VARCHAR(32) DEFAULT 'pending',
                    started_at TIMESTAMP WITH TIME ZONE,
                    completed_at TIMESTAMP WITH TIME ZONE,
                    error_message TEXT,
                    can_fail BOOLEAN DEFAULT FALSE,
                    dependencies TEXT[] DEFAULT '{}'
                );
            """
            )

            # Create indexes for performance
            await conn.execute(
                """
                CREATE INDEX IF NOT EXISTS idx_workflows_status_created 
                    ON orchestration_workflows(status, created_at);
                CREATE INDEX IF NOT EXISTS idx_agent_metrics_name_time 
                    ON agent_performance_metrics(agent_name, created_at);
                CREATE INDEX IF NOT EXISTS idx_workflow_steps_workflow 
                    ON workflow_steps(workflow_id, status);
            """
            )

            logger.info("âœ… Orchestration persistence tables ready")

    async def execute_command_set(
        self, command_set: CommandSet, user_id: str = None
    ) -> Dict[str, Any]:
        """Execute command set with full database persistence and monitoring"""

        start_time = time.time()

        # Store workflow start in database
        await self.store_workflow_start(command_set, user_id)
        logger.info(f"ðŸš€ Starting workflow '{command_set.name}' (ID: {command_set.id})")

        try:
            # Execute workflow with database tracking
            result = await self.execute_workflow_with_persistence(command_set, user_id)

            # Store successful completion
            duration = time.time() - start_time
            await self.store_workflow_completion(
                command_set.id, result, "completed", duration
            )

            # Log security event
            await self.log_security_event(
                "workflow_completed",
                user_id or "system",
                command_set.name,
                f"Duration: {duration:.2f}s",
            )

            # Update metrics
            await self.update_database_metrics()

            logger.info(
                f"âœ… Workflow '{command_set.name}' completed in {duration:.2f}s"
            )
            return result

        except Exception as e:
            # Store failure with full context
            duration = time.time() - start_time
            await self.store_workflow_completion(
                command_set.id, str(e), "failed", duration
            )

            # Log security event
            await self.log_security_event(
                "workflow_failed",
                user_id or "system",
                command_set.name,
                f"Error: {str(e)}",
            )

            logger.error(f"âŒ Workflow '{command_set.name}' failed: {e}")
            raise

    async def store_workflow_start(self, command_set: CommandSet, user_id: str):
        """Store workflow start in database"""
        async with self.db_pool.acquire() as conn:
            await conn.execute(
                """
                INSERT INTO orchestration_workflows 
                (workflow_id, name, type, mode, priority, status, state, user_id, 
                 estimated_duration, steps_total, started_at)
                VALUES ($1, $2, $3, $4, $5, 'running', $6, $7, $8, $9, NOW())
            """,
                command_set.id,
                command_set.name,
                command_set.type.value,
                command_set.mode.value,
                command_set.priority.value,
                json.dumps({"steps": [step.id for step in command_set.steps]}),
                user_id,
                command_set.timeout,
                len(command_set.steps),
            )

            # Store workflow steps with proper UUID generation
            for i, step in enumerate(command_set.steps):
                # Generate unique step ID if needed
                step_id = f"{command_set.id}_step_{i}_{str(uuid.uuid4())[:8]}"

                await conn.execute(
                    """
                    INSERT INTO workflow_steps 
                    (step_id, workflow_id, agent_name, action, payload, can_fail, dependencies)
                    VALUES ($1, $2, $3, $4, $5, $6, $7)
                    ON CONFLICT (step_id) DO UPDATE SET
                        status = 'pending', started_at = NULL, completed_at = NULL
                """,
                    step_id,
                    command_set.id,
                    step.agent,
                    step.action,
                    json.dumps(step.payload),
                    step.can_fail,
                    command_set.dependencies.get(step.id, []),
                )

    async def execute_workflow_with_persistence(
        self, command_set: CommandSet, user_id: str
    ) -> Dict[str, Any]:
        """Execute workflow with database persistence"""
        results = {}

        # Get step IDs from database for proper tracking
        async with self.db_pool.acquire() as conn:
            db_steps = await conn.fetch(
                """
                SELECT step_id, agent_name, action, payload 
                FROM workflow_steps 
                WHERE workflow_id = $1 
                ORDER BY step_id
            """,
                command_set.id,
            )

        for i, step in enumerate(command_set.steps):
            # Use database-generated step ID
            db_step_id = (
                db_steps[i]["step_id"]
                if i < len(db_steps)
                else f"{command_set.id}_step_{i}_{str(uuid.uuid4())[:8]}"
            )

            # Mark step as started
            await self.mark_step_started(db_step_id)

            # Execute step with timing
            step_start = time.time()
            try:
                # This would normally call the actual agent via Task tool
                # For now, simulate with database logging
                result = await self.simulate_agent_execution(step, user_id)

                step_duration = time.time() - step_start

                # Store step completion
                await self.store_step_result(
                    db_step_id, result, "completed", step_duration
                )

                # Track agent performance
                await self.track_agent_performance(
                    step.agent,
                    step.action,
                    step_duration * 1000,
                    len(str(result)),
                    True,
                    command_set.id,
                    user_id,
                )

                results[db_step_id] = result
                logger.info(
                    f"âœ… Step '{step.agent}.{step.action}' completed in {step_duration:.2f}s"
                )

            except Exception as e:
                step_duration = time.time() - step_start

                # Store step failure
                await self.store_step_result(
                    db_step_id, str(e), "failed", step_duration
                )

                # Track agent performance (failure)
                await self.track_agent_performance(
                    step.agent,
                    step.action,
                    step_duration * 1000,
                    len(str(e)),
                    False,
                    command_set.id,
                    user_id,
                )

                if not step.can_fail:
                    await self.mark_workflow_failed(command_set.id, str(e))
                    raise

                logger.warning(
                    f"âš ï¸  Step '{step.agent}.{step.action}' failed but marked as optional"
                )
                results[db_step_id] = {"error": str(e), "can_fail": True}

        return results

    async def simulate_agent_execution(
        self, step: CommandStep, user_id: str
    ) -> Dict[str, Any]:
        """Simulate agent execution with database integration"""

        # This demonstrates how the database integration works
        # In production, this would call: Task(subagent_type=step.agent, prompt=step.action)

        # Log the agent invocation
        await self.log_security_event(
            "agent_invocation",
            user_id or "system",
            step.agent,
            f"Action: {step.action}",
        )

        # Simulate processing time based on agent complexity
        processing_time = {
            "director": 2.0,
            "architect": 3.0,
            "security": 1.5,
            "constructor": 2.5,
            "testbed": 1.0,
            "docgen": 1.5,
        }.get(step.agent, 1.0)

        await asyncio.sleep(processing_time)

        return {
            "agent": step.agent,
            "action": step.action,
            "status": "completed",
            "output": f"Simulated result from {step.agent}",
            "duration": processing_time,
            "timestamp": datetime.now().isoformat(),
        }

    async def track_agent_performance(
        self,
        agent_name: str,
        action: str,
        duration_ms: float,
        output_size: int,
        success: bool,
        workflow_id: str,
        user_id: str,
    ):
        """Track agent performance in database"""
        async with self.db_pool.acquire() as conn:
            await conn.execute(
                """
                INSERT INTO agent_performance_metrics 
                (agent_name, action, duration_ms, output_size, success, workflow_id, user_id)
                VALUES ($1, $2, $3, $4, $5, $6, $7)
            """,
                agent_name,
                action,
                duration_ms,
                output_size,
                success,
                workflow_id,
                user_id,
            )

    async def log_security_event(
        self, event_type: str, user_id: str, description: str, details: str
    ):
        """Log security events to database"""
        async with self.db_pool.acquire() as conn:
            await conn.execute(
                """
                INSERT INTO security_events 
                (event_type, user_id, description, details, severity, timestamp)
                VALUES ($1, $2, $3, $4, 2, NOW())
            """,
                event_type,
                user_id,
                description,
                json.dumps({"details": details, "source": "orchestrator"}),
            )

    async def get_orchestration_dashboard(self) -> Dict[str, Any]:
        """Get real-time orchestration metrics"""
        async with self.db_pool.acquire() as conn:
            # Get workflow statistics
            workflow_stats = await conn.fetchrow(
                """
                SELECT 
                    COUNT(*) as total_workflows,
                    COUNT(*) FILTER (WHERE status = 'running') as active_workflows,
                    COUNT(*) FILTER (WHERE status = 'completed') as completed_workflows,
                    COUNT(*) FILTER (WHERE status = 'failed') as failed_workflows,
                    AVG(actual_duration) as avg_duration,
                    COUNT(*) FILTER (WHERE created_at >= NOW() - INTERVAL '1 hour') as workflows_last_hour
                FROM orchestration_workflows
                WHERE created_at >= NOW() - INTERVAL '24 hours'
            """
            )

            # Get agent performance statistics
            agent_stats = await conn.fetch(
                """
                SELECT 
                    agent_name,
                    COUNT(*) as invocations,
                    AVG(duration_ms) as avg_duration_ms,
                    COUNT(*) FILTER (WHERE success = true) as successful_invocations,
                    COUNT(*) FILTER (WHERE created_at >= NOW() - INTERVAL '1 hour') as invocations_last_hour
                FROM agent_performance_metrics
                WHERE created_at >= NOW() - INTERVAL '24 hours'
                GROUP BY agent_name
                ORDER BY invocations DESC
                LIMIT 10
            """
            )

            # Get recent security events
            recent_events = await conn.fetchval(
                """
                SELECT COUNT(*) FROM security_events 
                WHERE timestamp >= NOW() - INTERVAL '1 hour'
            """
            )

            return {
                "workflows": dict(workflow_stats) if workflow_stats else {},
                "top_agents": [dict(row) for row in agent_stats],
                "recent_security_events": recent_events,
                "database_status": "operational",
                "last_updated": datetime.now().isoformat(),
            }

    async def mark_step_started(self, step_id: str):
        """Mark workflow step as started"""
        async with self.db_pool.acquire() as conn:
            await conn.execute(
                """
                UPDATE workflow_steps 
                SET status = 'running', started_at = NOW()
                WHERE step_id = $1
            """,
                step_id,
            )

    async def store_step_result(
        self, step_id: str, result: Any, status: str, duration: float
    ):
        """Store step completion result"""
        async with self.db_pool.acquire() as conn:
            await conn.execute(
                """
                UPDATE workflow_steps 
                SET status = $2, result = $3, completed_at = NOW()
                WHERE step_id = $1
            """,
                step_id,
                status,
                json.dumps(result) if result else None,
            )

    async def store_workflow_completion(
        self, workflow_id: str, result: Any, status: str, duration: float
    ):
        """Store workflow completion"""
        async with self.db_pool.acquire() as conn:
            await conn.execute(
                """
                UPDATE orchestration_workflows 
                SET status = $2, completed_at = NOW(), actual_duration = $3,
                    steps_completed = (
                        SELECT COUNT(*) FROM workflow_steps 
                        WHERE workflow_id = $1 AND status = 'completed'
                    )
                WHERE workflow_id = $1
            """,
                workflow_id,
                status,
                duration,
            )

    async def recover_workflows(self) -> List[CommandSet]:
        """Recover interrupted workflows from database"""
        async with self.db_pool.acquire() as conn:
            interrupted_workflows = await conn.fetch(
                """
                SELECT workflow_id, name, type, mode, priority, state
                FROM orchestration_workflows 
                WHERE status = 'running'
                AND created_at >= NOW() - INTERVAL '24 hours'
            """
            )

            recovered = []
            for row in interrupted_workflows:
                # Create CommandSet from database state
                from claude_agents.orchestration.production_orchestrator import (
                    CommandType,
                )

                command_set = CommandSet(
                    id=row["workflow_id"],
                    name=row["name"],
                    type=CommandType(row["type"]),
                    mode=ExecutionMode(row["mode"]),
                    priority=row["priority"],
                )

                # Recover steps
                steps = await conn.fetch(
                    """
                    SELECT step_id, agent_name, action, payload, can_fail, dependencies
                    FROM workflow_steps
                    WHERE workflow_id = $1
                    ORDER BY step_id
                """,
                    row["workflow_id"],
                )

                for step_row in steps:
                    step = CommandStep(
                        id=step_row["step_id"],
                        agent=step_row["agent_name"],
                        action=step_row["action"],
                        payload=json.loads(step_row["payload"] or "{}"),
                        can_fail=step_row["can_fail"],
                    )
                    command_set.steps.append(step)

                recovered.append(command_set)

                # Mark as recovered for re-execution
                await conn.execute(
                    """
                    UPDATE orchestration_workflows 
                    SET status = 'recovered'
                    WHERE workflow_id = $1
                """,
                    row["workflow_id"],
                )

            return recovered

    async def update_database_metrics(self):
        """Update internal metrics from database"""
        async with self.db_pool.acquire() as conn:
            start_time = time.time()

            # Get database metrics
            db_stats = await conn.fetchrow(
                """
                SELECT 
                    COUNT(*) as total_workflows,
                    COUNT(*) FILTER (WHERE status = 'running') as active_workflows,
                    COUNT(DISTINCT user_id) as unique_users,
                    AVG(actual_duration) as avg_workflow_duration
                FROM orchestration_workflows
                WHERE created_at >= NOW() - INTERVAL '24 hours'
            """
            )

            db_latency = (time.time() - start_time) * 1000

            # Update metrics
            self.db_metrics.update(
                {
                    "persistent_workflows": db_stats["total_workflows"] or 0,
                    "active_workflows": db_stats["active_workflows"] or 0,
                    "unique_users": db_stats["unique_users"] or 0,
                    "avg_workflow_duration": float(
                        db_stats["avg_workflow_duration"] or 0
                    ),
                    "database_latency_ms": db_latency,
                }
            )

            # Update parent class metrics
            self.metrics.update(
                {
                    "workflows_executed": self.db_metrics["persistent_workflows"],
                    "database_integration": True,
                    "persistence_enabled": True,
                }
            )

    async def mark_workflow_failed(self, workflow_id: str, error_message: str):
        """Mark workflow as failed"""
        async with self.db_pool.acquire() as conn:
            await conn.execute(
                """
                UPDATE orchestration_workflows 
                SET status = 'failed', completed_at = NOW()
                WHERE workflow_id = $1
            """,
                workflow_id,
            )

    async def initialize_performance_monitoring(self):
        """Initialize performance monitoring"""
        # Create performance monitoring view
        async with self.db_pool.acquire() as conn:
            await conn.execute(
                """
                CREATE OR REPLACE VIEW orchestration_performance_metrics AS
                SELECT 
                    'workflow_throughput' as metric,
                    COUNT(*)::FLOAT / EXTRACT(EPOCH FROM (MAX(created_at) - MIN(created_at))) * 3600 as per_hour,
                    COUNT(*) as total_count
                FROM orchestration_workflows
                WHERE created_at >= NOW() - INTERVAL '1 hour'
                    AND status IN ('completed', 'failed')
                
                UNION ALL
                
                SELECT 
                    'agent_avg_latency' as metric,
                    AVG(duration_ms) as avg_latency_ms,
                    COUNT(*) as invocation_count
                FROM agent_performance_metrics
                WHERE created_at >= NOW() - INTERVAL '1 hour'
                
                UNION ALL
                
                SELECT
                    'success_rate' as metric,
                    COUNT(*) FILTER (WHERE status = 'completed')::FLOAT / COUNT(*) * 100 as success_percentage,
                    COUNT(*) as total_workflows
                FROM orchestration_workflows
                WHERE created_at >= NOW() - INTERVAL '1 hour';
            """
            )

            logger.info("âœ… Performance monitoring views initialized")


# Standard Workflows with Database Integration
class DatabaseStandardWorkflows:
    """Standard workflows enhanced with database persistence"""

    @staticmethod
    def create_database_demo_workflow() -> CommandSet:
        """Create a workflow to demonstrate database integration"""
        from claude_agents.orchestration.production_orchestrator import CommandType

        return CommandSet(
            name="Database Integration Demo",
            type=CommandType.WORKFLOW,
            mode=ExecutionMode.PYTHON_ONLY,
            steps=[
                CommandStep(
                    agent="director",
                    action="analyze_database_integration_benefits",
                    payload={
                        "database": "PostgreSQL 17",
                        "integration": "Python orchestrator",
                    },
                ),
                CommandStep(
                    agent="security",
                    action="validate_database_security",
                    payload={"tables": 11, "authentication": "RBAC"},
                ),
                CommandStep(
                    agent="monitor",
                    action="generate_performance_report",
                    payload={"target_throughput": ">2000 auth/sec", "latency": "<25ms"},
                ),
            ],
        )


async def demo_database_integration():
    """Demonstrate the PostgreSQL 17 integrated orchestrator"""
    print("ðŸš€ Database-Integrated Orchestration Demo")
    print("=" * 60)

    # Initialize orchestrator with database
    orchestrator = DatabaseIntegratedOrchestrator()

    if not await orchestrator.initialize():
        print("âŒ Failed to initialize database orchestrator")
        return

    print("âœ… Database-integrated orchestrator initialized")

    # Create demo workflow
    workflow = DatabaseStandardWorkflows.create_database_demo_workflow()

    # Execute with database persistence (using admin user UUID)
    admin_user_id = "07fc0dd7-90f9-43b9-b800-ac18ac0f9221"  # From database
    print(f"\nðŸ”§ Executing workflow: '{workflow.name}'")
    result = await orchestrator.execute_command_set(workflow, user_id=admin_user_id)

    print(f"\nðŸ“Š Workflow completed with {len(result)} steps")

    # Show dashboard metrics
    dashboard = await orchestrator.get_orchestration_dashboard()
    print(f"\nðŸ“ˆ Dashboard Metrics:")
    print(
        f"  Total Workflows: {dashboard.get('workflows', {}).get('total_workflows', 0)}"
    )
    print(
        f"  Active Workflows: {dashboard.get('workflows', {}).get('active_workflows', 0)}"
    )
    print(f"  Top Agents: {[a['agent_name'] for a in dashboard.get('top_agents', [])]}")
    print(f"  Recent Security Events: {dashboard.get('recent_security_events', 0)}")

    # Close database connections
    if orchestrator.db_pool:
        await orchestrator.db_pool.close()

    print("\nâœ… Database integration demo completed!")
    print(
        "ðŸ—„ï¸ All workflow state, metrics, and security events now persisted in PostgreSQL 17"
    )


if __name__ == "__main__":
    asyncio.run(demo_database_integration())
