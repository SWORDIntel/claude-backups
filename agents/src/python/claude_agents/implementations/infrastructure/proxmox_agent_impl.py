#!/usr/bin/env python3
"""
PROXMOX-AGENT Implementation v7.0.0
Virtualization management specialist for Proxmox VE
"""

import asyncio
import logging
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any, Tuple
from enum import Enum
from datetime import datetime, timedelta
import json
import random
from pathlib import Path

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger('PROXMOX-AGENT')


class HypervisorType(Enum):
    """Hypervisor types"""
    KVM_QEMU = "kvm_qemu"
    LXC = "lxc"


class StorageType(Enum):
    """Storage backend types"""
    LOCAL = "local"
    LVM = "lvm"
    LVM_THIN = "lvm-thin"
    ZFS = "zfs"
    NFS = "nfs"
    CIFS = "cifs"
    GLUSTERFS = "glusterfs"
    CEPH_RBD = "rbd"
    CEPHFS = "cephfs"


class NetworkType(Enum):
    """Network types"""
    LINUX_BRIDGE = "bridge"
    OVSWITCH = "ovs"
    VXLAN = "vxlan"
    EVPN = "evpn"


class VMState(Enum):
    """Virtual machine states"""
    RUNNING = "running"
    STOPPED = "stopped"
    SUSPENDED = "suspended"
    MIGRATING = "migrating"
    ERROR = "error"


@dataclass
class ProxmoxNode:
    """Proxmox node configuration"""
    name: str
    ip_address: str
    cpu_cores: int = 16
    memory_gb: int = 64
    storage_gb: int = 1000
    cluster_member: bool = False
    online: bool = True
    cpu_usage: float = 0.0
    memory_usage: float = 0.0
    vms: List[str] = field(default_factory=list)
    containers: List[str] = field(default_factory=list)


@dataclass
class VirtualMachine:
    """Virtual machine configuration"""
    vmid: int
    name: str
    node: str
    hypervisor: HypervisorType
    cpu_cores: int = 2
    memory_mb: int = 2048
    disk_gb: int = 32
    state: VMState = VMState.STOPPED
    os_type: str = "linux"
    network_interfaces: List[Dict[str, Any]] = field(default_factory=list)
    storage_pool: str = "local-lvm"
    ha_enabled: bool = False
    backup_enabled: bool = True


@dataclass
class StoragePool:
    """Storage pool configuration"""
    name: str
    type: StorageType
    size_gb: int
    used_gb: int = 0
    nodes: List[str] = field(default_factory=list)
    content_types: List[str] = field(default_factory=list)
    shared: bool = False
    replication_enabled: bool = False


@dataclass
class ClusterConfig:
    """Cluster configuration"""
    name: str
    nodes: List[ProxmoxNode]
    quorum: int = 2
    ha_enabled: bool = True
    fencing_enabled: bool = True
    corosync_network: str = "10.0.0.0/24"


class ProxmoxAgent:
    """Proxmox VE virtualization platform specialist"""
    
    def __init__(self):
        self.name = "PROXMOX-AGENT"
        self.version = "7.0.0"
        self.uuid = "pr0xm0x-v1r7-m4n4-g3r0-pr0xm0x0001"
        self.nodes: Dict[str, ProxmoxNode] = {}
        self.vms: Dict[int, VirtualMachine] = {}
        self.storage_pools: Dict[str, StoragePool] = {}
        self.cluster: Optional[ClusterConfig] = None
        self.next_vmid = 100
        logger.info(f"PROXMOX-AGENT v{self.version} initialized")
    
    async def add_node(self, node: ProxmoxNode) -> bool:
        """Add Proxmox node to management"""
        try:
            self.nodes[node.name] = node
            logger.info(f"Added node: {node.name} ({node.ip_address})")
            
            # Create default storage pools for node
            await self._create_default_storage(node.name)
            
            return True
        except Exception as e:
            logger.error(f"Failed to add node: {e}")
            return False
    
    async def _create_default_storage(self, node_name: str):
        """Create default storage pools for node"""
        # Local storage
        local_storage = StoragePool(
            name="local",
            type=StorageType.LOCAL,
            size_gb=100,
            nodes=[node_name],
            content_types=["iso", "vztmpl", "backup"],
            shared=False
        )
        self.storage_pools["local"] = local_storage
        
        # LVM-thin storage
        lvm_storage = StoragePool(
            name="local-lvm",
            type=StorageType.LVM_THIN,
            size_gb=900,
            nodes=[node_name],
            content_types=["images", "rootdir"],
            shared=False
        )
        self.storage_pools["local-lvm"] = lvm_storage
        
        logger.info(f"Created default storage pools for {node_name}")
    
    async def create_vm(self, name: str, node_name: str, cpu_cores: int = 2,
                       memory_gb: int = 4, disk_gb: int = 32, 
                       os_type: str = "linux") -> Dict[str, Any]:
        """Create a virtual machine"""
        logger.info(f"Creating VM: {name} on node {node_name}")
        
        result = {
            'status': 'success',
            'vmid': None,
            'name': name,
            'node': node_name
        }
        
        try:
            # Check node exists and has resources
            if node_name not in self.nodes:
                raise ValueError(f"Node {node_name} not found")
            
            node = self.nodes[node_name]
            
            # Check resources
            if not await self._check_resources(node, cpu_cores, memory_gb, disk_gb):
                raise ValueError("Insufficient resources on node")
            
            # Create VM
            vmid = self.next_vmid
            self.next_vmid += 1
            
            vm = VirtualMachine(
                vmid=vmid,
                name=name,
                node=node_name,
                hypervisor=HypervisorType.KVM_QEMU,
                cpu_cores=cpu_cores,
                memory_mb=memory_gb * 1024,
                disk_gb=disk_gb,
                os_type=os_type
            )
            
            # Add network interface
            vm.network_interfaces.append({
                'model': 'virtio',
                'bridge': 'vmbr0',
                'tag': None
            })
            
            self.vms[vmid] = vm
            node.vms.append(name)
            
            # Simulate creation delay
            await asyncio.sleep(0.5)
            
            result['vmid'] = vmid
            logger.info(f"VM created: {name} (VMID: {vmid}) on {node_name}")
            
        except Exception as e:
            result['status'] = 'failed'
            result['error'] = str(e)
            logger.error(f"VM creation failed: {e}")
        
        return result
    
    async def _check_resources(self, node: ProxmoxNode, cpu_cores: int,
                              memory_gb: int, disk_gb: int) -> bool:
        """Check if node has sufficient resources"""
        # Simple resource check (in production would be more sophisticated)
        used_cpu = len(node.vms) * 2  # Assume 2 cores per VM average
        used_memory = len(node.vms) * 4  # Assume 4GB per VM average
        used_disk = len(node.vms) * 32  # Assume 32GB per VM average
        
        available_cpu = node.cpu_cores - used_cpu
        available_memory = node.memory_gb - used_memory
        available_disk = node.storage_gb - used_disk
        
        return (cpu_cores <= available_cpu and 
                memory_gb <= available_memory and 
                disk_gb <= available_disk)
    
    async def create_container(self, name: str, node_name: str, 
                             template: str = "debian-12", cpu_cores: int = 1,
                             memory_gb: int = 2, disk_gb: int = 8) -> Dict[str, Any]:
        """Create LXC container"""
        logger.info(f"Creating container: {name} on node {node_name}")
        
        result = {
            'status': 'success',
            'vmid': None,
            'name': name,
            'node': node_name,
            'type': 'container'
        }
        
        try:
            if node_name not in self.nodes:
                raise ValueError(f"Node {node_name} not found")
            
            node = self.nodes[node_name]
            
            # Create container (similar to VM but lighter)
            vmid = self.next_vmid
            self.next_vmid += 1
            
            container = VirtualMachine(
                vmid=vmid,
                name=name,
                node=node_name,
                hypervisor=HypervisorType.LXC,
                cpu_cores=cpu_cores,
                memory_mb=memory_gb * 1024,
                disk_gb=disk_gb,
                os_type=template
            )
            
            self.vms[vmid] = container
            node.containers.append(name)
            
            # Containers are faster to create
            await asyncio.sleep(0.2)
            
            result['vmid'] = vmid
            logger.info(f"Container created: {name} (VMID: {vmid}) on {node_name}")
            
        except Exception as e:
            result['status'] = 'failed'
            result['error'] = str(e)
            logger.error(f"Container creation failed: {e}")
        
        return result
    
    async def start_vm(self, vmid: int) -> Dict[str, Any]:
        """Start a virtual machine or container"""
        logger.info(f"Starting VM/container: {vmid}")
        
        result = {
            'status': 'success',
            'vmid': vmid,
            'state': ''
        }
        
        try:
            if vmid not in self.vms:
                raise ValueError(f"VM {vmid} not found")
            
            vm = self.vms[vmid]
            
            if vm.state == VMState.RUNNING:
                result['message'] = "Already running"
            else:
                vm.state = VMState.RUNNING
                await asyncio.sleep(0.3)  # Simulate startup time
                result['state'] = vm.state.value
                logger.info(f"VM {vmid} started successfully")
            
        except Exception as e:
            result['status'] = 'failed'
            result['error'] = str(e)
            logger.error(f"Failed to start VM: {e}")
        
        return result
    
    async def stop_vm(self, vmid: int, force: bool = False) -> Dict[str, Any]:
        """Stop a virtual machine or container"""
        logger.info(f"Stopping VM/container: {vmid} (force={force})")
        
        result = {
            'status': 'success',
            'vmid': vmid,
            'state': ''
        }
        
        try:
            if vmid not in self.vms:
                raise ValueError(f"VM {vmid} not found")
            
            vm = self.vms[vmid]
            
            if vm.state == VMState.STOPPED:
                result['message'] = "Already stopped"
            else:
                vm.state = VMState.STOPPED
                await asyncio.sleep(0.1 if force else 0.5)
                result['state'] = vm.state.value
                logger.info(f"VM {vmid} stopped")
            
        except Exception as e:
            result['status'] = 'failed'
            result['error'] = str(e)
            logger.error(f"Failed to stop VM: {e}")
        
        return result
    
    async def migrate_vm(self, vmid: int, target_node: str, 
                        online: bool = True) -> Dict[str, Any]:
        """Migrate VM to another node"""
        logger.info(f"Migrating VM {vmid} to {target_node} (online={online})")
        
        result = {
            'status': 'success',
            'vmid': vmid,
            'source_node': '',
            'target_node': target_node,
            'migration_time': 0.0
        }
        
        try:
            if vmid not in self.vms:
                raise ValueError(f"VM {vmid} not found")
            
            if target_node not in self.nodes:
                raise ValueError(f"Target node {target_node} not found")
            
            vm = self.vms[vmid]
            source_node = vm.node
            result['source_node'] = source_node
            
            # Check if online migration is possible
            if online and vm.state != VMState.RUNNING:
                raise ValueError("VM must be running for online migration")
            
            # Simulate migration
            vm.state = VMState.MIGRATING
            start_time = asyncio.get_event_loop().time()
            
            # Migration time depends on VM size and online/offline
            migration_delay = (vm.disk_gb / 100) + (0.5 if online else 0.1)
            await asyncio.sleep(migration_delay)
            
            # Update VM location
            vm.node = target_node
            vm.state = VMState.RUNNING if online else VMState.STOPPED
            
            # Update node VM lists
            self.nodes[source_node].vms.remove(vm.name)
            self.nodes[target_node].vms.append(vm.name)
            
            result['migration_time'] = asyncio.get_event_loop().time() - start_time
            logger.info(f"VM {vmid} migrated in {result['migration_time']:.2f}s")
            
        except Exception as e:
            result['status'] = 'failed'
            result['error'] = str(e)
            logger.error(f"Migration failed: {e}")
        
        return result
    
    async def setup_cluster(self, cluster_name: str, nodes: List[str]) -> Dict[str, Any]:
        """Setup Proxmox cluster"""
        logger.info(f"Setting up cluster: {cluster_name} with nodes: {nodes}")
        
        result = {
            'status': 'success',
            'cluster_name': cluster_name,
            'nodes': nodes,
            'quorum': 0
        }
        
        try:
            if len(nodes) < 2:
                raise ValueError("Cluster requires at least 2 nodes")
            
            cluster_nodes = []
            for node_name in nodes:
                if node_name not in self.nodes:
                    raise ValueError(f"Node {node_name} not found")
                node = self.nodes[node_name]
                node.cluster_member = True
                cluster_nodes.append(node)
            
            # Calculate quorum
            quorum = (len(nodes) // 2) + 1
            
            # Create cluster configuration
            self.cluster = ClusterConfig(
                name=cluster_name,
                nodes=cluster_nodes,
                quorum=quorum
            )
            
            # Simulate cluster initialization
            await asyncio.sleep(1.0)
            
            result['quorum'] = quorum
            logger.info(f"Cluster {cluster_name} created with quorum={quorum}")
            
        except Exception as e:
            result['status'] = 'failed'
            result['error'] = str(e)
            logger.error(f"Cluster setup failed: {e}")
        
        return result
    
    async def configure_ha(self, vmid: int, group: str = "default", 
                          max_restart: int = 3) -> Dict[str, Any]:
        """Configure high availability for VM"""
        logger.info(f"Configuring HA for VM {vmid}")
        
        result = {
            'status': 'success',
            'vmid': vmid,
            'ha_enabled': False,
            'group': group,
            'max_restart': max_restart
        }
        
        try:
            if not self.cluster:
                raise ValueError("HA requires cluster configuration")
            
            if vmid not in self.vms:
                raise ValueError(f"VM {vmid} not found")
            
            vm = self.vms[vmid]
            vm.ha_enabled = True
            
            result['ha_enabled'] = True
            logger.info(f"HA enabled for VM {vmid} in group {group}")
            
        except Exception as e:
            result['status'] = 'failed'
            result['error'] = str(e)
            logger.error(f"HA configuration failed: {e}")
        
        return result
    
    async def add_storage(self, name: str, storage_type: StorageType,
                         size_gb: int, nodes: List[str], 
                         shared: bool = False) -> Dict[str, Any]:
        """Add storage pool"""
        logger.info(f"Adding storage pool: {name} ({storage_type.value})")
        
        result = {
            'status': 'success',
            'storage_name': name,
            'type': storage_type.value,
            'size_gb': size_gb,
            'shared': shared
        }
        
        try:
            # Validate nodes
            for node_name in nodes:
                if node_name not in self.nodes:
                    raise ValueError(f"Node {node_name} not found")
            
            # Determine content types based on storage type
            if storage_type in [StorageType.NFS, StorageType.CIFS]:
                content_types = ["images", "rootdir", "vztmpl", "backup", "iso"]
            elif storage_type in [StorageType.CEPH_RBD]:
                content_types = ["images", "rootdir"]
            else:
                content_types = ["images", "rootdir"]
            
            storage = StoragePool(
                name=name,
                type=storage_type,
                size_gb=size_gb,
                nodes=nodes,
                content_types=content_types,
                shared=shared
            )
            
            self.storage_pools[name] = storage
            
            # Simulate storage configuration
            await asyncio.sleep(0.5)
            
            logger.info(f"Storage pool {name} added successfully")
            
        except Exception as e:
            result['status'] = 'failed'
            result['error'] = str(e)
            logger.error(f"Failed to add storage: {e}")
        
        return result
    
    async def setup_ceph(self, nodes: List[str], pool_name: str = "rbd",
                        pg_num: int = 128) -> Dict[str, Any]:
        """Setup Ceph distributed storage"""
        logger.info(f"Setting up Ceph on nodes: {nodes}")
        
        result = {
            'status': 'success',
            'nodes': nodes,
            'pool': pool_name,
            'pg_num': pg_num,
            'monitors': [],
            'osds': []
        }
        
        try:
            if len(nodes) < 3:
                raise ValueError("Ceph requires at least 3 nodes")
            
            # Validate nodes
            for node_name in nodes:
                if node_name not in self.nodes:
                    raise ValueError(f"Node {node_name} not found")
            
            # Setup monitors (first 3 nodes)
            result['monitors'] = nodes[:3]
            
            # Setup OSDs (all nodes)
            for node in nodes:
                result['osds'].append({
                    'node': node,
                    'osd_id': nodes.index(node),
                    'size_gb': 500
                })
            
            # Create Ceph storage pool
            await self.add_storage(
                f"ceph-{pool_name}",
                StorageType.CEPH_RBD,
                len(nodes) * 500,  # Total size
                nodes,
                shared=True
            )
            
            # Simulate Ceph deployment
            await asyncio.sleep(2.0)
            
            logger.info(f"Ceph cluster deployed with {len(result['monitors'])} monitors and {len(result['osds'])} OSDs")
            
        except Exception as e:
            result['status'] = 'failed'
            result['error'] = str(e)
            logger.error(f"Ceph setup failed: {e}")
        
        return result
    
    async def backup_vm(self, vmid: int, storage: str = "local",
                       mode: str = "snapshot") -> Dict[str, Any]:
        """Backup virtual machine"""
        logger.info(f"Backing up VM {vmid} to {storage} (mode={mode})")
        
        result = {
            'status': 'success',
            'vmid': vmid,
            'storage': storage,
            'mode': mode,
            'backup_file': '',
            'backup_size_mb': 0,
            'duration': 0.0
        }
        
        try:
            if vmid not in self.vms:
                raise ValueError(f"VM {vmid} not found")
            
            if storage not in self.storage_pools:
                raise ValueError(f"Storage {storage} not found")
            
            vm = self.vms[vmid]
            start_time = asyncio.get_event_loop().time()
            
            # Generate backup filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            result['backup_file'] = f"vzdump-qemu-{vmid}-{timestamp}.vma"
            
            # Calculate backup size (compressed)
            result['backup_size_mb'] = int(vm.disk_gb * 1024 * 0.3)  # ~30% of disk size
            
            # Simulate backup time based on mode
            if mode == "snapshot":
                await asyncio.sleep(0.5)  # Fast snapshot
            elif mode == "suspend":
                await asyncio.sleep(1.0)  # Medium speed
            else:  # stop mode
                await asyncio.sleep(1.5)  # Slower but consistent
            
            result['duration'] = asyncio.get_event_loop().time() - start_time
            
            # Mark VM as backup enabled
            vm.backup_enabled = True
            
            logger.info(f"Backup completed: {result['backup_file']} ({result['backup_size_mb']}MB in {result['duration']:.2f}s)")
            
        except Exception as e:
            result['status'] = 'failed'
            result['error'] = str(e)
            logger.error(f"Backup failed: {e}")
        
        return result
    
    async def restore_vm(self, backup_file: str, target_node: str,
                        new_vmid: Optional[int] = None) -> Dict[str, Any]:
        """Restore VM from backup"""
        logger.info(f"Restoring VM from {backup_file} to {target_node}")
        
        result = {
            'status': 'success',
            'backup_file': backup_file,
            'target_node': target_node,
            'vmid': new_vmid or self.next_vmid,
            'duration': 0.0
        }
        
        try:
            if target_node not in self.nodes:
                raise ValueError(f"Node {target_node} not found")
            
            start_time = asyncio.get_event_loop().time()
            
            # Simulate restore operation
            await asyncio.sleep(2.0)
            
            # Create restored VM
            if not new_vmid:
                new_vmid = self.next_vmid
                self.next_vmid += 1
            
            restored_vm = VirtualMachine(
                vmid=new_vmid,
                name=f"restored-{new_vmid}",
                node=target_node,
                hypervisor=HypervisorType.KVM_QEMU,
                cpu_cores=2,
                memory_mb=4096,
                disk_gb=32
            )
            
            self.vms[new_vmid] = restored_vm
            self.nodes[target_node].vms.append(restored_vm.name)
            
            result['vmid'] = new_vmid
            result['duration'] = asyncio.get_event_loop().time() - start_time
            
            logger.info(f"VM restored as VMID {new_vmid} in {result['duration']:.2f}s")
            
        except Exception as e:
            result['status'] = 'failed'
            result['error'] = str(e)
            logger.error(f"Restore failed: {e}")
        
        return result
    
    def get_cluster_status(self) -> Dict[str, Any]:
        """Get cluster status"""
        if not self.cluster:
            return {'status': 'No cluster configured'}
        
        return {
            'cluster_name': self.cluster.name,
            'nodes': len(self.cluster.nodes),
            'quorum': self.cluster.quorum,
            'ha_enabled': self.cluster.ha_enabled,
            'total_vms': len(self.vms),
            'running_vms': sum(1 for vm in self.vms.values() if vm.state == VMState.RUNNING),
            'total_cpu_cores': sum(node.cpu_cores for node in self.nodes.values()),
            'total_memory_gb': sum(node.memory_gb for node in self.nodes.values()),
            'total_storage_gb': sum(pool.size_gb for pool in self.storage_pools.values())
        }
    
    def get_metrics(self) -> Dict[str, Any]:
        """Get performance metrics"""
        return {
            'nodes': len(self.nodes),
            'vms': len(self.vms),
            'running_vms': sum(1 for vm in self.vms.values() if vm.state == VMState.RUNNING),
            'containers': sum(1 for vm in self.vms.values() if vm.hypervisor == HypervisorType.LXC),
            'storage_pools': len(self.storage_pools),
            'ha_vms': sum(1 for vm in self.vms.values() if vm.ha_enabled),
            'cluster_configured': self.cluster is not None,
            'total_storage_gb': sum(pool.size_gb for pool in self.storage_pools.values()),
            'api_response_time': '<500ms',
            'vm_density': f"{len(self.vms) / max(len(self.nodes), 1):.1f}:1"
        }


async def main():
    """Test PROXMOX-AGENT implementation"""
    agent = ProxmoxAgent()
    
    print("=" * 80)
    print(f"PROXMOX-AGENT v{agent.version} - Virtualization Management Specialist")
    print("=" * 80)
    
    # Add nodes
    print("\n[1] Adding Proxmox Nodes...")
    nodes = [
        ProxmoxNode("pve-01", "192.168.1.10", cpu_cores=32, memory_gb=128, storage_gb=2000),
        ProxmoxNode("pve-02", "192.168.1.11", cpu_cores=32, memory_gb=128, storage_gb=2000),
        ProxmoxNode("pve-03", "192.168.1.12", cpu_cores=32, memory_gb=128, storage_gb=2000)
    ]
    
    for node in nodes:
        await agent.add_node(node)
        print(f"Added node: {node.name}")
    
    # Setup cluster
    print("\n[2] Setting up Cluster...")
    cluster = await agent.setup_cluster("production-cluster", ["pve-01", "pve-02", "pve-03"])
    print(f"Cluster Status: {cluster['status']}")
    print(f"Quorum: {cluster['quorum']}")
    
    # Create VMs
    print("\n[3] Creating Virtual Machines...")
    vm1 = await agent.create_vm("web-server-01", "pve-01", cpu_cores=4, memory_gb=8, disk_gb=100)
    print(f"Created VM: {vm1['name']} (VMID: {vm1.get('vmid')})")
    
    vm2 = await agent.create_vm("database-01", "pve-02", cpu_cores=8, memory_gb=16, disk_gb=200)
    print(f"Created VM: {vm2['name']} (VMID: {vm2.get('vmid')})")
    
    # Create container
    print("\n[4] Creating LXC Container...")
    container = await agent.create_container("app-container-01", "pve-03", cpu_cores=2, memory_gb=4)
    print(f"Created Container: {container['name']} (VMID: {container.get('vmid')})")
    
    # Start VMs
    print("\n[5] Starting VMs...")
    if vm1.get('vmid'):
        start1 = await agent.start_vm(vm1['vmid'])
        print(f"VM {vm1['vmid']} status: {start1.get('state', 'unknown')}")
    
    if vm2.get('vmid'):
        start2 = await agent.start_vm(vm2['vmid'])
        print(f"VM {vm2['vmid']} status: {start2.get('state', 'unknown')}")
    
    # Setup Ceph storage
    print("\n[6] Setting up Ceph Storage...")
    ceph = await agent.setup_ceph(["pve-01", "pve-02", "pve-03"])
    print(f"Ceph Status: {ceph['status']}")
    print(f"Monitors: {ceph.get('monitors', [])}")
    print(f"OSDs: {len(ceph.get('osds', []))}")
    
    # Configure HA
    print("\n[7] Configuring High Availability...")
    if vm1.get('vmid'):
        ha = await agent.configure_ha(vm1['vmid'])
        print(f"HA Status for VM {vm1['vmid']}: {'Enabled' if ha.get('ha_enabled') else 'Failed'}")
    
    # Migrate VM
    print("\n[8] Testing Live Migration...")
    if vm1.get('vmid'):
        migration = await agent.migrate_vm(vm1['vmid'], "pve-02", online=True)
        print(f"Migration Status: {migration['status']}")
        print(f"Migration Time: {migration.get('migration_time', 0):.2f}s")
    
    # Backup VM
    print("\n[9] Testing Backup...")
    if vm2.get('vmid'):
        backup = await agent.backup_vm(vm2['vmid'], storage="local", mode="snapshot")
        print(f"Backup Status: {backup['status']}")
        print(f"Backup File: {backup.get('backup_file', 'N/A')}")
        print(f"Backup Size: {backup.get('backup_size_mb', 0)}MB")
    
    # Get cluster status
    print("\n[10] Cluster Status:")
    print("-" * 40)
    status = agent.get_cluster_status()
    for key, value in status.items():
        print(f"{key.replace('_', ' ').title()}: {value}")
    
    # Get metrics
    print("\n[11] Performance Metrics:")
    print("-" * 40)
    metrics = agent.get_metrics()
    for metric, value in metrics.items():
        print(f"{metric.replace('_', ' ').title()}: {value}")
    
    print("\n" + "=" * 80)
    print("PROXMOX-AGENT Test Complete - Virtualization Excellence!")
    print("=" * 80)


if __name__ == "__main__":
    asyncio.run(main())