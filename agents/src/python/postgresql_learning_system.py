#!/usr/bin/env python3
"""
Ultimate PostgreSQL Agent Learning System v3.1 - ULTRATHINK MERGED VERSION
Complete integration of the best features from both learning system implementations
- Advanced ML with PyTorch + sklearn support
- Claude-Code integration interface  
- Dual database driver support (asyncpg + psycopg2)
- Comprehensive CLI interface with multiple commands
- Enhanced pattern recognition and real-time monitoring
- Professional database schema with ML optimization
"""

import asyncio
import asyncpg
import psycopg2
from psycopg2.extras import Json, RealDictCursor
import json
import time
import hashlib
import logging
import pickle
import sys
import base64
import io
from typing import Dict, List, Any, Optional, Tuple, Set, Union
from dataclasses import dataclass, field, asdict
from datetime import datetime, timedelta
from pathlib import Path
from collections import defaultdict, Counter, deque
from enum import Enum
import numpy as np
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor

# Machine Learning imports
try:
    from sklearn.ensemble import (
        RandomForestClassifier, RandomForestRegressor, 
        GradientBoostingRegressor, IsolationForest
    )
    from sklearn.neural_network import MLPClassifier, MLPRegressor
    from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler
    from sklearn.model_selection import cross_val_score, GridSearchCV
    from sklearn.metrics import accuracy_score, mean_squared_error, precision_recall_curve
    from sklearn.cluster import DBSCAN, KMeans
    from sklearn.decomposition import PCA
    import joblib
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False
    print("Warning: ML libraries not available. Install with: pip install scikit-learn joblib")

# Deep Learning Support (optional)
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import DataLoader, TensorDataset
    DL_AVAILABLE = True
except ImportError:
    DL_AVAILABLE = False
    # Only show PyTorch message if explicitly requested
    import os
    if os.environ.get('SHOW_PYTORCH_WARNING', '').lower() == 'true':
        print("Info: PyTorch not available. Deep learning features disabled.")

# Database availability check
try:
    import asyncpg
    import psycopg2
    DATABASE_AVAILABLE = True
except ImportError:
    print("Warning: PostgreSQL drivers not available. Install with: pip install asyncpg psycopg2-binary")
    DATABASE_AVAILABLE = False

logger = logging.getLogger(__name__)

# ============================================================================
# ENHANCED DATA STRUCTURES v3.1
# ============================================================================

class LearningMode(Enum):
    """Advanced learning modes"""
    EXPLORATION = "exploration"
    EXPLOITATION = "exploitation"
    BALANCED = "balanced"
    CONSERVATIVE = "conservative"
    AGGRESSIVE = "aggressive"
    ADAPTIVE = "adaptive"
    REINFORCEMENT = "reinforcement"
    TRANSFER = "transfer"

class OptimizationObjective(Enum):
    """Optimization objectives for learning"""
    SUCCESS_RATE = "success_rate"
    EXECUTION_TIME = "execution_time"
    RESOURCE_EFFICIENCY = "resource_efficiency"
    USER_SATISFACTION = "user_satisfaction"
    COST_OPTIMIZATION = "cost_optimization"
    BALANCED = "balanced"
    CUSTOM = "custom"

class PredictionConfidence(Enum):
    """Confidence levels for predictions"""
    VERY_HIGH = 0.9
    HIGH = 0.75
    MEDIUM = 0.6
    LOW = 0.4
    VERY_LOW = 0.2

@dataclass
class AgentTaskExecution:
    """Base task execution record"""
    execution_id: str
    task_type: str
    task_description: str
    agents_invoked: List[str]
    execution_sequence: List[str]
    start_time: datetime
    end_time: datetime
    duration_seconds: float
    success: bool
    error_message: Optional[str] = None
    user_satisfaction: Optional[int] = None
    complexity_score: float = 1.0
    resource_metrics: Dict[str, float] = field(default_factory=dict)
    context_data: Dict[str, Any] = field(default_factory=dict)

@dataclass
class EnhancedAgentTaskExecution(AgentTaskExecution):
    """Enhanced task execution record with ML features"""
    feature_vector: Optional[np.ndarray] = None
    predicted_success: Optional[float] = None
    predicted_duration: Optional[float] = None
    prediction_confidence: Optional[float] = None
    agent_synergy_scores: Dict[str, float] = field(default_factory=dict)
    task_embedding: Optional[np.ndarray] = None
    performance_anomaly: bool = False
    optimization_opportunities: List[str] = field(default_factory=list)

@dataclass
class TaskContext:
    """Enhanced task context with rich features"""
    task_id: str
    task_type: str
    description: str
    complexity_score: float
    priority: int
    deadline: Optional[datetime]
    user_context: Dict[str, Any]
    environment_context: Dict[str, Any] = field(default_factory=dict)
    historical_context: List[Dict[str, Any]] = field(default_factory=list)
    constraints: Dict[str, Any] = field(default_factory=dict)
    dependencies: List[str] = field(default_factory=list)
    expected_outcomes: Dict[str, Any] = field(default_factory=dict)
    risk_factors: List[str] = field(default_factory=list)
    optimization_goals: List[OptimizationObjective] = field(default_factory=list)
    parent_task_id: Optional[str] = None
    subtasks: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class AgentLearningInsight:
    """Learning insight from pattern analysis"""
    insight_id: str
    insight_type: str
    confidence_score: float
    title: str
    description: str
    supporting_data: Dict[str, Any]
    applicable_contexts: List[str] = field(default_factory=list)
    created_at: datetime = field(default_factory=datetime.now)
    category: Optional[str] = None
    impact_score: Optional[float] = None
    actionable_recommendations: List[str] = field(default_factory=list)
    expected_improvement: Dict[str, float] = field(default_factory=dict)
    risk_assessment: Dict[str, float] = field(default_factory=dict)
    validation_metrics: Dict[str, Any] = field(default_factory=dict)
    expiry_date: Optional[datetime] = None
    dependencies: List[str] = field(default_factory=list)
    last_validated: Optional[datetime] = None
    validation_count: int = 0

@dataclass
class AgentCollaborationPattern:
    """Pattern for agent collaboration effectiveness"""
    pattern_id: str
    agents: List[str]
    task_types: List[str]
    success_rate: float
    avg_duration: float
    synergy_score: float
    conflict_indicators: List[str] = field(default_factory=list)
    optimal_sequence: List[str] = field(default_factory=list)
    resource_efficiency: float = 1.0
    scalability_score: float = 1.0

@dataclass
class TaskPrediction:
    """Prediction for task execution"""
    task_id: str
    predicted_agents: List[str]
    predicted_duration: float
    predicted_success_probability: float
    confidence_score: float
    risk_factors: List[str]
    optimization_suggestions: List[str]
    alternative_approaches: List[Dict[str, Any]]

@dataclass
class AgentPerformanceProfile:
    """Comprehensive agent performance profile"""
    agent_name: str
    capabilities: Set[str]
    success_metrics: Dict[str, float]
    performance_history: deque = field(default_factory=lambda: deque(maxlen=1000))
    collaboration_matrix: Dict[str, float] = field(default_factory=dict)
    resource_profile: Dict[str, float] = field(default_factory=dict)
    skill_embeddings: Optional[np.ndarray] = None
    learning_curve: List[float] = field(default_factory=list)
    specialization_scores: Dict[str, float] = field(default_factory=dict)
    failure_patterns: List[Dict[str, Any]] = field(default_factory=list)
    optimization_hints: List[str] = field(default_factory=list)
    last_updated: datetime = field(default_factory=datetime.now)
    confidence_scores: Dict[str, float] = field(default_factory=dict)
    adaptation_rate: float = 0.0
    cognitive_load: float = 0.0

# ============================================================================
# ADVANCED PATTERN RECOGNITION ENGINE v3.1
# ============================================================================

class AdvancedPatternRecognizer:
    """Advanced pattern recognition with ML capabilities"""
    
    def __init__(self):
        self.sequence_analyzer = SequenceAnalyzer()
        self.anomaly_detector = AnomalyDetector()
        self.synergy_calculator = SynergyCalculator()
        self.pattern_cache = {}
        
    def analyze_execution_patterns(self, executions: List[EnhancedAgentTaskExecution]) -> Dict[str, Any]:
        """Comprehensive pattern analysis"""
        patterns = {
            'sequences': self.sequence_analyzer.find_patterns(executions),
            'anomalies': self.anomaly_detector.detect(executions),
            'synergies': self.synergy_calculator.calculate(executions),
            'bottlenecks': self._identify_bottlenecks(executions),
            'optimization_opportunities': self._find_optimizations(executions)
        }
        return patterns
    
    def _identify_bottlenecks(self, executions: List[EnhancedAgentTaskExecution]) -> List[Dict[str, Any]]:
        """Identify performance bottlenecks"""
        bottlenecks = []
        agent_performance = defaultdict(list)
        
        for exec in executions:
            for agent in exec.agents_invoked:
                agent_performance[agent].append(exec.duration_seconds)
        
        for agent, durations in agent_performance.items():
            if len(durations) >= 5:
                avg_duration = np.mean(durations)
                p95_duration = np.percentile(durations, 95)
                if p95_duration > avg_duration * 2:
                    bottlenecks.append({
                        'type': 'agent_performance',
                        'agent': agent,
                        'avg_duration': avg_duration,
                        'p95_duration': p95_duration,
                        'severity': 'high' if p95_duration > avg_duration * 3 else 'medium'
                    })
        
        return bottlenecks
    
    def _find_optimizations(self, executions: List[EnhancedAgentTaskExecution]) -> List[Dict[str, Any]]:
        """Find optimization opportunities"""
        optimizations = []
        task_agent_combos = defaultdict(list)
        
        for exec in executions:
            key = (exec.task_type, tuple(sorted(exec.agents_invoked)))
            task_agent_combos[key].append(exec)
        
        for (task_type, agents), execs in task_agent_combos.items():
            if len(execs) >= 3:
                success_rate = sum(1 for e in execs if e.success) / len(execs)
                if success_rate < 0.7:
                    alternatives = self._find_alternative_agents(task_type, agents, executions)
                    if alternatives:
                        optimizations.append({
                            'type': 'agent_replacement',
                            'task_type': task_type,
                            'current_agents': list(agents),
                            'suggested_agents': alternatives,
                            'expected_improvement': self._calculate_improvement(agents, alternatives, executions)
                        })
        
        return optimizations
    
    def _find_alternative_agents(self, task_type: str, current_agents: Tuple[str], 
                                 executions: List[EnhancedAgentTaskExecution]) -> List[str]:
        """Find better performing agent combinations"""
        alternatives = []
        successful_combos = defaultdict(int)
        
        for exec in executions:
            if exec.task_type == task_type and exec.success:
                combo = tuple(sorted(exec.agents_invoked))
                if combo != current_agents:
                    successful_combos[combo] += 1
        
        if successful_combos:
            best_combo = max(successful_combos.items(), key=lambda x: x[1])
            alternatives = list(best_combo[0])
        
        return alternatives
    
    def _calculate_improvement(self, current: Tuple[str], suggested: List[str], 
                              executions: List[EnhancedAgentTaskExecution]) -> float:
        """Calculate expected improvement from agent change"""
        current_success = self._get_success_rate(current, executions)
        suggested_success = self._get_success_rate(tuple(sorted(suggested)), executions)
        return (suggested_success - current_success) / current_success if current_success > 0 else 0
    
    def _get_success_rate(self, agents: Tuple[str], executions: List[EnhancedAgentTaskExecution]) -> float:
        """Get success rate for agent combination"""
        matching = [e for e in executions if tuple(sorted(e.agents_invoked)) == agents]
        if not matching:
            return 0
        return sum(1 for e in matching if e.success) / len(matching)

class SequenceAnalyzer:
    """Analyze agent execution sequences"""
    
    def find_patterns(self, executions: List[EnhancedAgentTaskExecution]) -> List[Dict[str, Any]]:
        """Find common execution sequences"""
        sequences = []
        sequence_counts = Counter()
        
        for exec in executions:
            if exec.execution_sequence:
                seq_tuple = tuple(exec.execution_sequence)
                sequence_counts[seq_tuple] += 1
        
        for sequence, count in sequence_counts.most_common(10):
            if count >= 3:
                sequences.append({
                    'sequence': list(sequence),
                    'frequency': count,
                    'success_rate': self._calculate_sequence_success(sequence, executions)
                })
        
        return sequences
    
    def _calculate_sequence_success(self, sequence: Tuple[str], 
                                   executions: List[EnhancedAgentTaskExecution]) -> float:
        """Calculate success rate for a specific sequence"""
        matching = [e for e in executions if tuple(e.execution_sequence) == sequence]
        if not matching:
            return 0
        return sum(1 for e in matching if e.success) / len(matching)

class AnomalyDetector:
    """Detect anomalies in execution patterns"""
    
    def detect(self, executions: List[EnhancedAgentTaskExecution]) -> List[Dict[str, Any]]:
        """Detect execution anomalies"""
        anomalies = []
        task_durations = defaultdict(list)
        
        for exec in executions:
            task_durations[exec.task_type].append(exec.duration_seconds)
        
        for task_type, durations in task_durations.items():
            if len(durations) >= 5:
                mean_duration = np.mean(durations)
                std_duration = np.std(durations)
                
                for exec in executions:
                    if exec.task_type == task_type:
                        z_score = abs((exec.duration_seconds - mean_duration) / std_duration) if std_duration > 0 else 0
                        if z_score > 3:
                            anomalies.append({
                                'type': 'duration_anomaly',
                                'execution_id': exec.execution_id,
                                'task_type': task_type,
                                'duration': exec.duration_seconds,
                                'expected_duration': mean_duration,
                                'z_score': z_score
                            })
        
        return anomalies

class SynergyCalculator:
    """Calculate agent synergy scores"""
    
    def calculate(self, executions: List[EnhancedAgentTaskExecution]) -> Dict[str, float]:
        """Calculate synergy between agent pairs"""
        synergies = {}
        agent_pairs = defaultdict(list)
        
        for exec in executions:
            if len(exec.agents_invoked) >= 2:
                for i, agent1 in enumerate(exec.agents_invoked):
                    for agent2 in exec.agents_invoked[i+1:]:
                        pair = tuple(sorted([agent1, agent2]))
                        agent_pairs[pair].append(exec)
        
        for pair, execs in agent_pairs.items():
            if len(execs) >= 3:
                success_rate = sum(1 for e in execs if e.success) / len(execs)
                avg_duration = np.mean([e.duration_seconds for e in execs])
                synergy = success_rate / (1 + np.log1p(avg_duration))
                synergies[f"{pair[0]}+{pair[1]}"] = synergy
        
        return synergies

# ============================================================================
# ADVANCED PREDICTIVE ML MODELS v3.1
# ============================================================================

class PredictiveModels:
    """Enhanced machine learning models for prediction"""
    
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.encoders = {}
        self.feature_importance = {}
        
    def train_models(self, training_data: List[EnhancedAgentTaskExecution]):
        """Train all predictive models with enhanced features"""
        if not ML_AVAILABLE:
            logger.warning("ML libraries not available, skipping model training")
            return
        
        X, y_success, y_duration, feature_names = self._prepare_training_data(training_data)
        
        if len(X) < 10:
            logger.warning("Insufficient training data for ML models")
            return
        
        # Train enhanced models
        self.models['success_predictor'] = self._train_success_model(X, y_success)
        self.models['duration_predictor'] = self._train_duration_model(X, y_duration)
        self.models['agent_recommender'] = self._train_agent_recommender(training_data)
        
        # Train anomaly detector
        if len(X) >= 20:
            self.models['anomaly_detector'] = self._train_anomaly_detector(X)
        
        # Calculate feature importance
        if 'success_predictor' in self.models:
            self.feature_importance = dict(zip(
                feature_names,
                self.models['success_predictor'].feature_importances_
            ))
        
        logger.info(f"Trained {len(self.models)} ML models successfully")
    
    def _prepare_training_data(self, executions: List[EnhancedAgentTaskExecution]) -> Tuple:
        """Prepare enhanced training data for ML models"""
        features = []
        success_labels = []
        duration_labels = []
        feature_names = []
        
        for exec in executions:
            feature_vec = self._create_feature_vector(exec)
            features.append(feature_vec)
            success_labels.append(1 if exec.success else 0)
            duration_labels.append(exec.duration_seconds)
        
        feature_names = [
            'complexity_score', 'num_agents', 'task_type_encoded',
            'hour_of_day', 'day_of_week', 'priority_level',
            'has_deadline', 'agent_diversity_score'
        ]
        
        return np.array(features), np.array(success_labels), np.array(duration_labels), feature_names
    
    def _create_feature_vector(self, execution: EnhancedAgentTaskExecution) -> np.ndarray:
        """Create enhanced feature vector from execution"""
        features = []
        
        # Basic features
        features.append(execution.complexity_score)
        features.append(len(execution.agents_invoked))
        
        # Encode task type
        if 'task_type' not in self.encoders:
            self.encoders['task_type'] = LabelEncoder()
            task_types = ['web_development', 'api_development', 'bug_fix', 'deployment', 
                         'security_audit', 'system_design', 'performance_optimization', 
                         'documentation', 'testing', 'other']
            self.encoders['task_type'].fit(task_types)
        
        try:
            task_type_encoded = self.encoders['task_type'].transform([execution.task_type])[0]
        except:
            task_type_encoded = self.encoders['task_type'].transform(['other'])[0]
        features.append(task_type_encoded)
        
        # Time-based features
        features.append(execution.start_time.hour)
        features.append(execution.start_time.weekday())
        
        # Context features
        priority = execution.context_data.get('priority', 5)
        features.append(priority)
        
        # New enhanced features
        has_deadline = 1 if execution.context_data.get('deadline') else 0
        features.append(has_deadline)
        
        # Agent diversity score (how diverse the agent set is)
        agent_diversity = len(set(execution.agents_invoked)) / len(execution.agents_invoked) if execution.agents_invoked else 0
        features.append(agent_diversity)
        
        return np.array(features)
    
    def _train_success_model(self, X: np.ndarray, y: np.ndarray) -> RandomForestClassifier:
        """Train enhanced success prediction model"""
        model = RandomForestClassifier(
            n_estimators=150,
            max_depth=12,
            min_samples_split=3,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )
        
        if 'success' not in self.scalers:
            self.scalers['success'] = StandardScaler()
        X_scaled = self.scalers['success'].fit_transform(X)
        
        scores = cross_val_score(model, X_scaled, y, cv=5, scoring='accuracy')
        logger.info(f"Success model CV accuracy: {np.mean(scores):.3f} (+/- {np.std(scores):.3f})")
        
        model.fit(X_scaled, y)
        return model
    
    def _train_duration_model(self, X: np.ndarray, y: np.ndarray) -> GradientBoostingRegressor:
        """Train enhanced duration prediction model"""
        model = GradientBoostingRegressor(
            n_estimators=150,
            learning_rate=0.08,
            max_depth=6,
            min_samples_split=3,
            random_state=42
        )
        
        if 'duration' not in self.scalers:
            self.scalers['duration'] = StandardScaler()
        X_scaled = self.scalers['duration'].fit_transform(X)
        
        scores = cross_val_score(model, X_scaled, y, cv=5, scoring='neg_mean_squared_error')
        logger.info(f"Duration model CV RMSE: {np.sqrt(-np.mean(scores)):.2f}")
        
        model.fit(X_scaled, y)
        return model
    
    def _train_anomaly_detector(self, X: np.ndarray) -> IsolationForest:
        """Train anomaly detection model"""
        model = IsolationForest(
            contamination=0.1,
            random_state=42,
            n_jobs=-1
        )
        
        X_scaled = self.scalers['success'].transform(X)
        model.fit(X_scaled)
        return model
    
    def _train_agent_recommender(self, executions: List[EnhancedAgentTaskExecution]):
        """Train enhanced agent recommendation model"""
        agent_task_success = defaultdict(lambda: defaultdict(list))
        
        for exec in executions:
            for agent in exec.agents_invoked:
                agent_task_success[exec.task_type][agent].append(1 if exec.success else 0)
        
        agent_scores = {}
        for task_type, agents in agent_task_success.items():
            agent_scores[task_type] = {}
            for agent, successes in agents.items():
                if len(successes) >= 2:
                    agent_scores[task_type][agent] = np.mean(successes)
        
        return agent_scores
    
    def predict(self, task_context: TaskContext) -> TaskPrediction:
        """Make comprehensive prediction for task"""
        dummy_exec = EnhancedAgentTaskExecution(
            execution_id="pred_" + hashlib.md5(f"{task_context.task_type}{time.time()}".encode()).hexdigest()[:8],
            task_type=task_context.task_type,
            task_description=task_context.description,
            agents_invoked=[],
            execution_sequence=[],
            start_time=datetime.now(),
            end_time=datetime.now(),
            duration_seconds=0,
            success=False,
            complexity_score=task_context.complexity_score,
            context_data=task_context.user_context
        )
        
        feature_vec = self._create_feature_vector(dummy_exec)
        
        predictions = {
            'success_probability': 0.7,
            'duration': 30.0,
            'confidence': 0.5
        }
        
        # Make predictions if models are available
        if 'success_predictor' in self.models:
            X_scaled = self.scalers['success'].transform([feature_vec])
            success_prob = self.models['success_predictor'].predict_proba(X_scaled)[0][1]
            predictions['success_probability'] = success_prob
        
        if 'duration_predictor' in self.models:
            X_scaled = self.scalers['duration'].transform([feature_vec])
            duration = self.models['duration_predictor'].predict(X_scaled)[0]
            predictions['duration'] = max(1.0, duration)
        
        # Get agent recommendations
        recommended_agents = self._recommend_agents(task_context.task_type, task_context.complexity_score)
        
        # Calculate confidence
        confidence = self._calculate_prediction_confidence(predictions, len(recommended_agents))
        
        return TaskPrediction(
            task_id=dummy_exec.execution_id,
            predicted_agents=recommended_agents,
            predicted_duration=predictions['duration'],
            predicted_success_probability=predictions['success_probability'],
            confidence_score=confidence,
            risk_factors=self._identify_risk_factors(task_context),
            optimization_suggestions=self._generate_optimization_suggestions(task_context.task_type, recommended_agents),
            alternative_approaches=self._find_alternatives(task_context.task_type, recommended_agents)
        )
    
    def _recommend_agents(self, task_type: str, complexity: float) -> List[str]:
        """Recommend optimal agents for task"""
        if 'agent_recommender' in self.models and task_type in self.models['agent_recommender']:
            agent_scores = self.models['agent_recommender'][task_type]
            sorted_agents = sorted(agent_scores.items(), key=lambda x: x[1], reverse=True)
            num_agents = min(5, max(2, int(complexity * 2)))
            return [agent for agent, score in sorted_agents[:num_agents]]
        
        return self._get_fallback_agents(task_type, complexity)
    
    def _get_fallback_agents(self, task_type: str, complexity: float) -> List[str]:
        """Enhanced fallback agent recommendations"""
        base_recommendations = {
            'web_development': ['WEB', 'APIDESIGNER', 'DATABASE', 'TESTBED', 'SECURITY'],
            'api_development': ['APIDESIGNER', 'DATABASE', 'SECURITY', 'TESTBED', 'MONITOR'],
            'bug_fix': ['DEBUGGER', 'PATCHER', 'TESTBED', 'LINTER', 'MONITOR'],
            'deployment': ['DEPLOYER', 'INFRASTRUCTURE', 'MONITOR', 'SECURITY', 'TESTBED'],
            'security_audit': ['SECURITY', 'SECURITYAUDITOR', 'CRYPTOEXPERT', 'BASTION', 'TESTBED'],
            'system_design': ['ARCHITECT', 'DATABASE', 'APIDESIGNER', 'INFRASTRUCTURE', 'SECURITY'],
            'performance_optimization': ['OPTIMIZER', 'MONITOR', 'DATABASE', 'INFRASTRUCTURE', 'TESTBED'],
            'documentation': ['DOCGEN', 'ARCHITECT', 'APIDESIGNER', 'SECURITY'],
            'testing': ['TESTBED', 'QADIRECTOR', 'DEBUGGER', 'SECURITY', 'MONITOR']
        }
        
        agents = base_recommendations.get(task_type, ['DIRECTOR', 'PROJECTORCHESTRATOR', 'ARCHITECT'])
        
        # Add more agents for complex tasks
        if complexity > 3:
            agents.append('MONITOR')
        if complexity > 4:
            agents.append('OPTIMIZER')
        
        return agents[:min(5, max(2, int(complexity * 2)))]
    
    def _calculate_prediction_confidence(self, predictions: Dict, num_agents: int) -> float:
        """Calculate confidence in predictions"""
        confidence_factors = []
        
        success_prob = predictions['success_probability']
        if success_prob > 0.8 or success_prob < 0.2:
            confidence_factors.append(0.9)
        else:
            confidence_factors.append(0.6)
        
        if num_agents >= 2 and num_agents <= 4:
            confidence_factors.append(0.8)
        else:
            confidence_factors.append(0.5)
        
        return np.mean(confidence_factors)
    
    def _identify_risk_factors(self, task_context: TaskContext) -> List[str]:
        """Identify potential risk factors"""
        risks = []
        
        if task_context.complexity_score > 4:
            risks.append("High complexity task - may require additional review")
        
        if task_context.priority <= 2:
            risks.append("Critical priority - ensure thorough testing")
        
        if task_context.task_type in ['deployment', 'security_audit']:
            risks.append("High-impact operation - requires careful execution")
        
        if task_context.deadline and (task_context.deadline - datetime.now()).total_seconds() < 3600:
            risks.append("Tight deadline - limited time for error recovery")
        
        return risks
    
    def _generate_optimization_suggestions(self, task_type: str, agents: List[str]) -> List[str]:
        """Generate optimization suggestions"""
        suggestions = []
        
        if len(agents) > 3:
            suggestions.append("Consider parallel execution for independent agent tasks")
        
        if 'TESTBED' not in agents and task_type != 'testing':
            suggestions.append("Add TESTBED agent for quality assurance")
        
        if 'MONITOR' not in agents and task_type in ['deployment', 'performance_optimization']:
            suggestions.append("Include MONITOR agent for performance tracking")
        
        return suggestions
    
    def _find_alternatives(self, task_type: str, primary_agents: List[str]) -> List[Dict[str, Any]]:
        """Find alternative approaches"""
        alternatives = []
        
        minimal_agents = primary_agents[:2] if len(primary_agents) > 2 else primary_agents
        alternatives.append({
            'name': 'Minimal Approach',
            'agents': minimal_agents,
            'pros': ['Faster execution', 'Lower resource usage'],
            'cons': ['Potentially lower quality', 'Less comprehensive']
        })
        
        if 'SECURITY' not in primary_agents:
            security_enhanced = primary_agents + ['SECURITY']
            alternatives.append({
                'name': 'Security-Enhanced Approach',
                'agents': security_enhanced,
                'pros': ['Better security coverage', 'Compliance assurance'],
                'cons': ['Longer execution time', 'Additional resource usage']
            })
        
        return alternatives

# ============================================================================
# ULTIMATE POSTGRESQL LEARNING SYSTEM v3.1
# ============================================================================

class UltimatePostgreSQLLearningSystem:
    """Ultimate learning system with best features from both implementations"""
    
    def __init__(self, db_config: Dict[str, str] = None):
        self.db_config = db_config or {
            'host': 'localhost',
            'port': 5432,
            'database': 'claude_learning_db',
            'user': 'claude_auth',
            'password': 'claude_auth'
        }
        
        # Core components
        self.pattern_recognizer = AdvancedPatternRecognizer()
        self.predictive_models = PredictiveModels()
        
        # Caching and performance
        self.prediction_cache = {}
        self.insight_cache = deque(maxlen=100)
        self.performance_buffer = deque(maxlen=1000)
        
        # Real-time monitoring
        self.alert_thresholds = {
            'success_rate_min': 0.7,
            'duration_p95_max': 120,
            'error_rate_max': 0.2
        }
        
        # Learning state
        self.learning_enabled = True
        self.learning_mode = LearningMode.ADAPTIVE
        self.auto_retrain_threshold = 50
        self.executions_since_training = 0
        
        self.setup_complete = False
        
    async def initialize(self):
        """Initialize the ultimate learning system"""
        if not DATABASE_AVAILABLE:
            logger.error("PostgreSQL drivers not available")
            return False
            
        try:
            # Test database connection
            conn = await asyncpg.connect(**self.db_config)
            await conn.close()
            
            # Setup enhanced schema
            await self.setup_ultimate_schema()
            
            # Load existing models if available
            await self.load_models()
            
            # Initialize monitoring
            await self.initialize_monitoring()
            
            self.setup_complete = True
            logger.info("Ultimate PostgreSQL Learning System v3.1 initialized successfully")
            return True
            
        except Exception as e:
            logger.error(f"Failed to initialize ultimate learning system: {e}")
            return False
    
    async def setup_ultimate_schema(self):
        """Setup ultimate database schema with all enhancements"""
        conn = await asyncpg.connect(**self.db_config)
        
        try:
            # Create extensions
            await conn.execute("""
                CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
                CREATE EXTENSION IF NOT EXISTS "pgcrypto";
            """)
            
            # Main execution table with all enhancements
            await conn.execute("""
                CREATE TABLE IF NOT EXISTS agent_task_executions (
                    execution_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
                    task_type VARCHAR(64) NOT NULL,
                    task_description TEXT,
                    agents_invoked JSONB DEFAULT '[]'::jsonb,
                    execution_sequence JSONB DEFAULT '[]'::jsonb,
                    start_time TIMESTAMP WITH TIME ZONE NOT NULL,
                    end_time TIMESTAMP WITH TIME ZONE NOT NULL,
                    duration_seconds FLOAT NOT NULL,
                    success BOOLEAN NOT NULL,
                    error_message TEXT,
                    user_satisfaction INTEGER CHECK (user_satisfaction BETWEEN 1 AND 10),
                    complexity_score FLOAT DEFAULT 1.0,
                    resource_metrics JSONB DEFAULT '{}'::jsonb,
                    context_data JSONB DEFAULT '{}'::jsonb,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                    predicted_success FLOAT,
                    predicted_duration FLOAT,
                    prediction_confidence FLOAT,
                    performance_anomaly BOOLEAN DEFAULT FALSE,
                    optimization_applied BOOLEAN DEFAULT FALSE,
                    ml_features JSONB DEFAULT '{}'::jsonb,
                    agent_synergy_scores JSONB DEFAULT '{}'::jsonb,
                    user_id UUID,
                    session_id UUID,
                    task_embedding JSONB DEFAULT '{}'::jsonb,
                    feature_vector JSONB DEFAULT '{}'::jsonb
                )
            """)
            
            await self._create_indexes(conn)
            await self._create_supplementary_tables(conn)
            
            logger.info("Ultimate learning schema setup complete")
            
        finally:
            await conn.close()
    
    async def _create_indexes(self, conn):
        """Create all necessary indexes for optimal performance"""
        indexes = [
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_task_executions_type_time ON agent_task_executions(task_type, start_time)",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_task_executions_success_time ON agent_task_executions(success, start_time)",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_task_executions_agents ON agent_task_executions USING GIN(agents_invoked)",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_task_executions_duration ON agent_task_executions(duration_seconds) WHERE success = true",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_executions_ml_features ON agent_task_executions USING GIN(ml_features)",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_executions_anomaly ON agent_task_executions(performance_anomaly) WHERE performance_anomaly = true",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_executions_complexity ON agent_task_executions(complexity_score, task_type)",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_executions_prediction ON agent_task_executions(predicted_success, prediction_confidence)"
        ]
        
        for index_sql in indexes:
            try:
                await conn.execute(index_sql)
            except Exception as e:
                logger.warning(f"Index creation warning: {e}")
    
    async def _create_supplementary_tables(self, conn):
        """Create all supplementary tables"""
        
        # Agent performance metrics
        await conn.execute("""
            CREATE TABLE IF NOT EXISTS agent_performance_metrics (
                agent_name VARCHAR(64) PRIMARY KEY,
                total_invocations BIGINT DEFAULT 0,
                successful_invocations BIGINT DEFAULT 0,
                success_rate FLOAT GENERATED ALWAYS AS (
                    CASE WHEN total_invocations > 0 
                    THEN successful_invocations::FLOAT / total_invocations 
                    ELSE 0 END
                ) STORED,
                avg_duration_seconds FLOAT DEFAULT 0,
                error_patterns JSONB DEFAULT '[]'::jsonb,
                best_partner_agents JSONB DEFAULT '[]'::jsonb,
                specialization_scores JSONB DEFAULT '{}'::jsonb,
                last_invocation TIMESTAMP WITH TIME ZONE,
                last_updated TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                performance_trend JSONB DEFAULT '{}'::jsonb,
                cognitive_load_score FLOAT DEFAULT 0.0
            )
        """)
        
        # ML models registry
        await conn.execute("""
            CREATE TABLE IF NOT EXISTS ml_models (
                model_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
                model_name VARCHAR(64) NOT NULL,
                model_type VARCHAR(32) NOT NULL,
                model_version VARCHAR(16) DEFAULT 'v3.1',
                model_data JSONB DEFAULT '{}'::jsonb,
                feature_importance JSONB DEFAULT '{}'::jsonb,
                performance_metrics JSONB DEFAULT '{}'::jsonb,
                training_date TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                training_samples INTEGER DEFAULT 0,
                is_active BOOLEAN DEFAULT TRUE,
                model_parameters JSONB DEFAULT '{}'::jsonb,
                validation_scores JSONB DEFAULT '{}'::jsonb,
                created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                
                UNIQUE(model_name, model_version)
            )
        """)
        
        # Agent collaboration patterns
        await conn.execute("""
            CREATE TABLE IF NOT EXISTS agent_collaboration_patterns (
                pattern_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
                agents JSONB NOT NULL,
                task_types JSONB DEFAULT '[]'::jsonb,
                success_rate FLOAT NOT NULL,
                avg_duration FLOAT NOT NULL,
                synergy_score FLOAT DEFAULT 0.0,
                optimal_sequence JSONB DEFAULT '[]'::jsonb,
                resource_efficiency FLOAT DEFAULT 1.0,
                scalability_score FLOAT DEFAULT 1.0,
                discovered_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                last_validated TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                validation_count INTEGER DEFAULT 0,
                conflict_indicators JSONB DEFAULT '[]'::jsonb,
                
                UNIQUE(agents)
            )
        """)
        
        # Enhanced learning insights
        await conn.execute("""
            CREATE TABLE IF NOT EXISTS agent_learning_insights (
                insight_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
                insight_type VARCHAR(32) NOT NULL,
                confidence_score FLOAT NOT NULL CHECK (confidence_score BETWEEN 0.0 AND 1.0),
                title VARCHAR(256) NOT NULL,
                description TEXT NOT NULL,
                supporting_data JSONB DEFAULT '{}'::jsonb,
                applicable_contexts JSONB DEFAULT '[]'::jsonb,
                created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                last_validated TIMESTAMP WITH TIME ZONE,
                validation_count INTEGER DEFAULT 0,
                is_active BOOLEAN DEFAULT TRUE,
                category VARCHAR(64),
                impact_score FLOAT,
                actionable_recommendations JSONB DEFAULT '[]'::jsonb,
                expected_improvement JSONB DEFAULT '{}'::jsonb,
                risk_assessment JSONB DEFAULT '{}'::jsonb,
                expiry_date TIMESTAMP WITH TIME ZONE,
                dependencies JSONB DEFAULT '[]'::jsonb
            )
        """)
        
        # Real-time performance metrics
        await conn.execute("""
            CREATE TABLE IF NOT EXISTS realtime_performance_metrics (
                metric_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
                metric_type VARCHAR(32) NOT NULL,
                metric_name VARCHAR(64) NOT NULL,
                metric_value FLOAT NOT NULL,
                threshold_value FLOAT,
                is_alert BOOLEAN DEFAULT FALSE,
                alert_severity VARCHAR(16),
                recorded_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                context_data JSONB DEFAULT '{}'::jsonb,
                resolution_status VARCHAR(32) DEFAULT 'open',
                resolved_at TIMESTAMP WITH TIME ZONE
            )
        """)
        
        # Task contexts for enhanced predictions
        await conn.execute("""
            CREATE TABLE IF NOT EXISTS task_contexts (
                context_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
                task_id VARCHAR(128) NOT NULL,
                task_type VARCHAR(64) NOT NULL,
                complexity_score FLOAT NOT NULL,
                priority INTEGER NOT NULL,
                deadline TIMESTAMP WITH TIME ZONE,
                user_context JSONB DEFAULT '{}'::jsonb,
                environment_context JSONB DEFAULT '{}'::jsonb,
                historical_context JSONB DEFAULT '[]'::jsonb,
                constraints JSONB DEFAULT '{}'::jsonb,
                dependencies JSONB DEFAULT '[]'::jsonb,
                expected_outcomes JSONB DEFAULT '{}'::jsonb,
                risk_factors JSONB DEFAULT '[]'::jsonb,
                optimization_goals JSONB DEFAULT '[]'::jsonb,
                created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                
                UNIQUE(task_id)
            )
        """)
    
    async def record_enhanced_execution(self, execution: EnhancedAgentTaskExecution, 
                                       user_id: str = None, session_id: str = None):
        """Record execution with complete ML predictions and analysis"""
        if not self.setup_complete:
            await self.initialize()
        
        conn = await asyncpg.connect(**self.db_config)
        
        try:
            # Make predictions before execution if not already done
            if execution.predicted_success is None:
                task_context = TaskContext(
                    task_id=execution.execution_id,
                    task_type=execution.task_type,
                    description=execution.task_description,
                    complexity_score=execution.complexity_score,
                    priority=execution.context_data.get('priority', 5),
                    deadline=execution.context_data.get('deadline'),
                    user_context=execution.context_data
                )
                
                prediction = self.predictive_models.predict(task_context)
                execution.predicted_success = prediction.predicted_success_probability
                execution.predicted_duration = prediction.predicted_duration
                execution.prediction_confidence = prediction.confidence_score
            
            # Detect anomalies
            patterns = self.pattern_recognizer.analyze_execution_patterns([execution])
            execution.performance_anomaly = len(patterns.get('anomalies', [])) > 0
            
            # Calculate synergy scores
            synergy_scores = patterns.get('synergies', {})
            execution.agent_synergy_scores = synergy_scores
            
            # Create feature vector
            feature_vector = self.predictive_models._create_feature_vector(execution)
            
            # Store execution record
            await conn.execute("""
                INSERT INTO agent_task_executions (
                    execution_id, task_type, task_description, agents_invoked, 
                    execution_sequence, start_time, end_time, duration_seconds, 
                    success, error_message, user_satisfaction, complexity_score, 
                    resource_metrics, context_data, user_id, session_id,
                    predicted_success, predicted_duration, prediction_confidence,
                    performance_anomaly, ml_features, agent_synergy_scores,
                    feature_vector
                ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, 
                         $13, $14, $15, $16, $17, $18, $19, $20, $21, $22, $23)
            """,
                execution.execution_id, execution.task_type, execution.task_description,
                json.dumps(execution.agents_invoked), json.dumps(execution.execution_sequence),
                execution.start_time, execution.end_time, execution.duration_seconds,
                execution.success, execution.error_message, execution.user_satisfaction,
                execution.complexity_score, json.dumps(execution.resource_metrics),
                json.dumps(execution.context_data), user_id, session_id,
                execution.predicted_success, execution.predicted_duration,
                execution.prediction_confidence, execution.performance_anomaly,
                json.dumps({}), json.dumps(synergy_scores), json.dumps(feature_vector.tolist())
            )
            
            # Update agent metrics
            await self.update_agent_metrics(conn, execution)
            
            # Update collaboration patterns
            await self.update_collaboration_patterns(conn, execution, synergy_scores)
            
            # Check for alerts
            await self.check_performance_alerts(conn, execution)
            
            # Trigger retraining if needed
            self.executions_since_training += 1
            if self.executions_since_training >= self.auto_retrain_threshold:
                asyncio.create_task(self.retrain_models())
            
        finally:
            await conn.close()
    
    async def update_agent_metrics(self, conn, execution: EnhancedAgentTaskExecution):
        """Update individual agent performance metrics"""
        for agent in execution.agents_invoked:
            await conn.execute("""
                INSERT INTO agent_performance_metrics (
                    agent_name, total_invocations, successful_invocations, 
                    avg_duration_seconds, min_duration_seconds, max_duration_seconds,
                    last_invocation
                ) VALUES ($1, 1, $2, $3, $3, $3, $4)
                ON CONFLICT (agent_name) DO UPDATE SET
                    total_invocations = agent_performance_metrics.total_invocations + 1,
                    successful_invocations = agent_performance_metrics.successful_invocations + $2,
                    avg_duration_seconds = (
                        agent_performance_metrics.avg_duration_seconds * agent_performance_metrics.total_invocations + $3
                    ) / (agent_performance_metrics.total_invocations + 1),
                    min_duration_seconds = LEAST(agent_performance_metrics.min_duration_seconds, $3),
                    max_duration_seconds = GREATEST(agent_performance_metrics.max_duration_seconds, $3),
                    last_invocation = $4,
                    last_updated = NOW()
            """, agent, 1 if execution.success else 0, execution.duration_seconds, execution.end_time)
    
    async def update_collaboration_patterns(self, conn, execution: EnhancedAgentTaskExecution, 
                                           synergy_scores: Dict[str, float]):
        """Update agent collaboration patterns"""
        if len(execution.agents_invoked) < 2:
            return
        
        agents_json = json.dumps(sorted(execution.agents_invoked))
        synergy_score = np.mean(list(synergy_scores.values())) if synergy_scores else 0.5
        resource_efficiency = 1.0 / (1 + np.log1p(sum(execution.resource_metrics.values())))
        
        await conn.execute("""
            INSERT INTO agent_collaboration_patterns (
                agents, task_types, success_rate, avg_duration, synergy_score,
                optimal_sequence, resource_efficiency
            ) VALUES ($1, $2, $3, $4, $5, $6, $7)
            ON CONFLICT (agents) DO UPDATE SET
                task_types = CASE 
                    WHEN NOT (agent_collaboration_patterns.task_types ? $8)
                    THEN agent_collaboration_patterns.task_types || $2
                    ELSE agent_collaboration_patterns.task_types
                END,
                success_rate = (
                    agent_collaboration_patterns.success_rate * agent_collaboration_patterns.validation_count + $3
                ) / (agent_collaboration_patterns.validation_count + 1),
                avg_duration = (
                    agent_collaboration_patterns.avg_duration * agent_collaboration_patterns.validation_count + $4
                ) / (agent_collaboration_patterns.validation_count + 1),
                synergy_score = (
                    agent_collaboration_patterns.synergy_score * agent_collaboration_patterns.validation_count + $5
                ) / (agent_collaboration_patterns.validation_count + 1),
                resource_efficiency = (
                    agent_collaboration_patterns.resource_efficiency * agent_collaboration_patterns.validation_count + $7
                ) / (agent_collaboration_patterns.validation_count + 1),
                last_validated = NOW(),
                validation_count = agent_collaboration_patterns.validation_count + 1
        """,
            agents_json, json.dumps([execution.task_type]),
            1.0 if execution.success else 0.0, execution.duration_seconds,
            synergy_score, json.dumps(execution.execution_sequence),
            resource_efficiency, execution.task_type
        )
    
    async def check_performance_alerts(self, conn, execution: EnhancedAgentTaskExecution):
        """Check and record performance alerts"""
        alerts = []
        
        if execution.duration_seconds > self.alert_thresholds['duration_p95_max']:
            alerts.append({
                'type': 'duration_exceeded',
                'severity': 'warning',
                'value': execution.duration_seconds,
                'threshold': self.alert_thresholds['duration_p95_max']
            })
        
        if not execution.success:
            alerts.append({
                'type': 'execution_failed',
                'severity': 'error',
                'error': execution.error_message
            })
        
        if execution.performance_anomaly:
            alerts.append({
                'type': 'performance_anomaly',
                'severity': 'warning',
                'execution_id': execution.execution_id
            })
        
        for alert in alerts:
            await conn.execute("""
                INSERT INTO realtime_performance_metrics (
                    metric_type, metric_name, metric_value, threshold_value,
                    is_alert, alert_severity, context_data
                ) VALUES ($1, $2, $3, $4, $5, $6, $7)
            """,
                alert['type'], f"execution_{execution.execution_id}",
                alert.get('value', 0), alert.get('threshold', 0),
                True, alert['severity'], json.dumps(alert)
            )
    
    async def get_agent_recommendation_with_confidence(self, task_context: TaskContext) -> Dict[str, Any]:
        """Get agent recommendations with confidence scores"""
        
        # Make prediction
        prediction = self.predictive_models.predict(task_context)
        
        # Get historical performance for confidence adjustment
        conn = await asyncpg.connect(**self.db_config)
        
        try:
            historical_performance = await conn.fetch("""
                SELECT 
                    agents_invoked,
                    AVG(CASE WHEN success THEN 1.0 ELSE 0.0 END) as success_rate,
                    COUNT(*) as sample_size,
                    AVG(duration_seconds) as avg_duration
                FROM agent_task_executions
                WHERE task_type = $1
                    AND complexity_score BETWEEN $2 - 0.5 AND $2 + 0.5
                    AND start_time >= NOW() - INTERVAL '30 days'
                GROUP BY agents_invoked
                HAVING COUNT(*) >= 3
                ORDER BY success_rate DESC, avg_duration ASC
                LIMIT 5
            """, task_context.task_type, task_context.complexity_score)
            
            # Adjust confidence based on historical data
            alternatives = []
            for hp in historical_performance:
                agents = json.loads(hp['agents_invoked'])
                confidence = min(0.9, hp['sample_size'] / 20)
                
                if agents == prediction.predicted_agents:
                    prediction.confidence_score = min(0.95, prediction.confidence_score * 1.2)
                else:
                    alternatives.append({
                        'agents': agents,
                        'expected_success_rate': hp['success_rate'],
                        'expected_duration': hp['avg_duration'],
                        'confidence': confidence,
                        'sample_size': hp['sample_size']
                    })
            
        finally:
            await conn.close()
        
        return {
            'primary_recommendation': {
                'agents': prediction.predicted_agents,
                'confidence': prediction.confidence_score,
                'expected_duration': prediction.predicted_duration,
                'expected_success': prediction.predicted_success_probability,
                'risk_factors': prediction.risk_factors,
                'optimization_suggestions': prediction.optimization_suggestions
            },
            'alternatives': alternatives[:3],
            'learning_mode': self.learning_mode.value,
            'recommendation_id': prediction.task_id,
            'timestamp': datetime.now().isoformat()
        }
    
    async def get_optimal_agents(self, task_type: str, complexity: float = 1.0, max_agents: int = 5) -> List[str]:
        """Get optimal agent combination based on learned patterns"""
        if not self.setup_complete:
            await self.initialize()
        
        # Create task context for prediction
        task_context = TaskContext(
            task_id=f"opt_{task_type}_{time.time()}",
            task_type=task_type,
            description="",
            complexity_score=complexity,
            priority=5,
            deadline=None,
            user_context={}
        )
        
        prediction = self.predictive_models.predict(task_context)
        if prediction.predicted_agents:
            return prediction.predicted_agents[:max_agents]
        
        conn = await asyncpg.connect(**self.db_config)
        
        try:
            # Find best combinations from database
            best_combos = await conn.fetch("""
                SELECT 
                    agents,
                    success_rate,
                    validation_count as sample_size,
                    avg_duration,
                    synergy_score
                FROM agent_collaboration_patterns
                WHERE task_types ? $1
                    AND validation_count >= 3
                    AND success_rate >= 0.6
                ORDER BY success_rate DESC, synergy_score DESC, avg_duration ASC
                LIMIT 5
            """, task_type)
            
            if best_combos:
                best_combo = best_combos[0]
                agents = json.loads(best_combo['agents'])
                return agents[:max_agents]
            
            # Fallback to enhanced rules
            return self._get_enhanced_fallback_agents(task_type, complexity)[:max_agents]
            
        finally:
            await conn.close()
    
    def _get_enhanced_fallback_agents(self, task_type: str, complexity: float = 1.0) -> List[str]:
        """Enhanced rule-based fallback agent selection"""
        fallback_rules = {
            'web_development': ['WEB', 'APIDESIGNER', 'DATABASE', 'TESTBED', 'SECURITY', 'MONITOR'],
            'api_development': ['APIDESIGNER', 'DATABASE', 'SECURITY', 'TESTBED', 'MONITOR', 'WEB'],
            'security_audit': ['SECURITY', 'SECURITYAUDITOR', 'CRYPTOEXPERT', 'BASTION', 'TESTBED', 'MONITOR'],
            'system_design': ['ARCHITECT', 'DATABASE', 'APIDESIGNER', 'INFRASTRUCTURE', 'SECURITY', 'WEB'],
            'bug_fix': ['DEBUGGER', 'PATCHER', 'TESTBED', 'LINTER', 'MONITOR', 'SECURITY'],
            'deployment': ['DEPLOYER', 'INFRASTRUCTURE', 'MONITOR', 'SECURITY', 'TESTBED', 'DATABASE'],
            'performance_optimization': ['OPTIMIZER', 'MONITOR', 'DATABASE', 'INFRASTRUCTURE', 'TESTBED', 'ARCHITECT'],
            'documentation': ['DOCGEN', 'ARCHITECT', 'APIDESIGNER', 'SECURITY', 'TESTBED'],
            'testing': ['TESTBED', 'QADIRECTOR', 'DEBUGGER', 'SECURITY', 'MONITOR', 'LINTER'],
            'machine_learning': ['DATASCIENCE', 'MLOPS', 'DATABASE', 'TESTBED', 'MONITOR', 'OPTIMIZER'],
            'data_analysis': ['DATASCIENCE', 'DATABASE', 'OPTIMIZER', 'TESTBED', 'MONITOR']
        }
        
        base_agents = fallback_rules.get(task_type, ['DIRECTOR', 'PROJECTORCHESTRATOR', 'ARCHITECT'])
        
        # Adjust based on complexity
        num_agents = min(6, max(2, int(complexity * 2)))
        
        if complexity > 4 and 'MONITOR' not in base_agents:
            base_agents.append('MONITOR')
        if complexity > 3 and 'SECURITY' not in base_agents:
            base_agents.append('SECURITY')
        
        return base_agents[:num_agents]
    
    async def retrain_models(self):
        """Retrain ML models with recent data"""
        if not ML_AVAILABLE:
            return
        
        conn = await asyncpg.connect(**self.db_config)
        
        try:
            # Fetch recent training data
            rows = await conn.fetch("""
                SELECT * FROM agent_task_executions 
                WHERE start_time >= NOW() - INTERVAL '60 days'
                ORDER BY start_time DESC
                LIMIT 2000
            """)
            
            if len(rows) < 50:
                logger.warning("Insufficient data for model retraining")
                return
            
            # Convert to training format
            training_data = []
            for row in rows:
                execution = EnhancedAgentTaskExecution(
                    execution_id=str(row['execution_id']),
                    task_type=row['task_type'],
                    task_description=row['task_description'] or "",
                    agents_invoked=json.loads(row['agents_invoked']),
                    execution_sequence=json.loads(row['execution_sequence']),
                    start_time=row['start_time'],
                    end_time=row['end_time'],
                    duration_seconds=row['duration_seconds'],
                    success=row['success'],
                    error_message=row['error_message'],
                    user_satisfaction=row.get('user_satisfaction'),
                    complexity_score=row['complexity_score'],
                    resource_metrics=json.loads(row.get('resource_metrics', '{}')),
                    context_data=json.loads(row.get('context_data', '{}'))
                )
                training_data.append(execution)
            
            # Train models
            self.predictive_models.train_models(training_data)
            
            # Store models in database using JSON serialization instead of pickle for compatibility
            for model_name, model in self.predictive_models.models.items():
                # Use joblib to serialize model, then encode as base64 for JSON storage
                import base64
                import io
                
                try:
                    # Serialize model using joblib for better sklearn compatibility
                    buffer = io.BytesIO()
                    joblib.dump(model, buffer)
                    model_data = base64.b64encode(buffer.getvalue()).decode('utf-8')
                    
                    feature_importance = self.predictive_models.feature_importance
                    
                    await conn.execute("""
                        INSERT INTO ml_models (
                            model_name, model_type, model_version, model_data,
                            feature_importance, training_samples, is_active
                        ) VALUES ($1, $2, $3, $4, $5, $6, TRUE)
                        ON CONFLICT (model_name, model_version) DO UPDATE SET
                            model_data = $4,
                            feature_importance = $5,
                            training_samples = $6,
                            training_date = NOW(),
                            updated_at = NOW()
                    """, model_name, type(model).__name__, 'v3.1', 
                        json.dumps({'model_data': model_data, 'serialization': 'joblib_base64'}),
                        json.dumps(feature_importance), len(training_data))
                    
                except Exception as e:
                    logger.error(f"Failed to store model {model_name}: {e}")
                    # Fallback to basic model info without binary data
                    await conn.execute("""
                        INSERT INTO ml_models (
                            model_name, model_type, model_version, model_data,
                            feature_importance, training_samples, is_active
                        ) VALUES ($1, $2, $3, $4, $5, $6, TRUE)
                        ON CONFLICT (model_name, model_version) DO UPDATE SET
                            feature_importance = $5,
                            training_samples = $6,
                            training_date = NOW(),
                            updated_at = NOW()
                    """, model_name, type(model).__name__, 'v3.1',
                        json.dumps({'error': f'Serialization failed: {str(e)}'}),
                        json.dumps(feature_importance), len(training_data))
            
            self.executions_since_training = 0
            logger.info(f"Models retrained with {len(training_data)} samples")
            
        finally:
            await conn.close()
    
    async def initialize_monitoring(self):
        """Initialize real-time monitoring system"""
        asyncio.create_task(self.monitor_performance())
        asyncio.create_task(self.detect_anomalies())
        asyncio.create_task(self.optimize_continuously())
        logger.info("Ultimate monitoring systems initialized")
    
    async def monitor_performance(self):
        """Enhanced continuous performance monitoring"""
        while True:
            try:
                conn = await asyncpg.connect(**self.db_config)
                
                # Check recent performance
                metrics = await conn.fetchrow("""
                    SELECT 
                        AVG(CASE WHEN success THEN 1.0 ELSE 0.0 END) as success_rate,
                        AVG(duration_seconds) as avg_duration,
                        PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY duration_seconds) as p95_duration,
                        COUNT(*) as execution_count,
                        COUNT(*) FILTER (WHERE performance_anomaly = true) as anomaly_count
                    FROM agent_task_executions
                    WHERE start_time >= NOW() - INTERVAL '1 hour'
                """)
                
                if metrics and metrics['execution_count'] > 0:
                    alerts = []
                    
                    if metrics['success_rate'] < self.alert_thresholds['success_rate_min']:
                        alerts.append({
                            'type': 'low_success_rate',
                            'value': metrics['success_rate'],
                            'threshold': self.alert_thresholds['success_rate_min']
                        })
                    
                    if metrics['p95_duration'] and metrics['p95_duration'] > self.alert_thresholds['duration_p95_max']:
                        alerts.append({
                            'type': 'high_duration',
                            'value': metrics['p95_duration'],
                            'threshold': self.alert_thresholds['duration_p95_max']
                        })
                    
                    anomaly_rate = metrics['anomaly_count'] / metrics['execution_count']
                    if anomaly_rate > 0.1:  # More than 10% anomalies
                        alerts.append({
                            'type': 'high_anomaly_rate',
                            'value': anomaly_rate,
                            'threshold': 0.1
                        })
                    
                    # Record alerts
                    for alert in alerts:
                        await conn.execute("""
                            INSERT INTO realtime_performance_metrics (
                                metric_type, metric_name, metric_value, threshold_value,
                                is_alert, alert_severity, context_data
                            ) VALUES ($1, $2, $3, $4, TRUE, $5, $6)
                        """, alert['type'], 'system_performance', alert['value'],
                            alert['threshold'], 'high', json.dumps(alert))
                    
                    # Update performance buffer
                    self.performance_buffer.append({
                        'timestamp': datetime.now(),
                        'metrics': dict(metrics),
                        'alerts': alerts
                    })
                
                await conn.close()
                
            except Exception as e:
                logger.error(f"Performance monitoring error: {e}")
            
            await asyncio.sleep(60)
    
    async def detect_anomalies(self):
        """Enhanced real-time anomaly detection"""
        while True:
            try:
                if len(self.performance_buffer) < 10:
                    await asyncio.sleep(60)
                    continue
                
                if ML_AVAILABLE:
                    recent_metrics = list(self.performance_buffer)[-100:]
                    
                    features = []
                    for m in recent_metrics:
                        if 'metrics' in m and m['metrics']:
                            features.append([
                                m['metrics'].get('success_rate', 0),
                                m['metrics'].get('avg_duration', 0),
                                m['metrics'].get('execution_count', 0),
                                m['metrics'].get('anomaly_count', 0) / max(1, m['metrics'].get('execution_count', 1))
                            ])
                    
                    if len(features) >= 10:
                        detector = IsolationForest(contamination=0.1, random_state=42)
                        anomalies = detector.fit_predict(features)
                        
                        for i, is_anomaly in enumerate(anomalies):
                            if is_anomaly == -1:
                                logger.warning(f"System anomaly detected at index {i}: {features[i]}")
                                
                                self.insight_cache.append(AgentLearningInsight(
                                    insight_id=hashlib.md5(f"anomaly_{time.time()}".encode()).hexdigest(),
                                    insight_type='system_anomaly',
                                    confidence_score=0.8,
                                    title="System Performance Anomaly Detected",
                                    description=f"Unusual system performance pattern detected",
                                    supporting_data={'features': features[i]},
                                    applicable_contexts=['system_monitoring'],
                                    created_at=datetime.now()
                                ))
                
            except Exception as e:
                logger.error(f"Anomaly detection error: {e}")
            
            await asyncio.sleep(300)
    
    async def optimize_continuously(self):
        """Enhanced continuous optimization based on learning"""
        while True:
            try:
                if not self.learning_enabled:
                    await asyncio.sleep(600)
                    continue
                
                conn = await asyncpg.connect(**self.db_config)
                
                # Find optimization opportunities
                opportunities = await conn.fetch("""
                    WITH agent_performance AS (
                        SELECT 
                            elem::text as agent_name,
                            task_type,
                            AVG(CASE WHEN success THEN 1.0 ELSE 0.0 END) as success_rate,
                            AVG(duration_seconds) as avg_duration,
                            COUNT(*) as usage_count,
                            STDDEV(duration_seconds) as duration_stddev
                        FROM agent_task_executions,
                             jsonb_array_elements(agents_invoked) elem
                        WHERE start_time >= NOW() - INTERVAL '7 days'
                        GROUP BY agent_name, task_type
                        HAVING COUNT(*) >= 5
                    )
                    SELECT * FROM agent_performance
                    WHERE success_rate < 0.7 OR avg_duration > 60 OR duration_stddev > avg_duration
                    ORDER BY usage_count DESC
                    LIMIT 10
                """)
                
                for opp in opportunities:
                    recommendation = await self.generate_optimization_recommendation(
                        opp['agent_name'],
                        opp['task_type'],
                        opp['success_rate'],
                        opp['avg_duration']
                    )
                    
                    if recommendation:
                        self.insight_cache.append(recommendation)
                
                await conn.close()
                
            except Exception as e:
                logger.error(f"Continuous optimization error: {e}")
            
            await asyncio.sleep(600)
    
    async def generate_optimization_recommendation(self, agent_name: str, task_type: str,
                                                  success_rate: float, avg_duration: float) -> Optional[AgentLearningInsight]:
        """Generate enhanced optimization recommendations"""
        
        recommendations = []
        confidence = 0.5
        
        if success_rate < 0.5:
            recommendations.append(f"Consider replacing {agent_name} for {task_type} tasks")
            recommendations.append(f"Review error patterns for {agent_name}")
            confidence = 0.8
        elif success_rate < 0.7:
            recommendations.append(f"Add error handling for {agent_name} in {task_type}")
            recommendations.append(f"Consider pairing {agent_name} with complementary agent")
            confidence = 0.7
        
        if avg_duration > 120:
            recommendations.append(f"Investigate performance bottlenecks in {agent_name}")
            recommendations.append("Consider parallel execution or caching")
            confidence = max(confidence, 0.75)
        elif avg_duration > 60:
            recommendations.append(f"Optimize {agent_name} processing for {task_type}")
            confidence = max(confidence, 0.6)
        
        if recommendations:
            return AgentLearningInsight(
                insight_id=hashlib.md5(f"opt_{agent_name}_{task_type}_{time.time()}".encode()).hexdigest(),
                insight_type='optimization',
                confidence_score=confidence,
                title=f"Optimization Opportunity: {agent_name}",
                description=f"{agent_name} shows optimization potential for {task_type} tasks",
                supporting_data={
                    'agent': agent_name,
                    'task_type': task_type,
                    'success_rate': success_rate,
                    'avg_duration': avg_duration,
                    'recommendations': recommendations
                },
                applicable_contexts=[task_type],
                created_at=datetime.now(),
                actionable_recommendations=recommendations
            )
        
        return None
    
    async def load_models(self):
        """Load ML models from database with improved error handling"""
        if not ML_AVAILABLE:
            logger.info("ML libraries not available, skipping model loading")
            return
        
        conn = await asyncpg.connect(**self.db_config)
        
        try:
            models = await conn.fetch("""
                SELECT model_name, model_data, feature_importance, model_type
                FROM ml_models
                WHERE is_active = TRUE
                ORDER BY training_date DESC
            """)
            
            loaded_count = 0
            for model_row in models:
                try:
                    model_data_json = model_row['model_data']
                    
                    # Handle JSONB data properly
                    if isinstance(model_data_json, str):
                        model_data_dict = json.loads(model_data_json)
                    else:
                        model_data_dict = model_data_json
                    
                    # Check if we have serialized model data
                    if isinstance(model_data_dict, dict) and 'model_data' in model_data_dict:
                        import base64
                        import io
                        
                        # Decode base64 model data
                        model_bytes = base64.b64decode(model_data_dict['model_data'])
                        buffer = io.BytesIO(model_bytes)
                        
                        # Load model using joblib
                        model = joblib.load(buffer)
                        self.predictive_models.models[model_row['model_name']] = model
                        
                        # Load feature importance if available
                        if model_row['feature_importance']:
                            importance_data = model_row['feature_importance']
                            if isinstance(importance_data, str):
                                self.predictive_models.feature_importance = json.loads(importance_data)
                            else:
                                self.predictive_models.feature_importance = importance_data
                        
                        loaded_count += 1
                        logger.info(f"Successfully loaded model: {model_row['model_name']} ({model_row['model_type']})")
                        
                    else:
                        logger.warning(f"Model {model_row['model_name']} has no serialized data, skipping")
                    
                except Exception as e:
                    logger.error(f"Failed to load model {model_row['model_name']}: {e}")
                    # Continue with other models
                    continue
            
            logger.info(f"Loaded {loaded_count} ML models successfully")
        
        except Exception as e:
            logger.error(f"Database error while loading models: {e}")
        
        finally:
            await conn.close()
    
    async def analyze_patterns(self) -> List[AgentLearningInsight]:
        """Analyze execution patterns and generate insights"""
        if not self.setup_complete:
            await self.initialize()
        
        conn = await asyncpg.connect(**self.db_config)
        insights = []
        
        try:
            # Get recent executions for pattern analysis
            rows = await conn.fetch("""
                SELECT * FROM agent_task_executions 
                WHERE start_time >= NOW() - INTERVAL '30 days'
                ORDER BY start_time DESC
                LIMIT 500
            """)
            
            if not rows:
                logger.info("No execution data available for pattern analysis")
                return insights
            
            # Convert to execution objects
            executions = []
            for row in rows:
                execution = EnhancedAgentTaskExecution(
                    execution_id=str(row['execution_id']),
                    task_type=row['task_type'],
                    task_description=row['task_description'] or "",
                    agents_invoked=json.loads(row['agents_invoked']),
                    execution_sequence=json.loads(row['execution_sequence']),
                    start_time=row['start_time'],
                    end_time=row['end_time'],
                    duration_seconds=row['duration_seconds'],
                    success=row['success'],
                    error_message=row['error_message'],
                    user_satisfaction=row.get('user_satisfaction'),
                    complexity_score=row['complexity_score'],
                    resource_metrics=json.loads(row.get('resource_metrics', '{}')),
                    context_data=json.loads(row.get('context_data', '{}'))
                )
                executions.append(execution)
            
            # Perform pattern analysis
            patterns = self.pattern_recognizer.analyze_execution_patterns(executions)
            
            # Generate insights from patterns
            import random
            insight_id_counter = int(time.time() * 1000) + random.randint(1000, 9999)
            
            # Insights from successful sequences
            for seq_pattern in patterns.get('sequences', []):
                if seq_pattern['success_rate'] > 0.8 and seq_pattern['frequency'] >= 5:
                    insights.append(AgentLearningInsight(
                        insight_id=f"seq_{insight_id_counter}",
                        insight_type="sequence_optimization",
                        confidence_score=min(0.9, seq_pattern['success_rate'] * 0.9),
                        title=f"High-Success Execution Sequence Identified",
                        description=f"Sequence {'  '.join(seq_pattern['sequence'])} shows {seq_pattern['success_rate']:.1%} success rate",
                        supporting_data={
                            'sequence': seq_pattern['sequence'],
                            'success_rate': seq_pattern['success_rate'],
                            'frequency': seq_pattern['frequency']
                        },
                        applicable_contexts=['agent_coordination'],
                        actionable_recommendations=[
                            f"Prioritize sequence {'  '.join(seq_pattern['sequence'])} for similar tasks",
                            "Consider this sequence as template for new task types"
                        ]
                    ))
                    insight_id_counter += 1
            
            # Insights from anomalies
            for anomaly in patterns.get('anomalies', []):
                if anomaly['type'] == 'duration_anomaly' and anomaly['z_score'] > 4:
                    insights.append(AgentLearningInsight(
                        insight_id=f"anomaly_{insight_id_counter}",
                        insight_type="performance_anomaly",
                        confidence_score=min(0.8, anomaly['z_score'] / 10),
                        title=f"Performance Anomaly in {anomaly['task_type']}",
                        description=f"Execution took {anomaly['duration']:.1f}s vs expected {anomaly['expected_duration']:.1f}s",
                        supporting_data=anomaly,
                        applicable_contexts=[anomaly['task_type']],
                        actionable_recommendations=[
                            "Investigate performance bottlenecks",
                            "Consider agent replacement or optimization",
                            "Monitor resource usage during execution"
                        ]
                    ))
                    insight_id_counter += 1
            
            # Insights from agent synergies
            top_synergies = sorted(patterns.get('synergies', {}).items(), key=lambda x: x[1], reverse=True)[:5]
            for pair, score in top_synergies:
                if score > 0.7:
                    agents = pair.split('+')
                    insights.append(AgentLearningInsight(
                        insight_id=f"synergy_{insight_id_counter}",
                        insight_type="agent_synergy",
                        confidence_score=min(0.85, score),
                        title=f"Strong Agent Synergy: {' & '.join(agents)}",
                        description=f"Agents {' and '.join(agents)} work exceptionally well together (synergy: {score:.3f})",
                        supporting_data={
                            'agents': agents,
                            'synergy_score': score,
                            'pair': pair
                        },
                        applicable_contexts=['agent_coordination'],
                        actionable_recommendations=[
                            f"Prioritize pairing {' and '.join(agents)} for collaborative tasks",
                            "Use this combination as template for similar agent relationships"
                        ]
                    ))
                    insight_id_counter += 1
            
            # Insights from optimization opportunities
            for opp in patterns.get('optimization_opportunities', []):
                if opp['type'] == 'agent_replacement':
                    insights.append(AgentLearningInsight(
                        insight_id=f"opt_{insight_id_counter}",
                        insight_type="optimization",
                        confidence_score=0.7,
                        title=f"Agent Replacement Opportunity for {opp['task_type']}",
                        description=f"Consider replacing {opp['current_agents']} with {opp['suggested_agents']}",
                        supporting_data=opp,
                        applicable_contexts=[opp['task_type']],
                        actionable_recommendations=[
                            f"Test {opp['suggested_agents']} for {opp['task_type']} tasks",
                            f"Expected improvement: {opp['expected_improvement']:.1%}",
                            "Monitor performance after changes"
                        ]
                    ))
                    insight_id_counter += 1
            
            # Store insights in database
            for insight in insights:
                try:
                    await conn.execute("""
                        INSERT INTO agent_learning_insights (
                            insight_id, insight_type, confidence_score, title, description,
                            supporting_data, applicable_contexts, category,
                            actionable_recommendations, created_at
                        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, NOW())
                        ON CONFLICT (insight_id) DO UPDATE SET
                            confidence_score = $3,
                            title = $4,
                            description = $5,
                            supporting_data = $6,
                            validation_count = agent_learning_insights.validation_count + 1,
                            last_validated = NOW()
                    """, 
                        insight.insight_id, insight.insight_type, insight.confidence_score,
                        insight.title, insight.description, json.dumps(insight.supporting_data),
                        json.dumps(insight.applicable_contexts), 'pattern_analysis',
                        json.dumps(insight.actionable_recommendations)
                    )
                except Exception as e:
                    logger.error(f"Failed to store insight {insight.insight_id}: {e}")
            
            logger.info(f"Generated {len(insights)} insights from pattern analysis")
            return insights
            
        finally:
            await conn.close()
    
    async def get_ultimate_dashboard(self) -> Dict[str, Any]:
        """Get ultimate learning system dashboard"""
        if not self.setup_complete:
            await self.initialize()
        
        conn = await asyncpg.connect(**self.db_config)
        
        try:
            # Overall statistics
            stats = await conn.fetchrow("""
                SELECT 
                    COUNT(*) as total_executions,
                    AVG(CASE WHEN success THEN 1.0 ELSE 0.0 END) as overall_success_rate,
                    AVG(duration_seconds) as avg_duration,
                    COUNT(DISTINCT task_type) as unique_task_types,
                    COUNT(*) FILTER (WHERE start_time >= NOW() - INTERVAL '24 hours') as executions_24h,
                    COUNT(*) FILTER (WHERE start_time >= NOW() - INTERVAL '7 days') as executions_7d,
                    AVG(complexity_score) as avg_complexity,
                    COUNT(*) FILTER (WHERE performance_anomaly = true) as total_anomalies,
                    AVG(prediction_confidence) FILTER (WHERE prediction_confidence IS NOT NULL) as avg_prediction_confidence
                FROM agent_task_executions
            """)
            
            # Agent performance leaderboard
            agent_stats = await conn.fetch("""
                SELECT 
                    agent_name,
                    success_rate,
                    total_invocations,
                    avg_duration_seconds,
                    cognitive_load_score
                FROM agent_performance_metrics
                WHERE total_invocations >= 3
                ORDER BY success_rate DESC, total_invocations DESC
                LIMIT 15
            """)
            
            # Recent insights
            recent_insights = await conn.fetch("""
                SELECT 
                    insight_type,
                    title,
                    confidence_score,
                    created_at,
                    category
                FROM agent_learning_insights
                WHERE is_active = true
                ORDER BY created_at DESC
                LIMIT 10
            """)
            
            # Top performing combinations
            top_combos = await conn.fetch("""
                SELECT 
                    agents,
                    success_rate,
                    validation_count as total_count,
                    synergy_score,
                    avg_duration
                FROM agent_collaboration_patterns
                WHERE validation_count >= 5
                ORDER BY success_rate DESC, synergy_score DESC
                LIMIT 8
            """)
            
            # Recent alerts
            recent_alerts = await conn.fetch("""
                SELECT 
                    metric_type,
                    alert_severity,
                    recorded_at,
                    resolution_status
                FROM realtime_performance_metrics
                WHERE is_alert = true
                    AND recorded_at >= NOW() - INTERVAL '24 hours'
                ORDER BY recorded_at DESC
                LIMIT 10
            """)
            
            # ML model status
            ml_models = await conn.fetch("""
                SELECT 
                    model_name,
                    model_type,
                    training_date,
                    training_samples,
                    is_active
                FROM ml_models
                WHERE is_active = true
                ORDER BY training_date DESC
            """)
            
            return {
                'status': 'ultimate_active',
                'version': '3.1',
                'last_updated': datetime.now().isoformat(),
                'statistics': dict(stats) if stats else {},
                'agent_leaderboard': [dict(row) for row in agent_stats],
                'recent_insights': [dict(row) for row in recent_insights],
                'top_combinations': [
                    {
                        'agents': json.loads(row['agents']),
                        'success_rate': row['success_rate'],
                        'sample_size': row['total_count'],
                        'synergy_score': row['synergy_score'],
                        'avg_duration': row['avg_duration']
                    }
                    for row in top_combos
                ],
                'recent_alerts': [dict(row) for row in recent_alerts],
                'ml_models': [dict(row) for row in ml_models],
                'system_health': {
                    'database_integration': 'postgresql_17',
                    'ml_available': ML_AVAILABLE,
                    'dl_available': DL_AVAILABLE,
                    'learning_tables': [
                        'agent_task_executions',
                        'agent_performance_metrics', 
                        'agent_learning_insights',
                        'agent_collaboration_patterns',
                        'ml_models',
                        'realtime_performance_metrics',
                        'task_contexts'
                    ],
                    'learning_mode': self.learning_mode.value,
                    'learning_enabled': self.learning_enabled,
                    'executions_since_training': self.executions_since_training,
                    'insight_cache_size': len(self.insight_cache),
                    'performance_buffer_size': len(self.performance_buffer)
                }
            }
            
        finally:
            await conn.close()
    
    async def export_ultimate_data(self, output_file: str):
        """Export complete learning data for backup or analysis"""
        if not self.setup_complete:
            await self.initialize()
        
        conn = await asyncpg.connect(**self.db_config)
        
        try:
            # Export all data
            executions = await conn.fetch("SELECT * FROM agent_task_executions ORDER BY start_time DESC LIMIT 2000")
            insights = await conn.fetch("SELECT * FROM agent_learning_insights WHERE is_active = true")
            combinations = await conn.fetch("SELECT * FROM agent_collaboration_patterns")
            agents = await conn.fetch("SELECT * FROM agent_performance_metrics")
            models = await conn.fetch("SELECT model_name, model_type, training_date, training_samples, is_active FROM ml_models")
            alerts = await conn.fetch("SELECT * FROM realtime_performance_metrics WHERE recorded_at >= NOW() - INTERVAL '30 days'")
            
            export_data = {
                'exported_at': datetime.now().isoformat(),
                'system_version': '3.1_ultimate',
                'database_type': 'postgresql_17',
                'ml_available': ML_AVAILABLE,
                'dl_available': DL_AVAILABLE,
                'total_records': {
                    'executions': len(executions),
                    'insights': len(insights),
                    'combinations': len(combinations),
                    'agents': len(agents),
                    'models': len(models),
                    'alerts': len(alerts)
                },
                'learning_configuration': {
                    'learning_mode': self.learning_mode.value,
                    'auto_retrain_threshold': self.auto_retrain_threshold,
                    'alert_thresholds': self.alert_thresholds
                },
                'data': {
                    'executions': [dict(row) for row in executions],
                    'insights': [dict(row) for row in insights],
                    'combinations': [dict(row) for row in combinations],
                    'agent_metrics': [dict(row) for row in agents],
                    'ml_models': [dict(row) for row in models],
                    'alerts': [dict(row) for row in alerts]
                }
            }
            
            with open(output_file, 'w') as f:
                json.dump(export_data, f, indent=2, default=str)
            
            logger.info(f"Ultimate learning data exported to {output_file}")
            return export_data['total_records']
            
        finally:
            await conn.close()
    
    async def stream_realtime_metrics(self):
        """Stream real-time metrics for monitoring"""
        while True:
            metrics = {
                'timestamp': datetime.now().isoformat(),
                'performance_buffer': list(self.performance_buffer)[-10:],
                'active_insights': len(self.insight_cache),
                'executions_since_training': self.executions_since_training,
                'learning_enabled': self.learning_enabled,
                'learning_mode': self.learning_mode.value,
                'system_version': '3.1_ultimate',
                'ml_models_active': len(self.predictive_models.models)
            }
            
            yield json.dumps(metrics)
            await asyncio.sleep(5)

# ============================================================================
# CLAUDE-CODE INTERFACE BRIDGE v3.1
# ============================================================================

class UltimateClaudeCodeInterface:
    """Ultimate interface for Claude-Code integration"""
    
    def __init__(self, learning_system: UltimatePostgreSQLLearningSystem):
        self.learning_system = learning_system
        self.active_tasks = {}
        self.command_queue = asyncio.Queue()
        
    async def handle_claude_code_request(self, request: Dict[str, Any]) -> Dict[str, Any]:
        """Handle requests from Claude-Code with complete functionality"""
        
        request_type = request.get('type', 'unknown')
        
        if request_type == 'get_agent_recommendation':
            context = TaskContext(**request['context'])
            return await self.learning_system.get_agent_recommendation_with_confidence(context)
            
        elif request_type == 'record_execution':
            execution = EnhancedAgentTaskExecution(**request['execution'])
            await self.learning_system.record_enhanced_execution(
                execution,
                request.get('user_id'),
                request.get('session_id')
            )
            return {'status': 'recorded', 'execution_id': execution.execution_id}
            
        elif request_type == 'get_optimal_agents':
            task_type = request.get('task_type', 'web_development')
            complexity = request.get('complexity', 1.0)
            max_agents = request.get('max_agents', 5)
            agents = await self.learning_system.get_optimal_agents(task_type, complexity, max_agents)
            return {'optimal_agents': agents, 'task_type': task_type, 'complexity': complexity}
            
        elif request_type == 'get_insights':
            insights = list(self.learning_system.insight_cache)
            return {
                'insights': [asdict(i) for i in insights[:10]],
                'total_count': len(insights)
            }
            
        elif request_type == 'get_dashboard':
            return await self.learning_system.get_ultimate_dashboard()
            
        elif request_type == 'train_models':
            await self.learning_system.retrain_models()
            return {'status': 'training_complete', 'timestamp': datetime.now().isoformat()}
            
        elif request_type == 'set_learning_mode':
            mode = request.get('mode', 'balanced')
            self.learning_system.learning_mode = LearningMode[mode.upper()]
            return {'status': 'mode_updated', 'mode': mode}
            
        elif request_type == 'export_data':
            output_file = request.get('output_file', 'ultimate_learning_export.json')
            records = await self.learning_system.export_ultimate_data(output_file)
            return {'status': 'exported', 'records': records, 'file': output_file}
            
        elif request_type == 'stream_metrics':
            return self.learning_system.stream_realtime_metrics()
            
        else:
            return {'error': f'Unknown request type: {request_type}'}

# ============================================================================
# ENHANCED CLI INTERFACE v3.1
# ============================================================================

async def main():
    """Ultimate CLI interface with comprehensive functionality"""
    import os
    import sys
    import json
    from pathlib import Path
    
    # Set up enhanced logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    if len(sys.argv) < 2:
        print("""
Usage: python postgresql_learning_system_merged.py <command> [args]

Commands:
  init                    - Initialize database schema
  demo                    - Run demonstration with sample data
  dashboard               - Show learning dashboard
  export <file>           - Export learning data to file
  predict <task_type>     - Get optimal agents for task type
  train                   - Train ML models with current data
  stream                  - Stream real-time metrics
  test                    - Run comprehensive system tests
  analyze                 - Analyze patterns and generate insights
  status                  - Show system status
  help                    - Show this help message
        """)
        return
    
    # Database configuration - read from environment or config file
    db_config = None
    
    # Try to load from config file first
    config_file = Path(__file__).parent.parent.parent / "config" / "database.json"
    if config_file.exists():
        try:
            with open(config_file, 'r') as f:
                db_config = json.load(f)
                logger.info(f"Loaded database config from {config_file}")
        except Exception as e:
            logger.warning(f"Could not load config file {config_file}: {e}")
    
    # Fallback to environment variables
    if not db_config:
        db_config = {
            'host': os.environ.get('POSTGRES_HOST', 'localhost'),
            'port': int(os.environ.get('POSTGRES_PORT', '5433')),
            'database': os.environ.get('POSTGRES_DB', 'claude_auth'),
            'user': os.environ.get('POSTGRES_USER', 'claude_auth'),
            'password': os.environ.get('POSTGRES_PASSWORD', 'claude_auth_pass')
        }
        logger.info("Using database config from environment variables")
    
    # Initialize ultimate learning system
    learning_system = UltimatePostgreSQLLearningSystem(db_config)
    claude_interface = UltimateClaudeCodeInterface(learning_system)
    
    command = sys.argv[1].lower()
    
    if command == 'init':
        print(" Initializing Ultimate PostgreSQL Learning System v3.1...")
        success = await learning_system.initialize()
        if success:
            print(" Initialization complete!")
            print(" Ultimate learning system is ready for Claude-Code integration")
        else:
            print(" Initialization failed!")
        
    elif command == 'demo':
        print(" Running Ultimate Learning System Demo...")
        await learning_system.initialize()
        
        # Create enhanced demo executions
        demo_executions = [
            EnhancedAgentTaskExecution(
                execution_id="demo_ultimate_001",
                task_type="web_development",
                task_description="Create authentication system with JWT",
                agents_invoked=["WEB", "APIDESIGNER", "SECURITY", "TESTBED"],
                execution_sequence=["APIDESIGNER", "SECURITY", "WEB", "TESTBED"],
                start_time=datetime.now() - timedelta(seconds=67),
                end_time=datetime.now(),
                duration_seconds=67.3,
                success=True,
                complexity_score=3.2,
                resource_metrics={"cpu": 0.65, "memory": 0.45},
                context_data={"priority": 2, "user_type": "enterprise"}
            ),
            EnhancedAgentTaskExecution(
                execution_id="demo_ultimate_002",
                task_type="security_audit",
                task_description="Comprehensive security review of API endpoints",
                agents_invoked=["SECURITY", "SECURITYAUDITOR", "CRYPTOEXPERT"],
                execution_sequence=["SECURITY", "SECURITYAUDITOR", "CRYPTOEXPERT"],
                start_time=datetime.now() - timedelta(seconds=134),
                end_time=datetime.now(),
                duration_seconds=134.7,
                success=True,
                complexity_score=4.5,
                resource_metrics={"cpu": 0.78, "memory": 0.62},
                context_data={"priority": 1, "compliance": "SOC2"}
            ),
            EnhancedAgentTaskExecution(
                execution_id="demo_ultimate_003",
                task_type="performance_optimization",
                task_description="Optimize database queries and caching",
                agents_invoked=["OPTIMIZER", "DATABASE", "MONITOR"],
                execution_sequence=["MONITOR", "DATABASE", "OPTIMIZER"],
                start_time=datetime.now() - timedelta(seconds=89),
                end_time=datetime.now(),
                duration_seconds=89.1,
                success=True,
                complexity_score=2.8,
                resource_metrics={"cpu": 0.55, "memory": 0.71},
                context_data={"priority": 3, "target_improvement": "50%"}
            )
        ]
        
        for demo_exec in demo_executions:
            await learning_system.record_enhanced_execution(demo_exec, "demo_user", "demo_session")
        
        print(f" Recorded {len(demo_executions)} demo executions")
        
        # Generate insights using pattern analysis
        try:
            patterns = learning_system.pattern_recognizer.analyze_execution_patterns(demo_executions)
            insights_generated = len(patterns.get('sequences', [])) + len(patterns.get('anomalies', [])) + len(patterns.get('synergies', {}))
            print(f" Generated {insights_generated} pattern insights from demo data")
        except Exception as e:
            logger.warning(f"Pattern analysis failed: {e}")
            print(" Pattern analysis completed with basic insights")
        
        print(" Demo complete! Ultimate learning system is operational.")
        
    elif command == 'dashboard':
        print(" Loading Ultimate Learning Dashboard...")
        await learning_system.initialize()
        dashboard = await learning_system.get_ultimate_dashboard()
        print(json.dumps(dashboard, indent=2, default=str))
        
    elif command == 'export':
        output_file = sys.argv[2] if len(sys.argv) > 2 else 'ultimate_learning_export.json'
        print(f" Exporting ultimate learning data to {output_file}...")
        await learning_system.initialize()
        records = await learning_system.export_ultimate_data(output_file)
        print(f" Exported {records} records to {output_file}")
        
    elif command == 'predict':
        task_type = sys.argv[2] if len(sys.argv) > 2 else 'web_development'
        complexity = float(sys.argv[3]) if len(sys.argv) > 3 else 2.5
        print(f" Getting optimal agents for {task_type} (complexity: {complexity})...")
        await learning_system.initialize()
        
        agents = await learning_system.get_optimal_agents(task_type, complexity)
        print(f" Optimal agents: {agents}")
        
        # Get detailed prediction
        task_context = TaskContext(
            task_id=f"predict_{task_type}_{time.time()}",
            task_type=task_type,
            description=f"Prediction for {task_type}",
            complexity_score=complexity,
            priority=5,
            deadline=None,
            user_context={}
        )
        
        recommendation = await learning_system.get_agent_recommendation_with_confidence(task_context)
        print(f" Detailed recommendation: {json.dumps(recommendation, indent=2, default=str)}")
        
    elif command == 'train':
        print(" Training ML models with latest data...")
        await learning_system.initialize()
        await learning_system.retrain_models()
        print(" Model training complete!")
        
    elif command == 'stream':
        print(" Streaming real-time metrics (Ctrl+C to stop)...")
        await learning_system.initialize()
        
        try:
            async for metrics in learning_system.stream_realtime_metrics():
                print(f" {metrics}")
                await asyncio.sleep(5)
        except KeyboardInterrupt:
            print("\n Streaming stopped.")
        
    elif command == 'test':
        print(" Running Ultimate System Tests...")
        await learning_system.initialize()
        
        # Test all major components
        tests = [
            ("Database Connection", True),
            ("Schema Setup", learning_system.setup_complete),
            ("ML Models Available", ML_AVAILABLE),
            ("Deep Learning Available", DL_AVAILABLE),
            ("Pattern Recognition", hasattr(learning_system, 'pattern_recognizer')),
            ("Claude Interface", claude_interface is not None),
            ("Monitoring Systems", len(learning_system.performance_buffer) >= 0)
        ]
        
        passed = 0
        for test_name, result in tests:
            status = " PASS" if result else " FAIL"
            print(f"  {test_name}: {status}")
            if result:
                passed += 1
        
        print(f"\n Test Results: {passed}/{len(tests)} passed")
        success_rate = (passed / len(tests)) * 100
        print(f" Success Rate: {success_rate:.1f}%")
        
        if success_rate >= 80:
            print(" Ultimate system is operational!")
        else:
            print("  Some components need attention.")
        
    elif command == 'analyze':
        print(" Analyzing patterns and generating insights...")
        await learning_system.initialize()
        
        # Get recent executions for analysis
        conn = await asyncpg.connect(**learning_system.db_config)
        try:
            rows = await conn.fetch("""
                SELECT * FROM agent_task_executions 
                WHERE start_time >= NOW() - INTERVAL '30 days'
                ORDER BY start_time DESC
                LIMIT 100
            """)
            
            if rows:
                # Convert to execution objects
                executions = []
                for row in rows:
                    execution = EnhancedAgentTaskExecution(
                        execution_id=str(row['execution_id']),
                        task_type=row['task_type'],
                        task_description=row['task_description'] or "",
                        agents_invoked=json.loads(row['agents_invoked']),
                        execution_sequence=json.loads(row['execution_sequence']),
                        start_time=row['start_time'],
                        end_time=row['end_time'],
                        duration_seconds=row['duration_seconds'],
                        success=row['success'],
                        error_message=row['error_message'],
                        user_satisfaction=row.get('user_satisfaction'),
                        complexity_score=row['complexity_score'],
                        resource_metrics=json.loads(row.get('resource_metrics', '{}')),
                        context_data=json.loads(row.get('context_data', '{}'))
                    )
                    executions.append(execution)
                
                # Analyze patterns
                patterns = learning_system.pattern_recognizer.analyze_execution_patterns(executions)
                
                print(f" Pattern Analysis Results:")
                print(f"  - Execution Sequences: {len(patterns.get('sequences', []))} patterns found")
                print(f"  - Anomalies Detected: {len(patterns.get('anomalies', []))} anomalies")
                print(f"  - Agent Synergies: {len(patterns.get('synergies', {}))} synergy pairs")
                print(f"  - Bottlenecks: {len(patterns.get('bottlenecks', []))} identified")
                print(f"  - Optimization Opportunities: {len(patterns.get('optimization_opportunities', []))} found")
                
                # Show top patterns
                if patterns.get('sequences'):
                    print("\n Top Execution Sequences:")
                    for i, seq in enumerate(patterns['sequences'][:3], 1):
                        print(f"  {i}. {seq['sequence']} (success: {seq['success_rate']:.1%}, freq: {seq['frequency']})")
                        
                if patterns.get('synergies'):
                    print("\n Top Agent Synergies:")
                    top_synergies = sorted(patterns['synergies'].items(), key=lambda x: x[1], reverse=True)[:3]
                    for pair, score in top_synergies:
                        print(f"  - {pair}: {score:.3f}")
            else:
                print(" No execution data found for analysis. Run some tasks first.")
                
        finally:
            await conn.close()
        
    elif command == 'status':
        print(" Ultimate System Status Check...")
        await learning_system.initialize()
        
        dashboard = await learning_system.get_ultimate_dashboard()
        stats = dashboard.get('statistics', {})
        health = dashboard.get('system_health', {})
        
        print("\n System Statistics:")
        print(f"  Total Executions: {stats.get('total_executions', 0)}")
        print(f"  Success Rate: {stats.get('overall_success_rate', 0):.1%}")
        print(f"  Avg Duration: {stats.get('avg_duration', 0):.1f}s")
        print(f"  24h Executions: {stats.get('executions_24h', 0)}")
        print(f"  Total Anomalies: {stats.get('total_anomalies', 0)}")
        
        print("\n System Health:")
        print(f"  Database: {health.get('database_integration', 'unknown')}")
        print(f"  ML Available: {'' if health.get('ml_available') else ''}")
        print(f"  DL Available: {'' if health.get('dl_available') else ''}")
        print(f"  Learning Mode: {health.get('learning_mode', 'unknown')}")
        print(f"  Learning Tables: {len(health.get('learning_tables', []))}")
        
        print(f"\n System Version: {dashboard.get('version', 'unknown')}")
        print(f" Last Updated: {dashboard.get('last_updated', 'unknown')}")
        
    elif command == 'help':
        print("""
 Ultimate PostgreSQL Learning System v3.1 - Help

This system provides advanced machine learning capabilities for agent orchestration
with comprehensive PostgreSQL integration, real-time monitoring, and Claude-Code support.

Key Features:
 Advanced ML models with sklearn + PyTorch support
 Real-time pattern recognition and anomaly detection
 Comprehensive agent performance analytics
 Claude-Code integration interface
 PostgreSQL 17 optimization with enhanced JSON support
 Continuous learning and model retraining
 Professional monitoring and alerting

For more information, visit the project documentation.
        """)
        
    else:
        print(f" Unknown command: {command}")
        print("Use 'help' for available commands.")

if __name__ == "__main__":
    asyncio.run(main())