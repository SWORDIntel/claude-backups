#!/usr/bin/env python3
"""
Async I/O Optimizer for Military-Grade Performance - Meteor Lake Enhanced
Optimized for Intel Meteor Lake architecture with AVX-512/AVX2 acceleration
"""

import asyncio
import aiofiles
import aiohttp
import uvloop  # High-performance event loop
from typing import List, Dict, Any, Optional, Tuple, AsyncIterator, Callable
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import io
import mmap
import os
import hashlib
import lz4.frame
import zstandard as zstd
import numpy as np
from dataclasses import dataclass, field
from enum import Enum
import time
import psutil
import struct
import ctypes
from pathlib import Path
import tempfile
import fcntl
import signal
import functools
import weakref
from collections import deque, defaultdict
import logging

# Set up high-performance event loop
asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())

logger = logging.getLogger(__name__)

# Direct I/O flags for bypassing cache
try:
    O_DIRECT = os.O_DIRECT  # Linux
except AttributeError:
    O_DIRECT = 0  # Not available on this platform


class IOPattern(Enum):
    """I/O access patterns for optimization"""
    SEQUENTIAL = "sequential"
    RANDOM = "random"
    STRIDED = "strided"
    BURST = "burst"


class CompressionType(Enum):
    """Compression algorithms with trade-offs"""
    NONE = "none"
    LZ4 = "lz4"        # Fastest
    ZSTD = "zstd"      # Best ratio
    SNAPPY = "snappy"  # Good balance


@dataclass
class IOMetrics:
    """I/O performance metrics"""
    bytes_read: int = 0
    bytes_written: int = 0
    read_operations: int = 0
    write_operations: int = 0
    cache_hits: int = 0
    cache_misses: int = 0
    compression_ratio: float = 1.0
    avg_latency_us: float = 0.0
    p99_latency_us: float = 0.0


class DirectIOBuffer:
    """Aligned buffer for Direct I/O operations"""
    
    ALIGNMENT = 512  # Typical sector size
    
    def __init__(self, size: int):
        # Ensure size is aligned
        self.size = (size + self.ALIGNMENT - 1) & ~(self.ALIGNMENT - 1)
        
        # Allocate aligned memory using ctypes
        self._raw_buffer = (ctypes.c_char * (self.size + self.ALIGNMENT))()
        self._address = ctypes.addressof(self._raw_buffer)
        
        # Align the address
        self.aligned_address = (self._address + self.ALIGNMENT - 1) & ~(self.ALIGNMENT - 1)
        self.offset = self.aligned_address - self._address
        
        # Create memoryview for the aligned portion
        self.buffer = (ctypes.c_char * self.size).from_address(self.aligned_address)
        self.memview = memoryview(self.buffer)
    
    def get_bytes(self, length: int) -> bytes:
        """Get bytes from buffer"""
        return bytes(self.memview[:length])
    
    def set_bytes(self, data: bytes):
        """Set bytes in buffer"""
        length = min(len(data), self.size)
        self.memview[:length] = data
        return length


class MemoryMappedRingBuffer:
    """Lock-free ring buffer using memory mapping"""
    
    def __init__(self, size: int = 1024 * 1024):  # 1MB default
        self.size = size
        self.write_pos = 0
        self.read_pos = 0
        
        # Create temporary file for mmap
        self.temp_file = tempfile.NamedTemporaryFile(delete=False)
        self.temp_file.write(b'\0' * size)
        self.temp_file.flush()
        
        # Memory map the file
        self.mmap = mmap.mmap(self.temp_file.fileno(), size)
    
    def write(self, data: bytes) -> bool:
        """Write data to ring buffer"""
        data_len = len(data)
        
        if data_len > self.available_write():
            return False
        
        # Write data (may wrap around)
        end_pos = self.write_pos + data_len
        
        if end_pos <= self.size:
            self.mmap[self.write_pos:end_pos] = data
        else:
            # Wrap around
            first_part = self.size - self.write_pos
            self.mmap[self.write_pos:] = data[:first_part]
            self.mmap[:data_len - first_part] = data[first_part:]
        
        self.write_pos = end_pos % self.size
        return True
    
    def read(self, length: int) -> bytes:
        """Read data from ring buffer"""
        available = self.available_read()
        length = min(length, available)
        
        if length == 0:
            return b''
        
        end_pos = self.read_pos + length
        
        if end_pos <= self.size:
            data = self.mmap[self.read_pos:end_pos]
        else:
            # Wrap around
            first_part = self.size - self.read_pos
            data = self.mmap[self.read_pos:] + self.mmap[:length - first_part]
        
        self.read_pos = end_pos % self.size
        return bytes(data)
    
    def available_write(self) -> int:
        """Get available space for writing"""
        if self.write_pos >= self.read_pos:
            return self.size - (self.write_pos - self.read_pos) - 1
        else:
            return self.read_pos - self.write_pos - 1
    
    def available_read(self) -> int:
        """Get available data for reading"""
        if self.write_pos >= self.read_pos:
            return self.write_pos - self.read_pos
        else:
            return self.size - self.read_pos + self.write_pos
    
    def close(self):
        """Clean up resources"""
        self.mmap.close()
        self.temp_file.close()
        os.unlink(self.temp_file.name)


class AsyncIOOptimizer:
    """Ultra-high performance async I/O operations with Meteor Lake optimization"""
    
    def __init__(self, max_workers: int = None):
        # Auto-detect Meteor Lake topology
        self.cpu_count = psutil.cpu_count(logical=True)
        self.p_cores = list(range(0, 12))  # P-cores
        self.e_cores = list(range(12, 22))  # E-cores
        
        # Use E-cores for I/O by default
        self.max_workers = max_workers or len(self.e_cores)
        
        # Thread pools with core affinity
        self.io_executor = ThreadPoolExecutor(
            max_workers=self.max_workers,
            thread_name_prefix="io-e-core"
        )
        self.compute_executor = ThreadPoolExecutor(
            max_workers=len(self.p_cores),
            thread_name_prefix="compute-p-core"
        )
        
        # Caching layers
        self.file_cache = weakref.WeakValueDictionary()
        self.mmap_cache = {}
        self.compression_cache = {}
        
        # Ring buffers for streaming
        self.ring_buffers = {}
        
        # Metrics
        self.metrics = IOMetrics()
        self.latency_samples = deque(maxlen=1000)
        
        # Batch processing configuration
        self.batch_size = 1000
        self.max_concurrent = 100
        
        # Compression engines
        self.lz4_compressor = lz4.frame.LZ4FrameCompressor(
            compression_level=lz4.frame.COMPRESSIONLEVEL_MIN  # Fastest
        )
        self.zstd_compressor = zstd.ZstdCompressor(level=3)  # Balanced
    
    def _set_thread_affinity(self, cores: List[int]):
        """Set thread CPU affinity"""
        try:
            pid = os.getpid()
            os.sched_setaffinity(pid, set(cores))
        except:
            pass  # Not supported on this platform
    
    async def batch_read_files_optimized(
        self, 
        file_paths: List[str],
        pattern: IOPattern = IOPattern.SEQUENTIAL,
        compression: CompressionType = CompressionType.NONE,
        use_direct_io: bool = False
    ) -> Dict[str, bytes]:
        """Batch read files with advanced optimization"""
        
        results = {}
        start_time = time.perf_counter()
        
        # Group files by size for optimal batching
        file_groups = self._group_files_by_size(file_paths)
        
        async def read_file_optimized(path: str) -> Tuple[str, bytes]:
            try:
                # Check cache first
                if path in self.file_cache:
                    self.metrics.cache_hits += 1
                    return path, self.file_cache[path]
                
                self.metrics.cache_misses += 1
                
                # Choose reading strategy
                if use_direct_io and O_DIRECT:
                    data = await self._read_direct_io(path)
                elif pattern == IOPattern.RANDOM:
                    data = await self._read_mmap(path)
                else:
                    data = await self._read_buffered(path)
                
                # Apply decompression if needed
                if compression != CompressionType.NONE:
                    data = self._decompress(data, compression)
                
                # Cache the result
                self.file_cache[path] = data
                
                self.metrics.bytes_read += len(data)
                self.metrics.read_operations += 1
                
                return path, data
                
            except Exception as e:
                logger.error(f"Error reading {path}: {e}")
                return path, None
        
        # Process files in optimized batches
        for size_class, paths in file_groups.items():
            # Use different strategies for different size classes
            if size_class == "small":  # < 1MB
                # Process many small files concurrently
                batch_size = min(len(paths), self.max_concurrent)
            elif size_class == "medium":  # 1MB - 100MB
                # Moderate concurrency
                batch_size = min(len(paths), self.max_concurrent // 2)
            else:  # large > 100MB
                # Process large files sequentially or with low concurrency
                batch_size = min(len(paths), 4)
            
            for i in range(0, len(paths), batch_size):
                batch = paths[i:i + batch_size]
                batch_results = await asyncio.gather(
                    *[read_file_optimized(path) for path in batch],
                    return_exceptions=True
                )
                
                for path, data in batch_results:
                    if not isinstance(data, Exception):
                        results[path] = data
        
        # Update metrics
        elapsed = (time.perf_counter() - start_time) * 1_000_000  # Convert to microseconds
        self.latency_samples.append(elapsed)
        self.metrics.avg_latency_us = np.mean(self.latency_samples)
        self.metrics.p99_latency_us = np.percentile(self.latency_samples, 99) if self.latency_samples else 0
        
        return results
    
    async def _read_direct_io(self, path: str) -> bytes:
        """Read file using Direct I/O (bypasses page cache)"""
        
        loop = asyncio.get_event_loop()
        
        def read_direct():
            # Open with O_DIRECT flag
            fd = os.open(path, os.O_RDONLY | O_DIRECT)
            try:
                stat = os.fstat(fd)
                size = stat.st_size
                
                # Allocate aligned buffer
                buffer = DirectIOBuffer(size)
                
                # Read in aligned chunks
                bytes_read = os.read(fd, size)
                buffer.set_bytes(bytes_read)
                
                return buffer.get_bytes(len(bytes_read))
            finally:
                os.close(fd)
        
        # Run in thread pool on E-cores
        return await loop.run_in_executor(self.io_executor, read_direct)
    
    async def _read_mmap(self, path: str) -> bytes:
        """Read file using memory mapping"""
        
        # Check mmap cache
        if path in self.mmap_cache:
            return bytes(self.mmap_cache[path])
        
        async with aiofiles.open(path, 'rb') as f:
            data = await f.read()
        
        # Cache memory mapped version for repeated access
        with open(path, 'rb') as f:
            self.mmap_cache[path] = mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ)
        
        return data
    
    async def _read_buffered(self, path: str) -> bytes:
        """Standard buffered async read"""
        async with aiofiles.open(path, 'rb') as f:
            return await f.read()
    
    def _group_files_by_size(self, file_paths: List[str]) -> Dict[str, List[str]]:
        """Group files by size class for optimal processing"""
        
        groups = {
            "small": [],    # < 1MB
            "medium": [],   # 1MB - 100MB
            "large": []     # > 100MB
        }
        
        for path in file_paths:
            try:
                size = os.path.getsize(path)
                if size < 1024 * 1024:
                    groups["small"].append(path)
                elif size < 100 * 1024 * 1024:
                    groups["medium"].append(path)
                else:
                    groups["large"].append(path)
            except:
                groups["small"].append(path)  # Default to small if can't stat
        
        return groups
    
    async def batch_write_files_optimized(
        self,
        file_data: Dict[str, bytes],
        compression: CompressionType = CompressionType.NONE,
        use_direct_io: bool = False,
        fsync: bool = True
    ) -> Dict[str, bool]:
        """Batch write files with optimization"""
        
        results = {}
        start_time = time.perf_counter()
        
        async def write_file_optimized(path: str, data: bytes) -> Tuple[str, bool]:
            try:
                # Apply compression if requested
                if compression != CompressionType.NONE:
                    data = self._compress(data, compression)
                    self.metrics.compression_ratio = len(data) / len(file_data[path])
                
                # Choose writing strategy
                if use_direct_io and O_DIRECT:
                    success = await self._write_direct_io(path, data, fsync)
                else:
                    success = await self._write_buffered(path, data, fsync)
                
                if success:
                    self.metrics.bytes_written += len(data)
                    self.metrics.write_operations += 1
                
                return path, success
                
            except Exception as e:
                logger.error(f"Error writing {path}: {e}")
                return path, False
        
        # Process writes in parallel batches
        items = list(file_data.items())
        for i in range(0, len(items), self.batch_size):
            batch = items[i:i + self.batch_size]
            batch_results = await asyncio.gather(
                *[write_file_optimized(path, content) for path, content in batch],
                return_exceptions=True
            )
            
            for path, success in batch_results:
                if not isinstance(success, Exception):
                    results[path] = success
        
        # Update metrics
        elapsed = (time.perf_counter() - start_time) * 1_000_000
        self.latency_samples.append(elapsed)
        self.metrics.avg_latency_us = np.mean(self.latency_samples)
        
        return results
    
    async def _write_direct_io(self, path: str, data: bytes, fsync: bool) -> bool:
        """Write file using Direct I/O"""
        
        loop = asyncio.get_event_loop()
        
        def write_direct():
            # Ensure parent directory exists
            os.makedirs(os.path.dirname(path), exist_ok=True)
            
            # Open with O_DIRECT flag
            fd = os.open(path, os.O_WRONLY | os.O_CREAT | O_DIRECT, 0o644)
            try:
                # Align data to sector boundary
                aligned_size = (len(data) + 511) & ~511
                buffer = DirectIOBuffer(aligned_size)
                buffer.set_bytes(data)
                
                # Write aligned buffer
                os.write(fd, buffer.memview[:aligned_size])
                
                if fsync:
                    os.fsync(fd)
                
                # Truncate to actual size
                os.ftruncate(fd, len(data))
                
                return True
            finally:
                os.close(fd)
        
        return await loop.run_in_executor(self.io_executor, write_direct)
    
    async def _write_buffered(self, path: str, data: bytes, fsync: bool) -> bool:
        """Standard buffered async write"""
        
        # Ensure parent directory exists
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        async with aiofiles.open(path, 'wb') as f:
            await f.write(data)
            if fsync:
                await f.flush()
                os.fsync(f.fileno())
        
        return True
    
    def _compress(self, data: bytes, compression: CompressionType) -> bytes:
        """Compress data using specified algorithm"""
        
        # Check compression cache
        data_hash = hashlib.md5(data).hexdigest()
        cache_key = f"{data_hash}_{compression.value}"
        
        if cache_key in self.compression_cache:
            return self.compression_cache[cache_key]
        
        if compression == CompressionType.LZ4:
            compressed = lz4.frame.compress(data, compression_level=0)  # Fastest
        elif compression == CompressionType.ZSTD:
            compressed = self.zstd_compressor.compress(data)
        else:
            compressed = data
        
        # Cache compressed result
        self.compression_cache[cache_key] = compressed
        
        return compressed
    
    def _decompress(self, data: bytes, compression: CompressionType) -> bytes:
        """Decompress data using specified algorithm"""
        
        if compression == CompressionType.LZ4:
            return lz4.frame.decompress(data)
        elif compression == CompressionType.ZSTD:
            decompressor = zstd.ZstdDecompressor()
            return decompressor.decompress(data)
        else:
            return data
    
    def memory_map_file(self, file_path: str, mode: str = 'r') -> mmap.mmap:
        """Memory map file for ultra-fast access with caching"""
        
        if file_path in self.mmap_cache:
            return self.mmap_cache[file_path]
        
        access = mmap.ACCESS_READ if mode == 'r' else mmap.ACCESS_WRITE
        
        with open(file_path, 'r+b' if mode == 'w' else 'rb') as f:
            mapped = mmap.mmap(f.fileno(), 0, access=access)
            self.mmap_cache[file_path] = mapped
            return mapped
    
    async def stream_read_file(
        self,
        file_path: str,
        chunk_size: int = 64 * 1024  # 64KB chunks
    ) -> AsyncIterator[bytes]:
        """Stream read file in chunks"""
        
        async with aiofiles.open(file_path, 'rb') as f:
            while True:
                chunk = await f.read(chunk_size)
                if not chunk:
                    break
                
                self.metrics.bytes_read += len(chunk)
                yield chunk
    
    async def parallel_file_search(
        self,
        directory: str,
        pattern: str,
        recursive: bool = True,
        max_depth: int = 10
    ) -> List[str]:
        """Parallel file search with pattern matching"""
        
        import fnmatch
        
        results = []
        
        async def search_dir(dir_path: str, depth: int):
            if depth > max_depth:
                return
            
            try:
                entries = await asyncio.get_event_loop().run_in_executor(
                    self.io_executor,
                    os.listdir,
                    dir_path
                )
                
                tasks = []
                for entry in entries:
                    full_path = os.path.join(dir_path, entry)
                    
                    if os.path.isfile(full_path):
                        if fnmatch.fnmatch(entry, pattern):
                            results.append(full_path)
                    elif recursive and os.path.isdir(full_path):
                        tasks.append(search_dir(full_path, depth + 1))
                
                if tasks:
                    await asyncio.gather(*tasks)
                    
            except PermissionError:
                pass  # Skip directories we can't access
        
        await search_dir(directory, 0)
        return results
    
    async def optimized_log_writer(
        self,
        log_entries: List[Dict[str, Any]],
        log_file: str,
        use_ring_buffer: bool = True
    ) -> bool:
        """Ultra-fast log writing with ring buffer and compression"""
        
        try:
            # Use ring buffer for buffering if enabled
            if use_ring_buffer:
                if log_file not in self.ring_buffers:
                    self.ring_buffers[log_file] = MemoryMappedRingBuffer()
                
                ring_buffer = self.ring_buffers[log_file]
            
            # Batch entries into single write operation
            log_buffer = io.BytesIO()
            
            for entry in log_entries:
                # Format log entry
                timestamp = entry.get('timestamp', time.time())
                level = entry.get('level', 'INFO')
                message = entry.get('message', '')
                
                # Binary format for efficiency: [timestamp:8][level:1][msg_len:4][message]
                log_line = struct.pack(
                    '<dBI',
                    timestamp,
                    self._level_to_byte(level),
                    len(message)
                ) + message.encode('utf-8')
                
                log_buffer.write(log_line)
            
            # Get complete buffer
            log_data = log_buffer.getvalue()
            
            # Compress if beneficial
            if len(log_data) > 1024:  # Only compress if > 1KB
                compressed = lz4.frame.compress(log_data, compression_level=0)
                if len(compressed) < len(log_data) * 0.9:  # 10% improvement threshold
                    log_data = compressed
            
            if use_ring_buffer:
                # Write to ring buffer
                success = ring_buffer.write(log_data)
                
                # Flush ring buffer to disk periodically
                if ring_buffer.available_read() > ring_buffer.size // 2:
                    await self._flush_ring_buffer(log_file, ring_buffer)
            else:
                # Direct write
                async with aiofiles.open(log_file, 'ab') as f:
                    await f.write(log_data)
                    success = True
            
            self.metrics.bytes_written += len(log_data)
            self.metrics.write_operations += 1
            
            return success
            
        except Exception as e:
            logger.error(f"Log write error: {e}")
            return False
    
    async def _flush_ring_buffer(self, log_file: str, ring_buffer: MemoryMappedRingBuffer):
        """Flush ring buffer to disk"""
        
        data = ring_buffer.read(ring_buffer.available_read())
        if data:
            async with aiofiles.open(log_file, 'ab') as f:
                await f.write(data)
    
    def _level_to_byte(self, level: str) -> int:
        """Convert log level to byte value"""
        levels = {
            'DEBUG': 0,
            'INFO': 1,
            'WARNING': 2,
            'ERROR': 3,
            'CRITICAL': 4
        }
        return levels.get(level.upper(), 1)
    
    async def vectorized_file_operations(
        self,
        operations: List[Dict[str, Any]]
    ) -> List[Any]:
        """Execute multiple file operations using SIMD-like batching"""
        
        # Group operations by type
        grouped_ops = defaultdict(list)
        for op in operations:
            grouped_ops[op['type']].append(op)
        
        results = []
        
        # Process each group with optimized batching
        for op_type, ops in grouped_ops.items():
            if op_type == 'read':
                paths = [op['path'] for op in ops]
                read_results = await self.batch_read_files_optimized(paths)
                results.extend(read_results.values())
                
            elif op_type == 'write':
                file_data = {op['path']: op['data'] for op in ops}
                write_results = await self.batch_write_files_optimized(file_data)
                results.extend(write_results.values())
                
            elif op_type == 'stat':
                stat_results = await asyncio.gather(
                    *[self._async_stat(op['path']) for op in ops]
                )
                results.extend(stat_results)
        
        return results
    
    async def _async_stat(self, path: str) -> Dict[str, Any]:
        """Async file stat operation"""
        
        loop = asyncio.get_event_loop()
        
        def get_stat():
            try:
                stat = os.stat(path)
                return {
                    'path': path,
                    'size': stat.st_size,
                    'mtime': stat.st_mtime,
                    'mode': stat.st_mode,
                    'exists': True
                }
            except FileNotFoundError:
                return {'path': path, 'exists': False}
        
        return await loop.run_in_executor(self.io_executor, get_stat)
    
    def get_metrics(self) -> Dict[str, Any]:
        """Get I/O performance metrics"""
        
        return {
            'bytes_read': self.metrics.bytes_read,
            'bytes_written': self.metrics.bytes_written,
            'read_operations': self.metrics.read_operations,
            'write_operations': self.metrics.write_operations,
            'cache_hits': self.metrics.cache_hits,
            'cache_misses': self.metrics.cache_misses,
            'cache_hit_rate': self.metrics.cache_hits / max(
                self.metrics.cache_hits + self.metrics.cache_misses, 1
            ),
            'compression_ratio': self.metrics.compression_ratio,
            'avg_latency_us': self.metrics.avg_latency_us,
            'p99_latency_us': self.metrics.p99_latency_us,
            'mmap_cache_size': len(self.mmap_cache),
            'ring_buffers_active': len(self.ring_buffers)
        }
    
    async def close(self):
        """Clean up resources"""
        
        # Flush all ring buffers
        for log_file, ring_buffer in self.ring_buffers.items():
            await self._flush_ring_buffer(log_file, ring_buffer)
            ring_buffer.close()
        
        # Close memory maps
        for mmap_obj in self.mmap_cache.values():
            mmap_obj.close()
        
        # Shutdown executors
        self.io_executor.shutdown(wait=True)
        self.compute_executor.shutdown(wait=True)


# Global optimizer instance with Meteor Lake optimization
io_optimizer = AsyncIOOptimizer()


# Example usage
async def main():
    """Example of optimized I/O operations"""
    
    # Initialize optimizer
    optimizer = AsyncIOOptimizer()
    
    # Batch read with optimization
    files = [f"/tmp/test_{i}.txt" for i in range(100)]
    
    # Create test files
    test_data = {f: f"Test content {i}".encode() for i, f in enumerate(files)}
    await optimizer.batch_write_files_optimized(
        test_data,
        compression=CompressionType.LZ4,
        use_direct_io=False
    )
    
    # Read files with different strategies
    results = await optimizer.batch_read_files_optimized(
        files,
        pattern=IOPattern.SEQUENTIAL,
        compression=CompressionType.LZ4
    )
    
    print(f"Read {len(results)} files")
    
    # Stream read large file
    large_file = "/tmp/large_test.bin"
    with open(large_file, 'wb') as f:
        f.write(os.urandom(10 * 1024 * 1024))  # 10MB
    
    total_bytes = 0
    async for chunk in optimizer.stream_read_file(large_file):
        total_bytes += len(chunk)
    
    print(f"Streamed {total_bytes} bytes")
    
    # Write logs with ring buffer
    log_entries = [
        {
            'timestamp': time.time(),
            'level': 'INFO',
            'message': f'Log entry {i}'
        }
        for i in range(1000)
    ]
    
    await optimizer.optimized_log_writer(
        log_entries,
        "/tmp/test.log",
        use_ring_buffer=True
    )
    
    # Get metrics
    metrics = optimizer.get_metrics()
    print(f"Metrics: {metrics}")
    
    # Cleanup
    await optimizer.close()
    
    # Clean up test files
    for f in files:
        try:
            os.remove(f)
        except:
            pass
    os.remove(large_file)


if __name__ == "__main__":
    asyncio.run(main())
