#!/usr/bin/env python3
"""
NPU Coordination Bridge v1.0
Bridges NPU orchestrator with Agent Coordination Matrix for enhanced parallel execution
"""

import asyncio
import time
import json
import numpy as np
from typing import Dict, List, Any, Optional, Set, Tuple
from dataclasses import dataclass
from enum import Enum
from pathlib import Path
import logging
import sys

# Import existing components

# Add project root to Python path for imports
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

try:
    from path_utilities import (
        get_project_root, get_agents_dir, get_database_dir,
        get_python_src_dir, get_shadowgit_paths, get_database_config
    )
except ImportError:
    # Fallback if path_utilities not available
    def get_project_root():
        return Path(__file__).parent.parent.parent
    def get_agents_dir():
        return get_project_root() / 'agents'
    def get_database_dir():
        return get_project_root() / 'database'
    def get_python_src_dir():
        return get_agents_dir() / 'src' / 'python'
    def get_shadowgit_paths():
        home_dir = Path.home()
        return {'root': home_dir / 'shadowgit'}
    def get_database_config():
        return {
            'host': 'localhost', 'port': 5433,
            'database': 'claude_agents_auth',
            'user': 'claude_agent', 'password': 'claude_auth_pass'
        }
try:
    from npu_optimized_final import OptimizedNPUOrchestrator
    NPU_AVAILABLE = True
except ImportError:
    NPU_AVAILABLE = False

# Add project root to path for agent coordination matrix
sys.path.append(str(get_project_root()))
try:
    from agent_coordination_matrix import AgentCoordinationMatrix, ExecutionMode, AgentCapability
    COORDINATION_AVAILABLE = True
except ImportError:
    COORDINATION_AVAILABLE = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CoordinationMode(Enum):
    """Enhanced coordination modes for NPU integration"""
    NPU_ACCELERATED = "npu_accelerated"
    PARALLEL_OPTIMIZED = "parallel_optimized"
    INTELLIGENT_ROUTING = "intelligent_routing"
    PERFORMANCE_FIRST = "performance_first"
    RELIABILITY_FIRST = "reliability_first"

@dataclass
class WorkflowTask:
    """Individual task in a multi-agent workflow"""
    task_id: str
    agent_name: str
    description: str
    priority: int = 50
    dependencies: List[str] = None
    estimated_duration: float = 1000  # milliseconds

    def __post_init__(self):
        if self.dependencies is None:
            self.dependencies = []

@dataclass
class WorkflowResult:
    """Result from workflow execution"""
    workflow_id: str
    tasks_completed: int
    total_tasks: int
    execution_time_ms: float
    success_rate: float
    performance_metrics: Dict[str, Any]
    agent_utilization: Dict[str, float]

class NPUCoordinationBridge:
    """Bridges NPU orchestrator with Agent Coordination Matrix"""

    def __init__(self):
        self.npu_orchestrator = None
        self.coordination_matrix = None
        self.agent_cache = {}
        self.performance_metrics = {
            'workflows_executed': 0,
            'total_execution_time': 0,
            'average_ops_per_sec': 0,
            'npu_utilization': 0,
            'coordination_overhead': 0
        }
        self.workflow_history = []

    async def initialize(self):
        """Initialize NPU orchestrator and coordination matrix"""
        logger.info("ðŸš€ Initializing NPU Coordination Bridge")

        # Initialize NPU orchestrator
        if NPU_AVAILABLE:
            try:
                self.npu_orchestrator = OptimizedNPUOrchestrator()
                await self.npu_orchestrator.initialize()
                logger.info("âœ… NPU orchestrator initialized")
            except Exception as e:
                logger.warning(f"NPU orchestrator initialization failed: {e}")
                self.npu_orchestrator = None

        # Initialize coordination matrix
        if COORDINATION_AVAILABLE:
            try:
                self.coordination_matrix = AgentCoordinationMatrix()
                await self.coordination_matrix.initialize()
                logger.info("âœ… Agent coordination matrix initialized")
            except Exception as e:
                logger.warning(f"Coordination matrix initialization failed: {e}")
                self.coordination_matrix = None

        # Load agent cache
        await self.load_agent_cache()

        logger.info(f"Bridge initialized: NPU={self.npu_orchestrator is not None}, Matrix={self.coordination_matrix is not None}")

    async def load_agent_cache(self):
        """Load and cache agent information for fast access"""
        agents_dir = Path('${CLAUDE_PROJECT_ROOT:-$(dirname "$0")/../../}agents')
        agent_files = [f for f in agents_dir.glob('*.md')
                      if f.name not in ['README.md', 'TEMPLATE.md', 'WHERE_I_AM.md']]

        for agent_file in agent_files:
            name = agent_file.stem.lower()

            # Categorize agents for intelligent routing
            capabilities = set()
            if any(term in name for term in ['security', 'audit', 'crypto']):
                capabilities.add('security')
            if any(term in name for term in ['debug', 'test', 'lint']):
                capabilities.add('development')
            if any(term in name for term in ['deploy', 'infrastructure', 'monitor']):
                capabilities.add('operations')
            if name in ['director', 'projectorchestrator']:
                capabilities.add('strategic')

            self.agent_cache[name] = {
                'name': name,
                'capabilities': capabilities,
                'priority': self.calculate_priority(name),
                'avg_execution_time': 1000,  # Default 1s
                'success_rate': 0.95
            }

        logger.info(f"Loaded {len(self.agent_cache)} agents into cache")

    def calculate_priority(self, agent_name: str) -> int:
        """Calculate agent priority (1=highest, 100=lowest)"""
        if agent_name in ['director', 'projectorchestrator']:
            return 1
        elif 'security' in agent_name:
            return 10
        elif agent_name in ['architect', 'debugger']:
            return 20
        else:
            return 50

    async def create_workflow(self, tasks: List[Dict[str, Any]],
                            mode: CoordinationMode = CoordinationMode.NPU_ACCELERATED) -> str:
        """Create an optimized multi-agent workflow"""
        workflow_id = f"workflow_{int(time.time() * 1000)}"

        # Convert tasks to WorkflowTask objects
        workflow_tasks = []
        for i, task in enumerate(tasks):
            workflow_task = WorkflowTask(
                task_id=f"{workflow_id}_task_{i}",
                agent_name=task.get('agent', 'director'),
                description=task.get('description', 'Task'),
                priority=task.get('priority', 50),
                dependencies=task.get('dependencies', [])
            )
            workflow_tasks.append(workflow_task)

        # Optimize task order and agent selection
        optimized_tasks = await self.optimize_workflow(workflow_tasks, mode)

        logger.info(f"Created workflow {workflow_id} with {len(optimized_tasks)} tasks")
        return workflow_id, optimized_tasks

    async def optimize_workflow(self, tasks: List[WorkflowTask],
                               mode: CoordinationMode) -> List[WorkflowTask]:
        """Optimize workflow using NPU acceleration and intelligent routing"""

        if mode == CoordinationMode.NPU_ACCELERATED and self.npu_orchestrator:
            # Use NPU for optimal agent selection
            for task in tasks:
                optimal_agent = await self.npu_select_agent(task.description)
                if optimal_agent and optimal_agent != task.agent_name:
                    task.agent_name = optimal_agent
                    logger.debug(f"NPU optimized agent selection: {task.task_id} -> {optimal_agent}")

        # Sort by priority and dependencies
        optimized_tasks = self.resolve_dependencies(tasks)

        return optimized_tasks

    async def npu_select_agent(self, task_description: str) -> Optional[str]:
        """Use NPU to select optimal agent for task"""
        if not self.npu_orchestrator:
            return None

        try:
            # Use NPU for intelligent agent selection
            keywords = task_description.lower().split()[:5]
            selected_agent = await self.npu_orchestrator.select_agent_ultra_fast(keywords)
            return selected_agent
        except Exception as e:
            logger.warning(f"NPU agent selection failed: {e}")
            return None

    def resolve_dependencies(self, tasks: List[WorkflowTask]) -> List[WorkflowTask]:
        """Resolve task dependencies for optimal execution order"""
        resolved = []
        remaining = tasks.copy()

        while remaining:
            # Find tasks with no unresolved dependencies
            ready_tasks = []
            for task in remaining:
                if all(dep_id in [t.task_id for t in resolved] or
                      not any(dep_id in [t.task_id for t in remaining] for dep_id in task.dependencies)
                      for dep_id in task.dependencies):
                    ready_tasks.append(task)

            if not ready_tasks:
                # Break circular dependencies by priority
                ready_tasks = [min(remaining, key=lambda t: t.priority)]

            # Sort ready tasks by priority
            ready_tasks.sort(key=lambda t: t.priority)

            for task in ready_tasks:
                resolved.append(task)
                remaining.remove(task)

        return resolved

    async def execute_workflow(self, workflow_id: str, tasks: List[WorkflowTask],
                             max_parallel: int = 10) -> WorkflowResult:
        """Execute multi-agent workflow with NPU acceleration"""
        start_time = time.perf_counter()

        logger.info(f"Executing workflow {workflow_id} with {len(tasks)} tasks")

        completed_tasks = 0
        failed_tasks = 0
        agent_usage = {}

        # Group tasks by execution level (handling dependencies)
        execution_levels = self.group_by_execution_level(tasks)

        for level, level_tasks in execution_levels.items():
            logger.info(f"Executing level {level} with {len(level_tasks)} tasks")

            # Execute tasks in parallel within the level
            semaphore = asyncio.Semaphore(min(max_parallel, len(level_tasks)))

            async def execute_task(task: WorkflowTask):
                async with semaphore:
                    return await self.execute_single_task(task)

            # Execute all tasks in this level concurrently
            level_results = await asyncio.gather(
                *[execute_task(task) for task in level_tasks],
                return_exceptions=True
            )

            # Process results
            for i, result in enumerate(level_results):
                task = level_tasks[i]
                agent_name = task.agent_name

                if isinstance(result, Exception):
                    logger.error(f"Task {task.task_id} failed: {result}")
                    failed_tasks += 1
                else:
                    completed_tasks += 1

                # Track agent usage
                agent_usage[agent_name] = agent_usage.get(agent_name, 0) + 1

        total_time = time.perf_counter() - start_time
        success_rate = completed_tasks / len(tasks) if tasks else 0

        # Calculate performance metrics
        ops_per_sec = len(tasks) / total_time if total_time > 0 else 0

        performance_metrics = {
            'execution_time_ms': total_time * 1000,
            'ops_per_sec': ops_per_sec,
            'tasks_per_second': len(tasks) / total_time if total_time > 0 else 0,
            'average_task_time_ms': (total_time * 1000) / len(tasks) if tasks else 0,
            'npu_utilization': self.calculate_npu_utilization(),
            'coordination_overhead_ms': self.calculate_coordination_overhead(total_time)
        }

        # Update global metrics
        self.update_performance_metrics(len(tasks), total_time, ops_per_sec)

        result = WorkflowResult(
            workflow_id=workflow_id,
            tasks_completed=completed_tasks,
            total_tasks=len(tasks),
            execution_time_ms=total_time * 1000,
            success_rate=success_rate,
            performance_metrics=performance_metrics,
            agent_utilization=agent_usage
        )

        self.workflow_history.append(result)
        logger.info(f"Workflow {workflow_id} completed: {ops_per_sec:.0f} ops/sec, {success_rate*100:.1f}% success")

        return result

    def group_by_execution_level(self, tasks: List[WorkflowTask]) -> Dict[int, List[WorkflowTask]]:
        """Group tasks by execution level based on dependencies"""
        levels = {}
        task_levels = {}

        def calculate_level(task: WorkflowTask) -> int:
            if task.task_id in task_levels:
                return task_levels[task.task_id]

            if not task.dependencies:
                level = 0
            else:
                # Find maximum level of dependencies
                max_dep_level = -1
                for dep_id in task.dependencies:
                    dep_task = next((t for t in tasks if t.task_id == dep_id), None)
                    if dep_task:
                        dep_level = calculate_level(dep_task)
                        max_dep_level = max(max_dep_level, dep_level)
                level = max_dep_level + 1

            task_levels[task.task_id] = level
            return level

        # Calculate levels for all tasks
        for task in tasks:
            level = calculate_level(task)
            if level not in levels:
                levels[level] = []
            levels[level].append(task)

        return levels

    async def execute_single_task(self, task: WorkflowTask) -> Dict[str, Any]:
        """Execute a single task using the best available method"""
        start_time = time.perf_counter()

        try:
            if self.npu_orchestrator:
                # Use NPU orchestrator for task execution
                result = await self.npu_orchestrator.execute_task_optimized(task.description)
                result['task_id'] = task.task_id
                result['agent_used'] = task.agent_name
            else:
                # Fallback to basic task simulation
                await asyncio.sleep(0.1)  # Simulate task execution
                result = {
                    'task_id': task.task_id,
                    'agent_used': task.agent_name,
                    'execution_time_ms': 100,
                    'status': 'completed'
                }

            execution_time = time.perf_counter() - start_time
            result['actual_execution_time_ms'] = execution_time * 1000

            return result

        except Exception as e:
            logger.error(f"Task execution failed: {e}")
            raise

    def calculate_npu_utilization(self) -> float:
        """Calculate current NPU utilization percentage"""
        if self.npu_orchestrator:
            try:
                metrics = self.npu_orchestrator.get_metrics()
                return metrics.get('npu_utilization', 0) * 100
            except:
                pass
        return 0

    def calculate_coordination_overhead(self, total_time: float) -> float:
        """Calculate coordination overhead in milliseconds"""
        # Estimate coordination overhead as 5% of total execution time
        return total_time * 1000 * 0.05

    def update_performance_metrics(self, task_count: int, execution_time: float, ops_per_sec: float):
        """Update global performance metrics"""
        self.performance_metrics['workflows_executed'] += 1
        self.performance_metrics['total_execution_time'] += execution_time

        # Calculate running average
        total_workflows = self.performance_metrics['workflows_executed']
        self.performance_metrics['average_ops_per_sec'] = (
            (self.performance_metrics['average_ops_per_sec'] * (total_workflows - 1) + ops_per_sec) / total_workflows
        )

    def get_performance_summary(self) -> Dict[str, Any]:
        """Get comprehensive performance summary"""
        recent_workflows = self.workflow_history[-10:] if self.workflow_history else []

        return {
            'bridge_metrics': self.performance_metrics,
            'recent_performance': {
                'workflows_count': len(recent_workflows),
                'average_ops_per_sec': np.mean([w.performance_metrics['ops_per_sec'] for w in recent_workflows]) if recent_workflows else 0,
                'average_success_rate': np.mean([w.success_rate for w in recent_workflows]) if recent_workflows else 0,
                'total_tasks_executed': sum(w.total_tasks for w in recent_workflows)
            },
            'system_status': {
                'npu_available': self.npu_orchestrator is not None,
                'coordination_available': self.coordination_matrix is not None,
                'agents_cached': len(self.agent_cache)
            }
        }

async def performance_test():
    """Test NPU Coordination Bridge performance"""
    print("ðŸš€ NPU Coordination Bridge Performance Test")
    print("=" * 60)

    # Initialize bridge
    bridge = NPUCoordinationBridge()
    await bridge.initialize()

    # Test 1: Simple workflow
    print("\nðŸ“Š Test 1: Simple Multi-Agent Workflow")
    simple_tasks = [
        {'agent': 'security', 'description': 'audit system security', 'priority': 10},
        {'agent': 'architect', 'description': 'design system architecture', 'priority': 20},
        {'agent': 'debugger', 'description': 'analyze system issues', 'priority': 30}
    ]

    workflow_id, optimized_tasks = await bridge.create_workflow(simple_tasks)
    result = await bridge.execute_workflow(workflow_id, optimized_tasks)

    print(f"âœ… Simple workflow: {result.performance_metrics['ops_per_sec']:.0f} ops/sec")
    print(f"   Success rate: {result.success_rate*100:.1f}%")
    print(f"   Execution time: {result.execution_time_ms:.1f}ms")

    # Test 2: Complex parallel workflow
    print("\nðŸ“Š Test 2: Complex Parallel Workflow")
    complex_tasks = []
    for i in range(20):
        task_type = ['security', 'development', 'operations'][i % 3]
        complex_tasks.append({
            'agent': task_type,
            'description': f'{task_type} task {i}',
            'priority': 10 + (i % 5) * 10
        })

    workflow_id, optimized_tasks = await bridge.create_workflow(
        complex_tasks,
        CoordinationMode.NPU_ACCELERATED
    )
    result = await bridge.execute_workflow(workflow_id, optimized_tasks, max_parallel=10)

    print(f"âœ… Complex workflow: {result.performance_metrics['ops_per_sec']:.0f} ops/sec")
    print(f"   Tasks executed: {result.tasks_completed}/{result.total_tasks}")
    print(f"   NPU utilization: {result.performance_metrics['npu_utilization']:.1f}%")

    # Performance summary
    summary = bridge.get_performance_summary()
    print(f"\nðŸ“ˆ Performance Summary:")
    print(f"   Average ops/sec: {summary['recent_performance']['average_ops_per_sec']:.0f}")
    print(f"   Total workflows: {summary['bridge_metrics']['workflows_executed']}")
    print(f"   NPU available: {summary['system_status']['npu_available']}")

    # Target validation
    target_ops = 50000
    current_ops = summary['recent_performance']['average_ops_per_sec']

    if current_ops >= target_ops:
        print(f"\nðŸŽ¯ TARGET ACHIEVED: {current_ops:.0f} >= {target_ops} ops/sec")
    else:
        progress = (current_ops / target_ops) * 100
        print(f"\nðŸ“ˆ TARGET PROGRESS: {progress:.1f}% ({current_ops:.0f}/{target_ops} ops/sec)")

if __name__ == "__main__":
    asyncio.run(performance_test())