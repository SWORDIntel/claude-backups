#!/usr/bin/env python3
"""
Test Runner for Specialized Integration and Performance Tests
Runs the specialized test suites for the Claude Unified Hook System
"""

import os
import sys
import asyncio
import subprocess
import time
from pathlib import Path

def run_command(cmd: str, description: str) -> bool:
    """Run a command and return success status"""
    print(f"\n{'='*60}")
    print(f"ðŸ§ª {description}")
    print(f"{'='*60}")
    
    try:
        result = subprocess.run(
            cmd, 
            shell=True, 
            check=True,
            cwd=Path(__file__).parent,
            capture_output=False
        )
        print(f"âœ… {description} completed successfully")
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ {description} failed with exit code {e.returncode}")
        return False
    except Exception as e:
        print(f"âŒ {description} failed with error: {e}")
        return False

def main():
    """Main test runner"""
    print("=" * 80)
    print("CLAUDE UNIFIED HOOKS SPECIALIZED TEST RUNNER")
    print("Integration Tests + Performance Benchmarks")
    print("=" * 80)
    
    start_time = time.time()
    results = []
    
    # Check if pytest is available
    try:
        subprocess.run([sys.executable, "-c", "import pytest"], check=True, capture_output=True)
    except subprocess.CalledProcessError:
        print("âŒ pytest is not installed. Installing...")
        try:
            subprocess.run([sys.executable, "-m", "pip", "install", "pytest", "pytest-asyncio"], check=True)
        except subprocess.CalledProcessError:
            print("âŒ Failed to install pytest. Please install manually:")
            print("   pip install pytest pytest-asyncio")
            return False
    
    # Check if required dependencies are available
    required_packages = ["psutil"]  # pathlib is built-in
    for package in required_packages:
        try:
            subprocess.run([sys.executable, "-c", f"import {package}"], check=True, capture_output=True)
        except subprocess.CalledProcessError:
            print(f"âŒ {package} is not installed. Installing...")
            try:
                subprocess.run([sys.executable, "-m", "pip", "install", package], check=True)
            except subprocess.CalledProcessError:
                print(f"âš ï¸  Failed to install {package}, some tests may fail")
    
    # Test 1: Integration Tests
    integration_cmd = f"{sys.executable} -m pytest test_integration.py -v --tb=short"
    integration_success = run_command(integration_cmd, "Integration Tests")
    results.append(("Integration Tests", integration_success))
    
    # Test 2: Performance Benchmarks  
    performance_cmd = f"{sys.executable} -m pytest test_performance.py -v --tb=short -x"
    performance_success = run_command(performance_cmd, "Performance Benchmarks")
    results.append(("Performance Benchmarks", performance_success))
    
    # Test 3: Run integration tests directly (alternative approach)
    if not integration_success:
        print(f"\n{'='*60}")
        print("ðŸ”„ Trying direct integration test execution...")
        print(f"{'='*60}")
        
        direct_integration_cmd = f"{sys.executable} test_integration.py"
        direct_integration_success = run_command(direct_integration_cmd, "Direct Integration Test")
        if direct_integration_success:
            results[0] = ("Integration Tests (Direct)", True)
    
    # Test 4: Run performance tests directly (alternative approach)
    if not performance_success:
        print(f"\n{'='*60}")
        print("ðŸ”„ Trying direct performance test execution...")
        print(f"{'='*60}")
        
        direct_performance_cmd = f"{sys.executable} test_performance.py"
        direct_performance_success = run_command(direct_performance_cmd, "Direct Performance Test")
        if direct_performance_success:
            results[1] = ("Performance Benchmarks (Direct)", True)
    
    # Test 5: Comprehensive test suite (if available)
    comprehensive_cmd = f"{sys.executable} -m pytest test_claude_unified_hooks.py -v --tb=short"
    if Path("test_claude_unified_hooks.py").exists():
        comprehensive_success = run_command(comprehensive_cmd, "Comprehensive Test Suite")
        results.append(("Comprehensive Test Suite", comprehensive_success))
    
    # Generate summary report
    total_time = time.time() - start_time
    
    print(f"\n{'='*80}")
    print("TEST EXECUTION SUMMARY")
    print(f"{'='*80}")
    
    passed_count = sum(1 for _, success in results if success)
    total_count = len(results)
    
    print(f"â±ï¸  Total execution time: {total_time:.1f}s")
    print(f"ðŸ“Š Test suites executed: {total_count}")
    print(f"âœ… Passed: {passed_count}")
    print(f"âŒ Failed: {total_count - passed_count}")
    
    print(f"\nðŸ“‹ DETAILED RESULTS:")
    for test_name, success in results:
        status = "âœ… PASS" if success else "âŒ FAIL"
        print(f"   {status}: {test_name}")
    
    # Overall assessment
    success_rate = (passed_count / total_count) * 100 if total_count > 0 else 0
    
    if success_rate >= 100:
        print(f"\nðŸŽ‰ ALL TESTS PASSED ({success_rate:.0f}%)")
        print("   The hook system meets all integration and performance targets!")
    elif success_rate >= 80:
        print(f"\nâœ… MOSTLY SUCCESSFUL ({success_rate:.0f}%)")
        print("   Most tests passed - system is largely functional.")
    elif success_rate >= 50:
        print(f"\nâš ï¸  PARTIAL SUCCESS ({success_rate:.0f}%)")
        print("   Some tests failed - system needs attention.")
    else:
        print(f"\nâŒ SIGNIFICANT ISSUES ({success_rate:.0f}%)")
        print("   Many tests failed - system requires major fixes.")
    
    # Performance targets summary
    print(f"\nðŸŽ¯ KEY PERFORMANCE TARGETS:")
    print("   ðŸ“ˆ 4-6x parallel execution speedup")
    print("   ðŸ“Š 1000 requests/minute throughput")
    print("   ðŸ’¾ >75% cache hit rate")
    print("   ðŸ§  <200MB memory usage under load")
    print("   â±ï¸  <100ms P99 latency")
    print("   ðŸ”’ Comprehensive security validation")
    
    # Integration targets summary
    print(f"\nðŸ”— KEY INTEGRATION TARGETS:")
    print("   ðŸ¤ Multi-agent coordination workflows")
    print("   ðŸ”’ File system operations with locking")
    print("   ðŸ›¡ï¸  Circuit breaker behavior under load")
    print("   ðŸ“Š Resource limit enforcement")
    print("   ðŸ‘¥ 76-agent registry integration")
    print("   ðŸŽ¯ Pattern matching against real agents")
    print("   ðŸŒŸ Shadowgit integration preparation")
    
    print("=" * 80)
    
    return success_rate >= 80

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)