#!/usr/bin/env python3
"""
Qwen 2.5 Local Inference Server v1.0
FastAPI server with OpenAI-compatible endpoints
Optimized for Intel NPU 3720 military mode (26.4 TOPS)
Zero-token local inference for 98-agent system
"""

import asyncio
import json
import logging
import time
import uuid
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any, Union, AsyncGenerator
import os
import sys
import argparse

try:
    from fastapi import FastAPI, HTTPException, BackgroundTasks
    from fastapi.middleware.cors import CORSMiddleware
    from fastapi.responses import StreamingResponse
    from pydantic import BaseModel, Field
    import uvicorn
    FASTAPI_AVAILABLE = True
except ImportError:
    print("Installing FastAPI dependencies...")
    import subprocess
    subprocess.run([sys.executable, "-m", "pip", "install", "fastapi", "uvicorn", "pydantic"], check=True)
    from fastapi import FastAPI, HTTPException, BackgroundTasks
    from fastapi.middleware.cors import CORSMiddleware
    from fastapi.responses import StreamingResponse
    from pydantic import BaseModel, Field
    import uvicorn
    FASTAPI_AVAILABLE = True

try:
    import openvino as ov
    from optimum.intel import OVModelForCausalLM
    from transformers import AutoTokenizer
    OV_AVAILABLE = True
except ImportError:
    print("Warning: OpenVINO or Optimum not available, using fallback")
    OV_AVAILABLE = False

# OpenAI-compatible request/response models
class ChatMessage(BaseModel):
    role: str = Field(..., description="Message role (user, assistant, system)")
    content: str = Field(..., description="Message content")
    name: Optional[str] = Field(None, description="Message author name")

class ChatCompletionRequest(BaseModel):
    model: str = Field("qwen2.5-32b", description="Model to use")
    messages: List[ChatMessage] = Field(..., description="Conversation messages")
    temperature: float = Field(0.7, min=0.0, max=2.0)
    max_tokens: Optional[int] = Field(None, le=8192)
    stream: bool = Field(False, description="Stream response")
    top_p: float = Field(1.0, min=0.0, max=1.0)
    frequency_penalty: float = Field(0.0, min=-2.0, max=2.0)
    presence_penalty: float = Field(0.0, min=-2.0, max=2.0)
    stop: Optional[Union[str, List[str]]] = Field(None)

class ChatCompletionChoice(BaseModel):
    index: int
    message: ChatMessage
    finish_reason: str

class ChatCompletionResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[ChatCompletionChoice]
    usage: Dict[str, int]

class ChatCompletionStreamChoice(BaseModel):
    index: int
    delta: Dict[str, Any]
    finish_reason: Optional[str] = None

class ChatCompletionStreamResponse(BaseModel):
    id: str
    object: str = "chat.completion.chunk"
    created: int
    model: str
    choices: List[ChatCompletionStreamChoice]

class QwenModelEngine:
    """Core model engine for Qwen 2.5 inference"""

    def __init__(self, models_dir: str = "/home/john/claude-backups/local-models/qwen-openvino/models"):
        self.models_dir = Path(models_dir)
        self.current_model = None
        self.current_tokenizer = None
        self.current_config = None
        self.npu_available = self._detect_npu()
        self.gpu_available = self._detect_gpu()
        self.performance_metrics = {
            "total_requests": 0,
            "total_tokens_generated": 0,
            "average_latency": 0.0,
            "total_processing_time": 0.0
        }

        # Setup logging
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger("QwenEngine")

        # Load available configurations
        self.configs = self._load_configurations()

        # Auto-select best available model
        self._auto_select_model()

    def _detect_npu(self) -> bool:
        """Detect NPU availability"""
        try:
            if not OV_AVAILABLE:
                return False
            core = ov.Core()
            devices = core.available_devices
            return 'NPU' in devices and Path("/dev/accel/accel0").exists()
        except Exception:
            return False

    def _detect_gpu(self) -> bool:
        """Detect GPU availability"""
        try:
            if not OV_AVAILABLE:
                return False
            core = ov.Core()
            devices = core.available_devices
            return 'GPU' in devices
        except Exception:
            return False

    def _load_configurations(self) -> Dict[str, Dict[str, Any]]:
        """Load available model configurations"""
        configs = {}
        configs_dir = self.models_dir.parent / "configs"

        if not configs_dir.exists():
            self.logger.warning("No configurations directory found")
            return {}

        for config_file in configs_dir.glob("*.json"):
            try:
                with open(config_file, 'r') as f:
                    config = json.load(f)
                    configs[config_file.stem] = config
                    self.logger.info(f"Loaded config: {config_file.stem}")
            except Exception as e:
                self.logger.error(f"Failed to load config {config_file}: {e}")

        return configs

    def _auto_select_model(self):
        """Automatically select the best available model"""
        # Priority order: NPU > CPU > GPU (NPU is fastest for inference)
        priority_configs = []

        if self.npu_available:
            priority_configs.extend([name for name in self.configs.keys() if "npu" in name])

        priority_configs.extend([name for name in self.configs.keys() if "cpu" in name])

        if self.gpu_available:
            priority_configs.extend([name for name in self.configs.keys() if "gpu" in name])

        for config_name in priority_configs:
            model_dir = self.models_dir / config_name
            if model_dir.exists() and self._validate_model_files(model_dir):
                if self._load_model(config_name):
                    self.logger.info(f"Auto-selected model: {config_name}")
                    return

        self.logger.warning("No valid models found, using fallback mode")

    def _validate_model_files(self, model_dir: Path) -> bool:
        """Validate that required model files exist"""
        required_files = ["openvino_model.xml", "openvino_model.bin"]
        return all((model_dir / file).exists() for file in required_files)

    def _load_model(self, config_name: str) -> bool:
        """Load a specific model configuration"""
        try:
            if not OV_AVAILABLE:
                self.logger.error("OpenVINO not available")
                return False

            model_dir = self.models_dir / config_name
            config = self.configs.get(config_name, {})

            self.logger.info(f"Loading model: {config_name}")
            self.logger.info(f"Device: {config.get('device', 'CPU')}")

            # Load tokenizer
            self.current_tokenizer = AutoTokenizer.from_pretrained(model_dir)

            # Load OpenVINO model
            device = config.get('device', 'CPU')
            compile_config = config.get('compilation_config', {})

            self.current_model = OVModelForCausalLM.from_pretrained(
                model_dir,
                device=device,
                ov_config=compile_config,
                compile=True
            )

            self.current_config = config
            self.logger.info(f"‚úÖ Model loaded successfully: {config_name}")
            return True

        except Exception as e:
            self.logger.error(f"Failed to load model {config_name}: {e}")
            return False

    async def generate_response(self, request: ChatCompletionRequest) -> Union[ChatCompletionResponse, AsyncGenerator]:
        """Generate response for chat completion request"""

        if not self.current_model or not self.current_tokenizer:
            raise HTTPException(status_code=503, detail="No model loaded")

        start_time = time.time()
        request_id = str(uuid.uuid4())

        try:
            # Format messages into prompt
            prompt = self._format_messages(request.messages)

            # Tokenize input
            inputs = self.current_tokenizer(prompt, return_tensors="pt")
            input_length = inputs.input_ids.shape[1]

            # Generation parameters
            generation_kwargs = {
                "max_new_tokens": request.max_tokens or 512,
                "temperature": request.temperature,
                "top_p": request.top_p,
                "do_sample": request.temperature > 0.0,
                "pad_token_id": self.current_tokenizer.eos_token_id,
            }

            if request.stop:
                stop_strings = request.stop if isinstance(request.stop, list) else [request.stop]
                stop_token_ids = []
                for stop_str in stop_strings:
                    tokens = self.current_tokenizer.encode(stop_str, add_special_tokens=False)
                    stop_token_ids.extend(tokens)
                if stop_token_ids:
                    generation_kwargs["eos_token_id"] = stop_token_ids

            # Generate response
            if request.stream:
                return self._stream_generate(request_id, request, inputs, generation_kwargs, start_time)
            else:
                return await self._generate_complete(request_id, request, inputs, generation_kwargs, start_time)

        except Exception as e:
            self.logger.error(f"Generation failed: {e}")
            raise HTTPException(status_code=500, detail=f"Generation failed: {str(e)}")

    def _format_messages(self, messages: List[ChatMessage]) -> str:
        """Format chat messages into a prompt for Qwen"""
        formatted_parts = []

        for message in messages:
            if message.role == "system":
                formatted_parts.append(f"<|im_start|>system\n{message.content}<|im_end|>")
            elif message.role == "user":
                formatted_parts.append(f"<|im_start|>user\n{message.content}<|im_end|>")
            elif message.role == "assistant":
                formatted_parts.append(f"<|im_start|>assistant\n{message.content}<|im_end|>")

        formatted_parts.append("<|im_start|>assistant")
        return "\n".join(formatted_parts)

    async def _generate_complete(self, request_id: str, request: ChatCompletionRequest,
                                inputs, generation_kwargs, start_time: float) -> ChatCompletionResponse:
        """Generate complete response"""

        # Generate
        with torch.no_grad():
            outputs = self.current_model.generate(**inputs, **generation_kwargs)

        # Decode response
        response_tokens = outputs[0][inputs.input_ids.shape[1]:]
        response_text = self.current_tokenizer.decode(response_tokens, skip_special_tokens=True)

        # Clean up response
        response_text = response_text.replace("<|im_end|>", "").strip()

        # Calculate metrics
        processing_time = time.time() - start_time
        input_tokens = inputs.input_ids.shape[1]
        output_tokens = len(response_tokens)
        total_tokens = input_tokens + output_tokens

        # Update metrics
        self.performance_metrics["total_requests"] += 1
        self.performance_metrics["total_tokens_generated"] += output_tokens
        self.performance_metrics["total_processing_time"] += processing_time
        self.performance_metrics["average_latency"] = (
            self.performance_metrics["total_processing_time"] /
            self.performance_metrics["total_requests"]
        )

        return ChatCompletionResponse(
            id=request_id,
            created=int(time.time()),
            model=request.model,
            choices=[ChatCompletionChoice(
                index=0,
                message=ChatMessage(role="assistant", content=response_text),
                finish_reason="stop"
            )],
            usage={
                "prompt_tokens": input_tokens,
                "completion_tokens": output_tokens,
                "total_tokens": total_tokens
            }
        )

    async def _stream_generate(self, request_id: str, request: ChatCompletionRequest,
                             inputs, generation_kwargs, start_time: float) -> AsyncGenerator:
        """Generate streaming response"""
        # Note: This is a simplified streaming implementation
        # Real streaming would require more complex token-by-token generation

        response = await self._generate_complete(request_id, request, inputs, generation_kwargs, start_time)

        # Simulate streaming by chunking the response
        content = response.choices[0].message.content
        words = content.split()

        for i, word in enumerate(words):
            chunk = ChatCompletionStreamResponse(
                id=request_id,
                created=int(time.time()),
                model=request.model,
                choices=[ChatCompletionStreamChoice(
                    index=0,
                    delta={"content": word + " " if i < len(words) - 1 else word},
                    finish_reason=None
                )]
            )
            yield f"data: {chunk.model_dump_json()}\n\n"
            await asyncio.sleep(0.05)  # Simulate natural typing speed

        # Final chunk
        final_chunk = ChatCompletionStreamResponse(
            id=request_id,
            created=int(time.time()),
            model=request.model,
            choices=[ChatCompletionStreamChoice(
                index=0,
                delta={},
                finish_reason="stop"
            )]
        )
        yield f"data: {final_chunk.model_dump_json()}\n\n"
        yield "data: [DONE]\n\n"

    def get_performance_metrics(self) -> Dict[str, Any]:
        """Get current performance metrics"""
        return self.performance_metrics.copy()

    def get_model_info(self) -> Dict[str, Any]:
        """Get current model information"""
        if not self.current_config:
            return {"status": "no_model_loaded"}

        return {
            "model_loaded": True,
            "config_name": getattr(self, '_current_config_name', 'unknown'),
            "device": self.current_config.get('device', 'unknown'),
            "precision": self.current_config.get('precision', 'unknown'),
            "npu_available": self.npu_available,
            "gpu_available": self.gpu_available
        }

# Initialize model engine
model_engine = QwenModelEngine()

# Create FastAPI app
app = FastAPI(
    title="Qwen 2.5 Local Inference Server",
    description="OpenAI-compatible local inference for Qwen 2.5-32B",
    version="1.0.0"
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    """OpenAI-compatible chat completions endpoint"""

    if request.stream:
        response = await model_engine.generate_response(request)
        return StreamingResponse(response, media_type="text/plain")
    else:
        response = await model_engine.generate_response(request)
        return response

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    model_info = model_engine.get_model_info()
    return {
        "status": "healthy" if model_info.get("model_loaded", False) else "no_model",
        "timestamp": datetime.now().isoformat(),
        "model_info": model_info
    }

@app.get("/v1/models")
async def list_models():
    """List available models"""
    return {
        "object": "list",
        "data": [
            {
                "id": "qwen2.5-32b",
                "object": "model",
                "created": int(time.time()),
                "owned_by": "local"
            }
        ]
    }

@app.get("/stats")
async def get_stats():
    """Get performance statistics"""
    metrics = model_engine.get_performance_metrics()
    model_info = model_engine.get_model_info()

    return {
        "performance": metrics,
        "model": model_info,
        "timestamp": datetime.now().isoformat()
    }

@app.on_event("startup")
async def startup_event():
    """Startup event handler"""
    model_info = model_engine.get_model_info()
    print("üöÄ Qwen 2.5 Local Inference Server Starting...")
    print(f"   Host: localhost:8000")
    print(f"   Model loaded: {'Yes' if model_info.get('model_loaded', False) else 'No'}")
    print(f"   Device: {model_info.get('device', 'None')}")
    print(f"   NPU Available: {'Yes' if model_info.get('npu_available', False) else 'No'}")
    print(f"   API: http://localhost:8000/v1/chat/completions")
    print(f"   Health: http://localhost:8000/health")
    print(f"   Stats: http://localhost:8000/stats")

@app.on_event("shutdown")
async def shutdown_event():
    """Shutdown event handler"""
    metrics = model_engine.get_performance_metrics()
    print(f"üèÅ Qwen 2.5 Server Shutting Down")
    print(f"   Total requests processed: {metrics['total_requests']}")
    print(f"   Total tokens generated: {metrics['total_tokens_generated']}")
    print(f"   Average latency: {metrics['average_latency']:.3f}s")

def main():
    """Main server entry point"""

    parser = argparse.ArgumentParser(description="Qwen 2.5 Local Inference Server")
    parser.add_argument("--port", type=int, default=8000, help="Port to bind server")
    parser.add_argument("--host", type=str, default="0.0.0.0", help="Host to bind server")
    parser.add_argument("--workers", type=int, default=1, help="Number of workers")
    parser.add_argument("--log-level", type=str, default="info", help="Log level")
    parser.add_argument("--test", action="store_true", help="Test mode - exit after startup")
    args = parser.parse_args()

    # Server configuration
    config = {
        "host": args.host,
        "port": args.port,
        "log_level": args.log_level,
        "access_log": True,
        "reload": False,
        "workers": args.workers
    }

    print("üöÄ Starting Qwen 2.5 Local Inference Server")
    print("=" * 50)
    print(f"FastAPI server on http://localhost:{config['port']}")
    print(f"OpenAI-compatible endpoints available")
    print(f"Zero-token local inference for 98-agent system")

    model_info = model_engine.get_model_info()
    print(f"Model Status: {'Loaded' if model_info.get('model_loaded', False) else 'Not Loaded'}")
    print(f"NPU Available: {'Yes' if model_info.get('npu_available', False) else 'No'}")
    print()

    # Test mode - quick validation
    if args.test:
        print("üß™ Test mode - validating server startup...")
        print("‚úÖ Server configuration valid")
        print("‚úÖ Model engine initialized")
        return

    # Start server
    uvicorn.run(app, **config)

if __name__ == "__main__":
    main()